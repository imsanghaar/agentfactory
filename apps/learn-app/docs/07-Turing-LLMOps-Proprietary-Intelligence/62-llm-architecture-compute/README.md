---
sidebar_position: 62
title: "Chapter 62: LLM Architecture & Compute"
description: "Model anatomy, tokenization, context, and compute choices that drive fine-tuning and deployment"
---

# Chapter 62: LLM Architecture & Compute

Learn how LLMs are built and what compute they need. You build an `llm-architecture` skill to reason about tokenization, context windows, scaling laws, and hardware choices that impact cost and performance.

---

## Goals

- Understand tokenization, embeddings, and context windows
- Compare model families/sizes and their tradeoffs
- Relate compute (GPU/TPU) choices to training/inference cost
- Capture architecture/compute considerations in a reusable skill

---

## Lesson Progression

- Model anatomy and tokenization
- Context, attention scaling, and performance tradeoffs
- Hardware and cost planning for training/inference
- Finalize the architecture & compute skill

---

## Outcome & Method

You finish with an architecture/compute reference skill that guides training and deployment choices in later chapters.

---

## Prerequisites

- Chapter 61 decision framework
