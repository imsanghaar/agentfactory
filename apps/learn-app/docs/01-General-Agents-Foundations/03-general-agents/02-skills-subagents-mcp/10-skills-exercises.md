---
slug: /General-Agents-Foundations/general-agents/skills-exercises
title: "Agent Skills Exercises"
practice_exercise: ch3-skills
sidebar_position: 10
chapter: 3
lesson: 10
duration_minutes: 120

# PEDAGOGICAL LAYER METADATA
primary_layer: "Layer 1"
layer_progression: "L1 (Manual Foundation)"
layer_1_foundation: "Hands-on practice building, testing, and composing agent skills through 27 guided exercises"
layer_2_collaboration: "N/A"
layer_3_intelligence: "N/A"
layer_4_capstone: "N/A"

# HIDDEN SKILLS METADATA
skills:
  - name: "Skill Architecture"
    proficiency_level: "B1"
    category: "Technical"
    bloom_level: "Apply"
    digcomp_area: "Digital Content Creation"
    measurable_at_this_level: "Student can read, analyze, and create SKILL.md files with proper structure and YAML frontmatter"
  - name: "Iterative Skill Development"
    proficiency_level: "B1"
    category: "Applied"
    bloom_level: "Evaluate"
    digcomp_area: "Problem Solving"
    measurable_at_this_level: "Student can test skills against edge cases and systematically improve them through iteration"
  - name: "Skill Composition"
    proficiency_level: "B1"
    category: "Technical"
    bloom_level: "Create"
    digcomp_area: "Digital Content Creation"
    measurable_at_this_level: "Student can compose multiple skills into workflows and build production-ready skill suites"

learning_objectives:
  - objective: "Analyze existing skills to understand SKILL.md structure and YAML frontmatter"
    proficiency_level: "B1"
    bloom_level: "Analyze"
    assessment_method: "Completion of Module 1 anatomy and comparison exercises"
  - objective: "Create functional skills with clear triggers, instructions, and examples"
    proficiency_level: "B1"
    bloom_level: "Create"
    assessment_method: "Skills created in Modules 2-4 that produce consistent output"
  - objective: "Test and iterate on skills using systematic evaluation"
    proficiency_level: "B1"
    bloom_level: "Evaluate"
    assessment_method: "Self-assessment using the 6-criteria rubric across Module 5 exercises"
  - objective: "Compose skills into multi-step workflows for real-world scenarios"
    proficiency_level: "B1"
    bloom_level: "Create"
    assessment_method: "Capstone projects producing complete skill suites"

cognitive_load:
  new_concepts: 3
  assessment: "3 concepts (skill architecture patterns, iterative testing, skill composition) — within B1 limit. Exercises reinforce L08-L09 concepts."

differentiation:
  extension_for_advanced: "Complete all 3 capstone projects; build skills that compose with MCP servers"
  remedial_for_struggling: "Start with Module 1 only; use provided sample skills as templates"

teaching_guide:
  lesson_type: "hands-on"
  session_group: 3
  session_title: "Skills Architecture and Hands-On Practice"
  key_points:
    - "The 6-step framework (Define, Draft, Test, Evaluate, Improve, Repeat) is the core iteration loop students will use for every skill they build going forward"
    - "Module progression mirrors professional skill development: read (Module 1) then write (2) then add examples (3) then add references (4) then test (5) then compose (6) then apply (7-8)"
    - "The self-assessment rubric with 6 criteria and 4 levels gives students an objective measure — score of 18+ means production-ready"
    - "Exercise 1.3 (Skill vs Raw Prompt comparison) provides the measurable evidence that skills outperform ad-hoc prompting"
  misconceptions:
    - "Students try to complete all 27 exercises in one sitting — this is a 120-minute lesson designed for selective progression through modules, not exhaustive completion"
    - "Students skip Module 1 (reading skills) and jump to Module 2 (writing skills) — reading fluency must come before writing, just like natural language"
    - "Students think capstone projects are optional extras — they are the integration point where isolated skills become coherent systems"
  discussion_prompts:
    - "After completing Exercise 1.3 (skill vs raw prompt), how many rubric points did the skill version score higher — and what does that tell you about when skills are worth the investment?"
    - "In Exercise 5.3 (user testing), what did your partner find confusing that seemed obvious to you — and what does this reveal about the gap between creator intent and user experience?"
  teaching_tips:
    - "Assign Module 1 as pre-work — students who arrive having analyzed the three sample skills can immediately start building in Module 2"
    - "Use the self-assessment rubric as a class activity: have students score the same skill independently, then compare scores — disagreements reveal where rubric interpretation needs calibration"
    - "For the capstones, let students self-select based on interest (Business, Education, Personal) — intrinsic motivation produces higher quality skills"
    - "Exercise 6.1 (Skill Pipeline) is the conceptual bridge to Lesson 11 (Subagents) — highlight how chaining skills manually prepares them for automated orchestration"
  assessment_quick_check:
    - "List the 6 steps of the skill-building framework in order"
    - "What score on the self-assessment rubric indicates a production-ready skill?"
    - "Name one thing you can only learn from Module 5 (Testing) that you cannot learn from Modules 2-4 (Building)"
---

# Agent Skills Exercises

You understand what skills are (Lesson 08). You've built your first one (Lesson 09). Theory is behind you. Now you build.

These exercises take you from dissecting existing skills to composing production-ready skill suites for real-world scenarios. Each module adds a layer of sophistication: first you read skills, then you write them, then you add examples and references, then you test and iterate, then you chain them into workflows, and finally you build complete skill libraries for business, education, or personal productivity.

The Big Idea behind all of this: **a skill is a reusable instruction file that teaches an AI agent how to handle a specific type of task consistently.** Building skills is the fundamental capability that separates "using AI" from "building with AI."

---

:::info Download Exercise Files
**[Download Skills Exercises (ZIP)](https://github.com/imsanghaar/claude-code-skills-exercises/releases/latest/download/skills-exercises.zip)**

After downloading, unzip the file. Each exercise has its own folder with an `INSTRUCTIONS.md` and any starter files you need.

If the download link doesn't work, visit the [repository releases page](https://github.com/imsanghaar/claude-code-skills-exercises/releases) directly.
:::

---

## How to Use These Exercises

Every exercise follows the same workflow:

1. **Navigate** to the exercise folder in your terminal
2. **Start** Claude Code (`claude`) or open the folder in Cowork
3. **Tell Claude**: `Read INSTRUCTIONS.md and help me build this skill. Walk me through each decision and test the result.`
4. **Work through** each task in the instructions
5. **Reflect** on the questions at the end before moving on

You can work through exercises at your own pace. Each module builds on the previous one, so work in order within a module. You can skip ahead between modules if a particular topic interests you, but the full sequence gives you the strongest foundation.

---

## Tool Guide

Each exercise is marked with the tools it works best with:

- **Claude Code** — Terminal-based, best for building and testing skills with file system access
- **Cowork** — Desktop app, best for reviewing skills and working with reference documents side-by-side

Most exercises work with either tool. Where one is clearly better, the exercise notes will say so.

---

## The Skill-Building Framework

Use this framework for **every** skill you build across these exercises:

### 1. DEFINE — What problem does this skill solve?

- What task is being automated?
- What does "good" output look like?
- What's the current pain (without the skill)?

### 2. DRAFT — Write the first version of SKILL.md

- Frontmatter (name + trigger description)
- Step-by-step process
- Output format
- Rules and constraints

### 3. TEST — Run it on real examples

- Start with 2-3 "normal" test cases
- Then try edge cases designed to break it

### 4. EVALUATE — Score the output

- Does it match the expected format?
- Is the content correct?
- Would you actually USE this output?

### 5. IMPROVE — Fix what's broken

- Add examples for areas that were inconsistent
- Add rules for edge cases that weren't handled
- Tighten vague instructions

### 6. REPEAT — Until quality is consistent

- Test, Evaluate, Improve is an iterative loop
- 2-3 rounds is typical; production skills may need 5+

This framework applies whether you're building a simple email skill or a complex multi-step pipeline. Return to it whenever you get stuck.

---

## Self-Assessment Rubric

Use this rubric to evaluate every skill you build. Score yourself honestly after each exercise:

| Criteria                |      1 (Beginner)      |    2 (Developing)    |          3 (Proficient)           |             4 (Advanced)              |
| ----------------------- | :--------------------: | :------------------: | :-------------------------------: | :-----------------------------------: |
| **Trigger Description** | Too vague or too broad |  Covers main cases   |       Specific and complete       |     Handles edge cases in trigger     |
| **Instructions**        |     Missing steps      |  Basic steps listed  |     Clear, ordered, complete      |        Includes decision logic        |
| **Examples**            |          None          | One embedded example | Multiple examples + anti-examples | Example pairs showing input to output |
| **Edge Case Handling**  |     Not considered     |    Some mentioned    |      Rules for common edges       |          Tested and verified          |
| **Testing**             |        Ran once        |   Tested 2-3 cases   |       Systematic test suite       |       Tested by another person        |
| **Iteration**           |    First draft only    |     One revision     |        Multiple iterations        |    Measurably improved via rubric     |

A score of 18+ (out of 24) means you have a production-ready skill. Below 12 means you need another iteration round. Track your scores across exercises to see your growth.

---

## Module 1: Understanding Skills

Before you build skills, you need to read them fluently. These three exercises develop your ability to analyze existing skills, judge when a skill is the right tool, and understand what skills add beyond raw prompts.

<ExerciseCard id="1.1" title="Anatomy of a Skill" />

### Exercise 1.1 — Anatomy of a Skill

**The Problem:**
You have three sample skills of increasing complexity: a simple greeting formatter, a medium weekly report generator, and a complex content reviewer with examples and references. Your job is to take them apart and understand how each piece works.

**Your Task:**
Open each SKILL.md and identify the five core components: frontmatter, trigger conditions, step-by-step instructions, output format, and rules/constraints. Write a 3-sentence summary for each skill. Then predict what Claude will produce for provided test prompts before running them. Finally, find the intentional weakness in each skill (language handling, empty sections, non-text content).

**What You'll Learn:**

- How to read SKILL.md files quickly and identify their components
- How skill structure maps to output quality
- Where skills break and how to spot gaps

**Starter Prompt:**

> "Read the three sample skills in `sample-skills/` and walk me through the anatomy of each one. Help me identify the frontmatter, triggers, instructions, output format, and constraints."

**Reflection Questions:**

1. Which of the three skills would produce the most consistent output? Why?
2. How does the complexity of the SKILL.md correlate with the quality of the output?
3. Could you fix each skill's weakness in one sentence?

---

<ExerciseCard id="1.2" title="When to Build a Skill" />

### Exercise 1.2 — When to Build a Skill (and When Not To)

**The Problem:**
Not every task needs a skill. Building unnecessary skills wastes time; skipping useful ones means repeating yourself. You need to develop judgment about which tasks are "skill-worthy."

**Your Task:**
Classify 15 provided scenarios as BUILD A SKILL, JUST PROMPT, or MAYBE using the four-criteria framework: Is it repeatable? Does it need consistent output? Can you write clear rules? Does it require background context? For every scenario you mark as skill-worthy, write a one-sentence trigger description. Then identify 3 tasks from your own work that deserve skills.

**What You'll Learn:**

- The four criteria for deciding when to build a skill
- How to write effective trigger descriptions in YAML frontmatter
- How to apply the skill-worthiness framework to your own tasks

**Starter Prompt:**

> "Read `scenarios.md` and help me classify each scenario. For each one, let's decide together: build a skill, just prompt, or maybe? Use the four criteria from the decision framework."

**Reflection Questions:**

1. Which criterion was hardest to evaluate? Why?
2. Of the 3 tasks you identified from your own work, which one would save the most time?
3. What's the risk of building a skill for something that changes frequently?

---

<ExerciseCard id="1.3" title="Skill vs. Raw Prompt: A Side-by-Side Comparison" />

### Exercise 1.3 — Skill vs. Raw Prompt: A Side-by-Side Comparison

**The Problem:**
You intuitively sense that skills produce better output than raw prompts, but you haven't measured the difference. This exercise makes the difference concrete and measurable.

**Your Task:**
Generate a weekly status report from provided data twice: once with your best raw prompt (no skill), and once with the provided weekly-report skill. Score both outputs on a 7-criteria rubric (format, metrics, blocked items, conciseness, action language, tone, send-worthiness). Then improve the skill based on what you learned and run a third round.

**What You'll Learn:**

- The measurable difference between prompted and skilled output
- How skills enforce consistency that prompts cannot
- The iteration cycle: test, evaluate, improve

**Starter Prompt:**

> "I'm going to generate a weekly report two ways. First, let me write my own prompt. Then we'll use the skill. Help me compare the outputs on the rubric in `comparison.md`."

**Reflection Questions:**

1. How many points did the skill version score higher than the raw prompt?
2. What did the skill enforce that your prompt missed?
3. After improving the skill, did every rubric score go up, or did fixing one thing break another?

---

## Module 2: Your First Skills

Now you build. These three exercises each produce a complete, working skill for a common task. The focus is on writing clear instructions, good trigger descriptions, and testing against real inputs.

<ExerciseCard id="2.1" title="Email Style Guide" />

### Exercise 2.1 — Email Style Guide

**The Problem:**
Every time you ask Claude to write an email, it sounds like Claude. You want emails that match YOUR voice: your greeting style, your formality level, your sentence patterns, your sign-off.

**Your Task:**
Analyze 5 sample emails (provided) to identify your writing patterns. Then create a `my-email-style/SKILL.md` that captures those patterns as concrete rules. Test it against provided prompts, compare output to your real style, and iterate at least 2 rounds until Claude writes emails that sound like you.

**What You'll Learn:**

- How to extract implicit style rules from examples
- How to write style constraints that are specific enough for Claude to follow
- The difference between vague rules ("write naturally") and actionable ones ("open with first name, no greeting word")

**Starter Prompt:**

> "Analyze the 5 emails in `my-emails/` and identify my writing patterns. What's my greeting style, formality level, sentence length, and sign-off? We'll turn these into a SKILL.md."

**Reflection Questions:**

1. Which style rule was hardest to capture in writing?
2. How many iteration rounds did it take before the output felt like "you"?

---

<ExerciseCard id="2.2" title="File Organization Skill" />

### Exercise 2.2 — File Organization Skill

**The Problem:**
You keep asking Claude to organize files, and every time you re-explain your preferences from scratch: folder structure, naming conventions, handling duplicates, creating changelogs.

**Your Task:**
Define your file organization rules, then encode them into a `file-organizer/SKILL.md`. Test against provided sample files first, then against edge cases designed to break simple rules: files with no extension, very long names, special characters, empty files, deeply nested duplicates. Improve and re-test.

**What You'll Learn:**

- How to handle edge cases in skill instructions
- The importance of explicit rules for ambiguous situations (what happens with duplicates? unknown file types?)
- How to write skills that work on unexpected inputs

**Starter Prompt:**

> "Help me define my file organization rules. I'll answer your questions about folder structure, naming conventions, and edge cases. Then we'll write it as a SKILL.md and test it."

**Reflection Questions:**

1. How many edge cases did the first version of your skill miss?
2. What category of files was hardest to write rules for?

---

<ExerciseCard id="2.3" title="Data Cleaning Skill" />

### Exercise 2.3 — Data Cleaning Skill

**The Problem:**
You regularly receive CSV files with inconsistent formatting: mixed date formats, random capitalization, duplicate rows, missing values. You want Claude to clean them the same way every time.

**Your Task:**
Study two provided messy CSV files and catalog every data quality issue. Write specific cleaning rules (not "fix the dates" but "convert all dates to YYYY-MM-DD; if ambiguous, assume MM/DD/YYYY"). Build `data-cleaner/SKILL.md` with rules for dates, names, phone numbers, email addresses, and deduplication. Require a preview before changes and a change log after. Test on both files, then test on a surprise third file you haven't seen.

**What You'll Learn:**

- How to write precise, unambiguous transformation rules
- The value of requiring preview-before-execution in destructive operations
- How well your rules generalize to unseen data

**Starter Prompt:**

> "Open `messy-data/customers.csv` and `messy-data/transactions.csv`. Help me catalog every data quality issue. We'll write specific cleaning rules for each issue type."

**Reflection Questions:**

1. Did your skill handle the surprise dataset without modifications?
2. What's the risk of overly strict cleaning rules?

---

## Module 3: Skills with Examples

Rules tell Claude what to do. Examples show Claude what "good" looks like. These exercises teach you to use examples, templates, and tone samples to dramatically improve skill output consistency.

<ExerciseCard id="3.1" title="Report Formatter" />

### Exercise 3.1 — Report Formatter

**The Problem:**
You write monthly stakeholder reports. The format, tone, and structure should be identical every month, but Claude gives you slightly different formatting, section ordering, and detail levels each time.

**Your Task:**
Run the same report data through two versions of a skill: v1 (rules only, no examples) and v2 (same rules plus an example of a perfect report). Compare the outputs. Then build your own v3 with two examples plus anti-examples showing what NOT to produce. Test on new data.

**What You'll Learn:**

- How examples constrain output more effectively than rules alone
- The power of anti-examples (showing what NOT to do)
- How two good examples create a tighter output range than pages of written rules

**Starter Prompt:**

> "Run `report-data/february-data/` through `skill-v1/SKILL.md` and save the output. Then run the same data through `skill-v2/SKILL.md`. Let's compare them side by side."

**Reflection Questions:**

1. How much closer was v2's output to what you'd actually send to stakeholders?
2. Did adding anti-examples in v3 catch problems that positive examples missed?
3. At what point do more examples stop helping?

---

<ExerciseCard id="3.2" title="Meeting Minutes" />

### Exercise 3.2 — Meeting Minutes

**The Problem:**
Raw meeting notes need to become polished minutes. The output should follow an exact template, extract action items consistently, and exclude off-topic chatter. You need a skill with a separate template file that's easy to update.

**Your Task:**
Build a complete skill folder: `SKILL.md` (instructions referencing the template), `templates/minutes-template.md` (the output format), and `examples/` (an input/output pair). Your SKILL.md tells Claude to read the template, study the example, then transform raw notes following specific rules for inclusion, exclusion, and action item extraction. Test on three provided meetings. Then extend the skill to produce two outputs: formal minutes and a quick Slack summary.

**What You'll Learn:**

- How to structure a multi-file skill (SKILL.md + templates + examples)
- How reference files keep the SKILL.md clean and templates easy to update
- The concept of multi-output skills (same input, different formats)

**Starter Prompt:**

> "Let's build a meeting minutes skill with a separate template file. Read the raw meetings in `raw-meetings/` and the starter template in `starter-template.md`. Help me create the full skill folder structure."

**Reflection Questions:**

1. Was the template easier to maintain as a separate file than embedded in SKILL.md?
2. How did the Slack summary differ from the formal minutes in what it included?
3. Would this skill work for a meeting you attended recently?

---

<ExerciseCard id="3.3" title="Feedback Writer" />

### Exercise 3.3 — Feedback Writer

**The Problem:**
Writing student or employee feedback is hard because tone matters enormously. "Be warm but honest" means different things to different people. You need a skill that captures YOUR specific version of warm-but-honest.

**Your Task:**
Study four tone examples (harsh, soft, corporate, ideal) and build a skill that references the ideal example as the target tone and the others as anti-patterns. Define feedback structure (strengths, areas for improvement, specific examples, encouragement) with rules requiring specificity and personalization. Test on five student profiles. The ultimate test: can a classmate tell which feedback was skill-generated vs. hand-written?

**What You'll Learn:**

- How to use tone examples (both positive and negative) to control voice
- Why specificity rules prevent generic output ("Great job!" is never acceptable)
- How anti-pattern examples are as valuable as positive examples

**Starter Prompt:**

> "Read the four tone examples in `tone-examples/`. Let's analyze what makes `ideal.md` work and what's wrong with the other three. Then we'll build a feedback skill that nails the right tone every time."

**Reflection Questions:**

1. Which anti-pattern (harsh, soft, corporate) was hardest to avoid in generated feedback?
2. Did the skill produce feedback that felt personal for each student, or did it start sounding generic?
3. What additional rules would you add after seeing the outputs?

---

## Module 4: Skills with References

Real-world skills often need to consult external documents: brand guides, policy documents, teaching standards. These exercises teach you to build skills that reference material too long to embed directly in the SKILL.md.

<ExerciseCard id="4.1" title="Brand Voice Enforcer" />

### Exercise 4.1 — Brand Voice Enforcer

**The Problem:**
A company (fictional "NovaTech") has a detailed brand guide that governs all communications. Instead of re-reading the guide every time you create content, you build a skill that automatically applies the brand voice.

**Your Task:**
Study the provided brand guide, then build a `brand-voice-skill/` with a SKILL.md that tells Claude to read the brand guide from `references/` before writing anything. Define when to activate (any NovaTech content), how to apply voice rules, how to handle different content types (technical vs. marketing), and include a brand compliance checklist. Test on five content tasks. Then extend the skill with a compliance review mode that scores existing content against the brand guide.

**What You'll Learn:**

- How to structure skills that reference external documents
- The pattern: read reference first, then apply rules
- How to build dual-mode skills (create mode + review mode)

**Starter Prompt:**

> "Read `brand-guide-source/brand-guide.md` thoroughly. Then help me build a skill that enforces NovaTech's brand voice. The skill should reference the brand guide and include a compliance checklist."

**Reflection Questions:**

1. How did the skill handle the difference between technical and marketing content?
2. Was the compliance review mode useful, or did it feel like overkill?
3. Could this pattern work for any organization's brand guide?

---

<ExerciseCard id="4.2" title="Policy Compliance Checker" />

### Exercise 4.2 — Policy Compliance Checker

**The Problem:**
Your company has an AI Usage Policy. Before any AI-generated content goes public, it must pass compliance. Checking manually is slow and inconsistent. You build a skill that automates the check.

**Your Task:**
Build a `policy-checker-skill/` that reads content, reads the policy document from `references/`, checks each policy requirement, and produces a compliance scorecard with PASS/FAIL per requirement. Flag specific violations with quotes from the content and suggest fixes. Test on three content samples (at least one should pass, at least one should fail). Then add a "fix it" mode that rewrites violating content to bring it into compliance.

**What You'll Learn:**

- How to build checklist-style skills that evaluate against criteria
- The scorecard output pattern (PASS/FAIL with evidence)
- How to extend a checker skill into a fixer skill

**Starter Prompt:**

> "Read `policy-document/ai-usage-policy.md`. Let's build a skill that checks any content against this policy and produces a compliance scorecard. I want PASS/FAIL for each requirement with specific quotes if it fails."

**Reflection Questions:**

1. Did the skill catch violations you might have missed manually?
2. Was the "fix it" mode able to preserve the original message while achieving compliance?
3. How would you adapt this pattern for a different policy (HR, legal, safety)?

---

<ExerciseCard id="4.3" title="Curriculum Standards Alignment" />

### Exercise 4.3 — Curriculum Standards Alignment

**The Problem:**
You're creating course materials that must align with a teaching standards framework (the "Seven Pillars of AI-Driven Development"). Instead of manually checking alignment for every lesson, you build a skill that enforces it automatically.

**Your Task:**
Build a `curriculum-skill/` with SKILL.md, a reference to the Seven Pillars standards document, and a lesson plan template. The skill should generate aligned lesson plans, create exercises targeting specific pillars, review existing content for alignment, and produce an alignment matrix. Test by generating three lesson plans, then run a gap analysis across all three to identify which pillars are well-covered and which are underrepresented.

**What You'll Learn:**

- How to build skills that enforce standards alignment
- The alignment matrix pattern (content vs. criteria grid)
- How gap analysis reveals blind spots in curriculum design

**Starter Prompt:**

> "Read `standards/seven-pillars.md` and `templates/lesson-plan.md`. Build a skill that generates lesson plans aligned with the Seven Pillars and can check existing content for coverage gaps."

**Reflection Questions:**

1. Which pillars were naturally easy to cover? Which required deliberate effort?
2. Did the gap analysis reveal anything surprising about your lesson plans?
3. Could this alignment pattern work for other frameworks (Bloom's Taxonomy, CEFR levels)?

---

## Module 5: Testing and Iteration

Building a skill is half the work. Making it reliable is the other half. These exercises develop your ability to find where skills break, measure improvement, and get feedback from others.

<ExerciseCard id="5.1" title="Edge Case Hunt" />

### Exercise 5.1 — Edge Case Hunt

**The Problem:**
You have an invoice processing skill that works on standard invoices. But invoices in the real world are messy: handwritten notes, multiple currencies, missing fields, unusual formats. Your job is to break the skill, then fix it.

**Your Task:**
Test the provided invoice skill against normal invoices (should work) and tricky invoices (will break). Document every failure. Then create 3 new invoice files specifically designed to break the skill in ways the provided tricky set didn't. Fix the skill to handle all edge cases and verify the fix works across every test invoice.

**What You'll Learn:**

- How to think adversarially about your own skills
- How to write edge cases that expose assumptions in instructions
- The improvement cycle: break, document, fix, verify

**Starter Prompt:**

> "Test `invoice-skill/SKILL.md` on the invoices in `invoices/normal/` first. Then try the ones in `invoices/tricky/`. Let's document every failure and figure out why it breaks."

**Reflection Questions:**

1. What assumptions did the original skill make that the edge cases violated?
2. Were your custom edge cases harder to fix than the provided tricky ones?
3. How many rules did you need to add to handle all edge cases?

---

<ExerciseCard id="5.2" title="Before/After: Measuring Improvement" />

### Exercise 5.2 — Before/After: Measuring Improvement

**The Problem:**
You changed a skill, but did it actually get better? Without measurement, you're guessing. This exercise teaches you to use a fixed rubric and fixed test cases to isolate the effect of skill changes.

**Your Task:**
Run five customer emails through v1 of an email response skill and score each response on the provided rubric. Identify the three biggest weaknesses. Build v2 addressing those weaknesses. Re-run the exact same five emails through v2 using the exact same rubric. Compare: Did scores improve? Did fixing one thing break another?

**What You'll Learn:**

- How to measure skill quality with a fixed rubric
- The discipline of controlled testing (same inputs, same rubric, only the skill changes)
- How fixing one weakness can inadvertently introduce another

**Starter Prompt:**

> "Run all 5 emails in `customer-emails/` through `email-response-skill-v1/SKILL.md`. Score each response using `rubric.md`. Let's record the scores in `scorecard.md`."

**Reflection Questions:**

1. Did every score improve from v1 to v2, or did some go down?
2. What was the biggest single improvement between versions?
3. How confident are you that v2 is genuinely better, not just different?

---

<ExerciseCard id="5.3" title="User Testing: Does Your Skill Work for Others?" />

### Exercise 5.3 — User Testing: Does Your Skill Work for Others?

**The Problem:**
A skill that works for you might confuse someone else. The ultimate quality test is handing your skill to another person and seeing if they can use it without help.

**Your Task:**
Package your best skill from previous exercises with a README, reference files, and sample test prompts. Swap with a classmate. Test their skill: run their test prompts, then create 2 new prompts of your own. Score on output correctness, quality, usability, and suggestions for improvement. Receive their feedback on your skill and create a v2.

If working solo: wait 24 hours, then re-read your SKILL.md with fresh eyes. The time gap creates enough mental distance to see your own skill's weaknesses.

**What You'll Learn:**

- Why skills need clear README documentation
- How other people interpret your instructions differently than you intended
- The gap between "works for me" and "works for anyone"

**Starter Prompt:**

> "Help me package my best skill for someone else to use. I need a README explaining what the skill does, how to use it, and 3 sample test prompts."

**Reflection Questions:**

1. What did the other person find confusing that seemed obvious to you?
2. How much did you have to change based on their feedback?
3. What would you add to every future skill's README based on this experience?

---

## Module 6: Composing Skills

Individual skills are useful. Skills that work together are powerful. These exercises teach you to chain skills into pipelines, organize them into libraries, and build skill sets for teams.

<ExerciseCard id="6.1" title="Skill Pipeline" />

### Exercise 6.1 — Skill Pipeline

**The Problem:**
A single task often involves multiple steps: raw meeting notes become formatted minutes, which become action items, which become team notifications. Instead of running each skill manually, you build a pipeline that chains them together.

**Your Task:**
Build (or reuse from earlier exercises) three skills: Meeting Minutes, Action Item Extractor, and Task Notifier. Then create a `pipeline-skill/SKILL.md` that orchestrates all three: reads raw notes, passes output from each step to the next, and produces a final summary. Test on provided meeting notes. Then add error handling: what if there are no action items? What if an item has no assignee?

**What You'll Learn:**

- How to chain skill outputs into inputs for the next skill
- The orchestrator pattern: a skill that delegates to other skills
- How to handle errors and edge cases in multi-step workflows

**Starter Prompt:**

> "Let's build a meeting notes pipeline. We need three skills that chain together: raw notes into minutes, minutes into action items, action items into team notifications. Start by showing me the data flow."

**Reflection Questions:**

1. How is a pipeline skill different from the skills it orchestrates?
2. What happens when one skill in the chain produces unexpected output?
3. How might this pattern connect to agent delegation workflows? (You'll explore this in Lesson 11.)

---

<ExerciseCard id="6.2" title="Skill Library" />

### Exercise 6.2 — Skill Library

**The Problem:**
You've built skills across multiple exercises. They're scattered in different folders with different structures. You need to organize them into a reusable personal library.

**Your Task:**
Inventory every skill you've built. Standardize each to a consistent structure (SKILL.md, README.md, optional examples/templates/references folders). Create a `LIBRARY.md` index categorizing skills by type (Writing, Data, Workflow). Pick your top 3 and polish them to production-ready: complete documentation, at least 2 test cases each, all edge cases handled. Package them so someone else could use them without explanation.

**What You'll Learn:**

- How to organize skills into a maintainable library
- The standard skill folder structure
- What "production-ready" means for a skill (complete, tested, documented)

**Starter Prompt:**

> "Help me inventory all the skills I've built across these exercises. For each one, note: name, what it does, completeness level (draft/tested/production-ready), and whether it needs other files."

**Reflection Questions:**

1. How many of your skills were truly production-ready without additional work?
2. What pattern did you notice across your skill READMEs after standardizing them?
3. Which category (writing, data, workflow) has the most skills? Is that where your real needs are?

---

<ExerciseCard id="6.3" title="Team Skills" />

### Exercise 6.3 — Team Skills

**The Problem:**
A design agency has 5 roles that each need AI skills: Creative Director, Senior Designer, Junior Designer, Project Manager, and Admin. The skills must work together, using consistent terminology and cross-referencing where relevant.

**Your Task:**
Build one skill for each role: Creative Brief, Design Feedback, Asset Checklist, Status Update, and Client Invoice. Each must be usable by someone unfamiliar with it. Skills should share consistent project and team terminology and cross-reference each other where relevant (the invoice skill references project names from the status update skill). Create a `LIBRARY.md` indexing all five with usage guides.

**What You'll Learn:**

- How to build interconnected skills that share context
- The importance of consistent terminology across a skill set
- How team skills differ from personal skills (clarity over brevity)

**Starter Prompt:**

> "We're building AI skills for a 5-person design agency. Each role gets one skill. Let's start with the Creative Director's Creative Brief skill, since other skills will reference its outputs."

**Reflection Questions:**

1. Which skill was most dependent on the others? Which was most independent?
2. How did you handle consistent terminology across all five skills?
3. What would break if you changed a project name in one skill but not the others?

---

## Module 7: Real-World Skills

These exercises simulate building skills for actual business needs. The quality bar is higher: a non-technical person should be able to use each skill by reading only its README.

<ExerciseCard id="7.1" title="Invoice Processor" />

### Exercise 7.1 — Invoice Processor

**The Problem:**
You need a production-ready skill for extracting data from invoices in multiple formats (text, structured, informal). It must handle multiple currencies, detect anomalies (negative amounts, missing dates, duplicates), generate both a data file and a summary report, and include clear error messages when extraction fails.

**Your Task:**
Build a complete `invoice-processor/` skill. Test against 10 provided invoices of varying complexity. Compare your extraction against expected output for the first 5 (provided), then do a blind test on invoices 6-10. Polish until a bookkeeper could use it with zero additional instructions.

**What You'll Learn:**

- How to handle messy, real-world input formats
- The importance of anomaly detection and error reporting
- What "production-ready" actually means in practice

**Starter Prompt:**

> "Let's build a production invoice processor. Start with a basic SKILL.md, test it against the first 5 invoices in `invoices/`, and compare to `expected-output.csv`. We'll iterate from there."

**Reflection Questions:**

1. Which invoice format was hardest to handle? Why?
2. How many iteration rounds did it take to match the expected output?
3. Would you trust this skill to process invoices without reviewing every output?

---

<ExerciseCard id="7.2" title="Content Creation Pipeline" />

### Exercise 7.2 — Content Creation Pipeline

**The Problem:**
A single topic needs to become a complete content package: blog post, LinkedIn posts, Twitter/X thread, and email newsletter blurb. All outputs must convey the same core message but be adapted to each channel's length, tone, and formatting conventions.

**Your Task:**
Build a `content-pipeline-skill/` that takes a topic brief (subject, key points, audience, CTA) and produces all four content types. The skill must reference provided channel guidelines and avoid patterns shown in anti-examples. Test on 3 content briefs and verify cross-channel message consistency.

**What You'll Learn:**

- How to build multi-output skills that adapt content to different channels
- The balance between consistency (same message) and adaptation (different format)
- How anti-examples prevent specific failure patterns

**Starter Prompt:**

> "Read `channel-guidelines.md` and `anti-examples.md`. Then let's build a content pipeline skill that takes a brief and produces blog post, LinkedIn, Twitter, and email content. Start with `content-briefs/` brief #1."

**Reflection Questions:**

1. Which channel was hardest to adapt to while keeping the core message?
2. Did the anti-examples prevent real problems, or were they unnecessary?
3. How would you extend this skill to support additional channels?

---

<ExerciseCard id="7.3" title="Research Analyst" />

### Exercise 7.3 — Research Analyst

**The Problem:**
You need a skill that conducts structured research on any topic and produces a comprehensive analysis document with citations, key themes, disagreements, and a confidence assessment of the findings.

**Your Task:**
Build a `research-skill/` that accepts a research question, defines scope, gathers information from provided documents or web search, synthesizes findings, and produces a structured research memo. The output must follow the provided analysis template and include a confidence assessment. Test across three research questions with different source requirements (documents only, web only, combined).

**What You'll Learn:**

- How to build skills that handle variable input sources
- The research memo pattern with confidence assessment
- How to write skills that know when to use tools (web search) vs. provided materials

**Starter Prompt:**

> "Read `analysis-template.md` and the first research question in `research-questions/`. Build a research skill that can gather from source documents, web search, or both, and produce a structured analysis."

**Reflection Questions:**

1. How did the confidence assessment differ between document-based and web-based research?
2. Did the skill handle combining multiple source types gracefully?
3. Would a senior analyst consider this output a useful starting point?

---

## Module 8: Capstone Projects

These capstones bring everything together. Each asks you to build a complete, interconnected skill suite for a real scenario. Choose one based on your interests, or tackle all three.

<ExerciseCard id="A" title="Business Operations Suite" />

### Capstone A — Business Operations Suite

**The Mission:**
Build 6 interconnected skills for Pixel Perfect Design Studio, a 5-person graphic design agency. Skills must share consistent context, cross-reference each other, and include a pipeline orchestrator.

**Skills to Build:**

1. **Client Onboarding** — Welcome email, project questionnaire, timeline template
2. **Invoice Generator** — Professional invoices from project data with company details and payment terms
3. **Weekly Status** — Client status updates from task data, different format per client
4. **Project Retrospective** — End-of-project analysis: what went well, improvements, key metrics
5. **Portfolio Case Study** — Convert completed projects into case studies: challenge, approach, results
6. **Pipeline Orchestrator** — Chain: project completes, generate retrospective, generate case study, update portfolio

**Quality Bar:** All skills share terminology, each has 2+ test cases, the pipeline chains at least 3 skills, and a non-technical person could use any skill from its README alone.

**What You'll Learn:**

- How to build a coherent skill ecosystem where skills reference each other
- Pipeline orchestration across multiple skills
- Production-quality documentation and testing

---

<ExerciseCard id="B" title="AI-Native Education Kit" />

### Capstone B — AI-Native Education Kit

**The Mission:**
Build 6 skills that power a course delivery system. These help instructors create, deliver, and assess content consistently while maintaining alignment with a teaching standards framework.

**Skills to Build:**

1. **Lesson Plan Generator** — Structured plans aligned with Seven Pillars framework
2. **Exercise Builder** — Hands-on exercises from learning objectives, with setup, tasks, and criteria
3. **Quiz Generator** — 5 multiple choice + 3 short answer, with answer keys and explanations
4. **Student Feedback Writer** — Personalized feedback in the ideal tone, referencing tone examples
5. **Curriculum Alignment Checker** — Coverage matrix of all Seven Pillars across lessons, with gap analysis
6. **Course Packager** — Orchestrates: Lesson Plan, Exercise, Quiz for each topic, then runs Alignment Checker

**Reference Materials:** Seven Pillars standards document, tone guide, and a sample lesson plan are provided.

**What You'll Learn:**

- How to build skills that enforce educational standards
- Orchestrating content generation with quality checking
- Building for a team of instructors, not just yourself

---

<ExerciseCard id="C" title="Personal AI Productivity Set" />

### Capstone C — Personal AI Productivity Set

**The Mission:**
This capstone is different. Instead of a provided scenario, you build skills for YOUR real life. Audit your own recurring tasks, select the top 5, build complete skills for each, and measure the actual time saved.

**Your Process:**

1. **Audit** — List 10+ tasks you do repeatedly, noting frequency and time cost
2. **Prioritize** — Select top 5 based on frequency, time cost, consistency need, and skill potential
3. **Build** — Create a complete skill for each (SKILL.md, README, examples, tests)
4. **Test** — Run at least 3 test cases per skill and iterate
5. **Library** — Create a `LIBRARY.md` indexing all 5 skills
6. **Measure** — Estimate time before vs. after for each skill and calculate weekly time saved

**What Makes This Special:**
Unlike Capstones A and B, these skills solve YOUR actual problems. This is where skill-building stops being academic and becomes genuinely useful. The skills you build here are ones you'll keep using after this course.

**What You'll Learn:**

- How to identify skill-worthy tasks in your own workflow
- The full skill lifecycle: audit, build, test, measure
- The real ROI of investing time in skill building

---

## What Comes Next

You've built skills from scratch, tested them systematically, composed them into pipelines, and created complete skill suites for real scenarios. You've gone from understanding what a SKILL.md is to building production-ready skill libraries.

Next in **Lesson 11: Subagents and Orchestration**, you'll learn how Claude delegates complex tasks to specialized sub-agents — the same skill composition principles you practiced here, but automated. Then in **Lesson 12: MCP Integration**, you'll connect your skills to external tools and services through the Model Context Protocol.
