This lesson provides 13 hands-on exercises across 5 modules plus capstone projects, bridging the gap between understanding computation and data extraction workflows and executing them on real financial data. Three core skills are practiced throughout: **data processing tool building** (creating stdin/stdout Python utilities with correct decimal handling and CSV parsing), **script debugging and verification** (finding logic bugs that hide behind exit code 0), and **pipeline orchestration** (connecting tools into verified multi-step workflows). Every exercise uses the seven-step Data Processing Framework: Understand, Build, Test, Verify, Edge Cases, Pipeline, Permanent.

The modules progress from arithmetic fundamentals and stdin tools (Module 1: Arithmetic & Stdin Tools) through zero-trust verification (Module 2: Testing & Verification), real-world CSV processing (Module 3: CSV Processing), regex-based categorization (Module 4: Categorization & Patterns), and multi-step pipelines (Module 5: Pipeline Orchestration), culminating in three capstone projects that build genuinely useful financial tools. Early modules provide starter and better prompts to scaffold learning; later modules remove prompts so students design their own approaches.

Students assess their work using a five-criteria rubric covering decimal handling, verification practices, CSV processing, pattern matching, and pipeline design. These exercises prepare students for building automated data processing agents in later chapters, where the verified, composable tools practiced here become the foundation for autonomous workflows.
