# Chapter 55: Observability & Cost Engineering - Implementation Plan

**Generated by**: chapter-planner v2.0.0 (Reasoning-Activated)
**Source Spec**: specs/chapter-55-observability-cost-engineering/spec.md
**Expertise Skill**: .claude/skills/building-with-observability/SKILL.md
**Created**: 2025-12-30
**Constitution**: v6.0.0 (Reasoning Mode)

---

## I. Chapter Analysis

### Chapter Type
**Technical/Code-Focused** — Learning objectives use "implement/create/configure/instrument" verbs. Code examples required throughout (Helm commands, PromQL queries, Python instrumentation, YAML manifests). Hands-on labs deploying Prometheus, Grafana, Jaeger, Loki, OpenCost.

### Concept Density Analysis

**Core Concepts** (from spec): 12 major concepts
1. Three Pillars of Observability (metrics, traces, logs)
2. Prometheus architecture and PromQL
3. Grafana dashboards and visualization
4. OpenTelemetry instrumentation
5. Distributed tracing with Jaeger
6. Centralized logging with Loki/LogQL
7. SLI/SLO/SLA definitions
8. Error budgets and burn rates
9. Multi-burn-rate alerting
10. OpenCost and FinOps principles
11. Cost allocation and right-sizing
12. Dapr observability integration

**Complexity Assessment**: Complex (12 interconnected concepts requiring production tooling)

**Proficiency Tier**: B1 (Intermediate - from spec and chapter-index.md)

**Justified Lesson Count**: 11 lessons (L00-L10)
- **L00 (Skill Building)**: 1 lesson - Build observability-cost-engineer skill
- **Layer 1 (Conceptual Foundation)**: 1 lesson - Three Pillars + 4 Golden Signals
- **Layer 2 (AI Collaboration)**: 7 lessons - Hands-on implementation with AI
- **Layer 3 (Intelligence Design)**: 1 lesson - Dapr integration patterns
- **Layer 4 (Capstone)**: 1 lesson - Full stack integration

**Reasoning**: 12 core concepts across 11 lessons = ~1.1 concepts primary focus per lesson. This respects B1 cognitive load limits (2-4 new concepts max per lesson including secondary concepts). Complex tooling (Prometheus, Grafana, OpenTelemetry, Loki) requires dedicated lessons for proper hands-on practice.

---

## II. Success Evals (from Spec)

**Predefined Success Criteria** (evals-first requirement):
1. **SC-001**: Students can install the complete observability stack in under 30 minutes
2. **SC-002**: Students can instrument FastAPI with OpenTelemetry and see traces in Jaeger within 15 minutes
3. **SC-003**: Students can write PromQL queries for the 4 golden signals
4. **SC-004**: Students can define an SLO and create a multi-burn-rate alert rule
5. **SC-005**: Students can identify top 3 cost drivers using OpenCost
6. **SC-006**: Students can create a dashboard showing SLO compliance and error budget
7. **SC-007**: 90% complete capstone with working observability stack
8. **SC-008**: Students can explain when to use metrics vs traces vs logs

**All lessons below map to these evals.**

---

## III. Pedagogical Arc

```
Foundation ──────────────────────────────────────────────────────────────────────────► Mastery

L00         L01              L02-L05               L06-L07            L08-L09         L10
Skill       Conceptual       Implementation        SRE                Cost +          Integration
Building    Foundation       Practice              Foundations        Dapr            Capstone

[Create     [Understand      [Deploy + Query       [Define SLOs,      [FinOps +       [Full Stack
 Skill]      3 Pillars]       each tool]           Alerting]          Dapr Obs]        Deploy]
```

---

## IV. Lesson Sequence

### L00: Build Your Observability Skill (15 min)
**Layer**: L3 (Skill Building) - Skill-First Pattern
**Proficiency**: B1
**DACA Source**: N/A (skills-lab pattern)

**Learning Objective**: Create `observability-cost-engineer` skill using natural conversation with Claude

**Cognitive Load Assessment**:
- **New Concepts**: 1 (skill-first learning pattern)
- **Assessment**: Single concept, within B1 limits

**Content Elements**:
1. Clone skills-lab repository fresh
2. Write LEARNING-SPEC.md for observability goals
3. `/fetching-library-docs` for Prometheus, OpenTelemetry, Loki docs
4. `/skill-creator` to build initial skill
5. Test skill with basic metrics query

**Maps to Evals**: FR-018

**Key Concepts from Expertise Skill**: N/A (meta-skill creation)

**Estimated Word Count**: 800-1000

---

### L01: Three Pillars of Observability (20 min)
**Layer**: L1 (Manual/Conceptual Foundation)
**Proficiency**: B1
**DACA Source**: N/A (conceptual foundation - new content)

**Learning Objectives**:
- Explain the three pillars (metrics, traces, logs) and their complementary nature
- Describe the 4 Golden Signals (latency, traffic, errors, saturation)
- Choose the right signal for different debugging scenarios

**Cognitive Load Assessment**:
- **New Concepts**: 4 (metrics pillar, traces pillar, logs pillar, 4 golden signals)
- **Assessment**: 4 concepts, within B1 limits (2-4 per lesson)

**Content Elements**:
1. Why observability matters for AI applications
2. Three pillars comparison table (from expertise skill):
   | Pillar | Tool | Query Language | Purpose |
   |--------|------|----------------|---------|
   | Metrics | Prometheus | PromQL | Aggregated numerical data |
   | Traces | Jaeger | - | Request flow across services |
   | Logs | Loki | LogQL | Detailed event records |
3. 4 Golden Signals explained with Task API examples
4. Decision matrix: which signal for which scenario (from expertise skill)
5. NO AI collaboration yet (Layer 1 - manual understanding first)

**Maps to Evals**: SC-008

**Key Concepts from Expertise Skill**:
- Three pillars table
- Decision logic: "When to Use Each Tool"
- 4 Golden Signals concept

**Safety Notes from Expertise Skill**: N/A (conceptual lesson)

**Estimated Word Count**: 2000-2500

**"Reflect on Your Skill" Section**: Test if skill can explain three pillars correctly

---

### L02: Metrics with Prometheus (30 min)
**Layer**: L2 (AI Collaboration)
**Proficiency**: B1
**DACA Source**: `05_Observability/Install-Prometheus-Grafana.md`

**Learning Objectives**:
- Install kube-prometheus-stack via Helm
- Understand Prometheus architecture (scraping, TSDB, rules)
- Write basic PromQL queries (selectors, aggregations)
- Add metrics to Task API with `prometheus_client`
- Create ServiceMonitor for custom applications

**Cognitive Load Assessment**:
- **New Concepts**: 4 (Prometheus architecture, PromQL basics, prometheus_client, ServiceMonitor)
- **Assessment**: 4 concepts, at B1 limit

**Three Roles Demonstrations** (Layer 2 Required):
1. **AI as Teacher**: Claude suggests PromQL query patterns student didn't know (histogram_quantile for percentiles)
2. **AI as Student**: Student provides Task API context, Claude adapts ServiceMonitor to their specific label scheme
3. **AI as Co-Worker**: Iterate on recording rules together - Claude suggests optimization, student validates business logic

**Content Elements**:
1. Helm installation commands (from DACA source)
2. Prometheus architecture diagram (from expertise skill)
3. PromQL fundamentals with examples (from expertise skill):
   ```promql
   sum(rate(http_requests_total[5m])) by (service)
   histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))
   ```
4. Adding metrics to FastAPI (from expertise skill TaskManager example)
5. ServiceMonitor CRD creation (from expertise skill)
6. Recording rules for efficient queries

**Maps to Evals**: SC-001, SC-003

**Key Concepts from Expertise Skill**:
- Prometheus + Grafana architecture diagram
- PromQL queries for Kubernetes
- TaskManager metrics example
- ServiceMonitor YAML template

**Safety Notes from Expertise Skill**:
- NEVER alert on every metric (causes alert fatigue)
- ALWAYS start with 4 golden signals

**Estimated Word Count**: 3000-3500

**"Reflect on Your Skill" Section**: Test if skill can generate PromQL for golden signals

---

### L03: Visualization with Grafana (25 min)
**Layer**: L2 (AI Collaboration)
**Proficiency**: B1
**DACA Source**: `05_Observability/Install-Prometheus-Grafana.md` (Grafana sections)

**Learning Objectives**:
- Understand Grafana dashboard JSON model
- Create panels for the 4 golden signals
- Use dashboard variables for multi-service views
- Import and customize community dashboards

**Cognitive Load Assessment**:
- **New Concepts**: 3 (dashboard JSON, panel types, variables)
- **Assessment**: 3 concepts, within B1 limits

**Three Roles Demonstrations**:
1. **AI as Teacher**: Claude explains Grafana panel types and when to use each (gauge vs graph vs stat)
2. **AI as Student**: Student specifies Task API metrics, Claude adapts dashboard to match
3. **AI as Co-Worker**: Collaborate on threshold colors and alert annotations

**Content Elements**:
1. Grafana datasource configuration (from DACA source)
2. Dashboard JSON structure (from expertise skill SLO Dashboard example)
3. Panel creation for latency, errors, traffic, saturation
4. Variables: `$namespace`, `$service`
5. Community dashboard import (e.g., Kubernetes cluster monitoring)
6. Dashboard best practices: don't overload, consistent colors

**Maps to Evals**: SC-006

**Key Concepts from Expertise Skill**:
- SLO Dashboard JSON template
- Grafana panel configuration patterns

**Estimated Word Count**: 2500-3000

**"Reflect on Your Skill" Section**: Test if skill can generate dashboard JSON for SLO tracking

---

### L04: Distributed Tracing with OpenTelemetry & Jaeger (30 min)
**Layer**: L2 (AI Collaboration)
**Proficiency**: B1
**DACA Source**: `05_Observability/Enable-Dapr-Metrics-and-Tracing.md` (tracing sections)

**Learning Objectives**:
- Understand OpenTelemetry concepts (traces, spans, context propagation)
- Instrument FastAPI with OpenTelemetry SDK
- Create custom spans for business operations
- Deploy Jaeger and visualize traces
- Apply sampling strategies for production

**Cognitive Load Assessment**:
- **New Concepts**: 4 (traces/spans, context propagation, OTel SDK, sampling)
- **Assessment**: 4 concepts, at B1 limit

**Three Roles Demonstrations**:
1. **AI as Teacher**: Claude explains span context propagation - how trace_id flows through HTTP headers
2. **AI as Student**: Student describes Task API flow, Claude adapts instrumentation
3. **AI as Co-Worker**: Iterate on custom span attributes for business context

**Content Elements**:
1. OpenTelemetry architecture diagram (from expertise skill)
2. Python SDK instrumentation (from expertise skill):
   ```python
   from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
   FastAPIInstrumentor.instrument_app(app)
   ```
3. Custom span creation for task operations
4. Jaeger Helm deployment (from expertise skill)
5. Trace analysis: finding bottlenecks
6. Sampling strategies: 1% prod vs 100% dev (from expertise skill safety notes)

**Maps to Evals**: SC-002

**Key Concepts from Expertise Skill**:
- OpenTelemetry tracing architecture
- FastAPI instrumentation code
- Jaeger Helm installation command

**Safety Notes from Expertise Skill**:
- NEVER use sampling rate 1.0 in high-traffic production (performance impact)

**Estimated Word Count**: 3000-3500

**"Reflect on Your Skill" Section**: Test if skill can instrument a FastAPI endpoint correctly

---

### L05: Centralized Logging with Loki (25 min)
**Layer**: L2 (AI Collaboration)
**Proficiency**: B1
**DACA Source**: `05_Observability/Centralized-Logging-with-Loki-or-EFK.md`

**Learning Objectives**:
- Understand Loki architecture (index labels, not content)
- Install Loki stack with Promtail
- Write LogQL queries (stream selectors, filters, parsers)
- Implement structured logging in Python
- Correlate logs with traces via trace_id

**Cognitive Load Assessment**:
- **New Concepts**: 3 (Loki architecture, LogQL, log-trace correlation)
- **Assessment**: 3 concepts, within B1 limits

**Three Roles Demonstrations**:
1. **AI as Teacher**: Claude explains Loki's label-first philosophy vs Elasticsearch full-text
2. **AI as Student**: Student provides log format, Claude adapts Promtail scrape config
3. **AI as Co-Worker**: Build LogQL queries together for error investigation

**Content Elements**:
1. Loki architecture (from DACA source)
2. Loki-stack Helm installation (from expertise skill)
3. LogQL query patterns (from expertise skill):
   ```logql
   {namespace="default"} |= "error"
   {namespace="default"} | json | level="error"
   ```
4. Structured logging in Python (JSON format)
5. Promtail configuration (from DACA source)
6. Log-trace correlation: adding trace_id to logs
7. Retention and storage considerations

**Maps to Evals**: SC-001

**Key Concepts from Expertise Skill**:
- LogQL queries
- Loki Helm installation

**Estimated Word Count**: 2500-3000

**"Reflect on Your Skill" Section**: Test if skill can write LogQL for error investigation

---

### L06: SRE Foundations - SLIs, SLOs, and Error Budgets (30 min)
**Layer**: L1 (Conceptual) transitioning to L2 (AI Collaboration)
**Proficiency**: B1
**DACA Source**: `06_Load_Testing_and_Capacity_Planning/Define-SLOs-and-SLAs.md`

**Learning Objectives**:
- Define SLI, SLO, and SLA with concrete examples
- Choose good SLIs (availability, latency, correctness)
- Set realistic SLO targets based on service type
- Calculate error budgets and interpret burn rates
- Create SLO recording rules in Prometheus

**Cognitive Load Assessment**:
- **New Concepts**: 4 (SLI, SLO, SLA, error budget)
- **Assessment**: 4 concepts, at B1 limit

**Three Roles Demonstrations**:
1. **AI as Teacher**: Claude explains error budget math (99.9% SLO = 43.2 min/month budget)
2. **AI as Student**: Student provides Task API requirements, Claude suggests appropriate SLO target
3. **AI as Co-Worker**: Iterate on SLI definitions - Claude proposes, student validates against business needs

**Content Elements**:
1. SLI/SLO/SLA definitions with examples (from DACA source)
2. Choosing SLIs: availability vs latency vs correctness
3. SLO target selection table (from expertise skill):
   | Service Type | Typical SLO | Error Budget (30 days) |
   |-------------|-------------|------------------------|
   | User-facing API | 99.9% | 43.2 minutes |
   | Internal service | 99.5% | 3.6 hours |
4. Error budget calculation walkthrough
5. Prometheus recording rules for SLO tracking
6. Grafana SLO dashboard (from expertise skill JSON template)

**Maps to Evals**: SC-004, SC-006

**Key Concepts from Expertise Skill**:
- SLO target selection table
- SLO recording rules
- Error budget burn rate formulas

**Estimated Word Count**: 3000-3500

**"Reflect on Your Skill" Section**: Test if skill can calculate error budget from SLO target

---

### L07: Alerting and Incident Response (25 min)
**Layer**: L2 (AI Collaboration)
**Proficiency**: B1
**DACA Source**: N/A (new content - alerting patterns)

**Learning Objectives**:
- Implement multi-window, multi-burn-rate alerting (Google SRE pattern)
- Create PrometheusRule CRDs for alert definitions
- Configure Alertmanager routing
- Apply alert hygiene principles
- Structure incident runbooks

**Cognitive Load Assessment**:
- **New Concepts**: 3 (burn-rate alerting, PrometheusRule, runbooks)
- **Assessment**: 3 concepts, within B1 limits

**Three Roles Demonstrations**:
1. **AI as Teacher**: Claude explains why multi-burn-rate alerting prevents both missed incidents and alert fatigue
2. **AI as Student**: Student provides Task API SLO, Claude adapts alert thresholds
3. **AI as Co-Worker**: Build runbook together - Claude suggests structure, student adds domain specifics

**Content Elements**:
1. Alerting strategy decision tree (from expertise skill)
2. Multi-burn-rate alerting pattern (from expertise skill SLO Alert Rules):
   ```yaml
   - alert: TaskAPIHighErrorBudgetBurn
     expr: task_api:error_budget_burn_rate:5m > 14.4 * 0.001
   ```
3. PrometheusRule CRD structure
4. Alertmanager routing and notification
5. Alert hygiene: severity levels, actionability
6. Runbook template: symptoms, investigation, remediation
7. Post-incident review process

**Maps to Evals**: SC-004

**Key Concepts from Expertise Skill**:
- Multi-burn-rate alerting YAML
- Alerting decision tree
- NEVER alert on every metric

**Safety Notes from Expertise Skill**:
- NEVER set SLOs at 100% (impossible, blocks releases)
- Alert fatigue kills teams

**Estimated Word Count**: 2500-3000

**"Reflect on Your Skill" Section**: Test if skill can generate multi-burn-rate alert rules

---

### L08: Cost Engineering and FinOps (30 min)
**Layer**: L2 (AI Collaboration)
**Proficiency**: B1
**DACA Source**: `06_Load_Testing_and_Capacity_Planning/Resource-Sizing-Guidelines.md`, `03_Planetary-Scale-Deployment/03_Advanced_Autoscaling_and_Cost_Optimization/Use-of-Spot-Instances.md`

**Learning Objectives**:
- Apply FinOps principles (visibility, optimization, operation)
- Deploy OpenCost for Kubernetes cost monitoring
- Tag resources for cost allocation by team/product
- Identify waste: idle resources, over-provisioning
- Use VPA recommendations for right-sizing
- Create cost dashboards and budget alerts

**Cognitive Load Assessment**:
- **New Concepts**: 4 (FinOps principles, OpenCost, cost allocation, right-sizing)
- **Assessment**: 4 concepts, at B1 limit

**Three Roles Demonstrations**:
1. **AI as Teacher**: Claude explains cloud cost anatomy (compute vs storage vs network)
2. **AI as Student**: Student describes Task API resource usage, Claude suggests right-sizing
3. **AI as Co-Worker**: Build cost allocation strategy together - labels, namespaces, reports

**Content Elements**:
1. FinOps principles overview
2. OpenCost Helm deployment (from expertise skill)
3. Cost allocation labels (from expertise skill):
   ```yaml
   labels:
     cost-center: "platform"
     team: "agents"
     environment: "production"
   ```
4. Waste identification patterns
5. VPA recommendations interpretation
6. OpenCost PromQL queries (from expertise skill):
   ```promql
   sum(container_cpu_allocation * on(node) group_left() node_cpu_hourly_cost * 24) by (namespace)
   ```
7. Cost dashboards and budget alerts
8. Scheduling non-production environments (40h vs 168h = 75% savings)

**Maps to Evals**: SC-005

**Key Concepts from Expertise Skill**:
- Cost allocation labels
- Right-sizing resources
- OpenCost PromQL queries

**Safety Notes from Expertise Skill**:
- Set budget alerts at 80% and 100%
- Tag ALL resources for cost allocation

**Estimated Word Count**: 3000-3500

**"Reflect on Your Skill" Section**: Test if skill can identify cost optimization opportunities

---

### L09: Dapr Observability Integration (25 min)
**Layer**: L2/L3 (AI Collaboration + Intelligence Design)
**Proficiency**: B1
**DACA Source**: `05_Observability/Enable-Dapr-Metrics-and-Tracing.md`

**Learning Objectives**:
- Configure Dapr metrics via Configuration CRD
- Create ServiceMonitor for Dapr sidecars
- Set up Dapr tracing with OpenTelemetry Collector
- Observe Dapr Actors and Workflows
- Correlate app traces with Dapr traces

**Cognitive Load Assessment**:
- **New Concepts**: 3 (Dapr metrics config, Dapr tracing, actor/workflow observability)
- **Assessment**: 3 concepts, within B1 limits

**Three Roles Demonstrations**:
1. **AI as Teacher**: Claude explains Dapr's built-in observability hooks
2. **AI as Student**: Student provides actor workflow, Claude suggests specific spans to monitor
3. **AI as Co-Worker**: Design correlated dashboard showing app + Dapr metrics together

**Content Elements**:
1. Dapr Configuration CRD for metrics/tracing (from expertise skill):
   ```yaml
   apiVersion: dapr.io/v1alpha1
   kind: Configuration
   metadata:
     name: dapr-observability
   spec:
     tracing:
       samplingRate: "1"
       otel:
         endpointAddress: jaeger-collector.monitoring:4317
     metric:
       enabled: true
   ```
2. ServiceMonitor for Dapr sidecars (from DACA source)
3. Dapr actor metrics: activation count, method latency
4. Dapr workflow metrics: step duration, failure rates
5. Trace correlation: app trace_id through Dapr sidecar
6. Unified dashboard: app + Dapr in single view

**Maps to Evals**: FR-015, FR-016, FR-017

**Key Concepts from Expertise Skill**:
- Dapr observability Configuration CRD
- ServiceMonitor for Dapr

**Estimated Word Count**: 2500-3000

**"Reflect on Your Skill" Section**: Test if skill can configure Dapr observability correctly

---

### L10: Capstone - Full Observability Stack for Task API (40 min)
**Layer**: L4 (Spec-Driven Integration)
**Proficiency**: B1
**DACA Source**: All previous sources integrated

**Learning Objectives**:
- Deploy complete observability stack via Helm (spec-first approach)
- Instrument Task API with metrics, traces, and structured logs
- Define SLOs for Task API (99.9% availability, P95 < 200ms)
- Create comprehensive SLO dashboard
- Configure multi-burn-rate alerts
- Apply cost allocation labels
- Finalize and test `observability-cost-engineer` skill

**Cognitive Load Assessment**:
- **New Concepts**: 1 (integration patterns - all other concepts are reinforcement)
- **Assessment**: Integration lesson, synthesizes previous concepts

**Content Elements**:
1. **Specification Writing** (PRIMARY SKILL - Layer 4):
   - Write SPEC.md for Task API observability requirements
   - Define success criteria: traces visible in 15 min, dashboards showing 4 golden signals
2. **Component Composition**:
   - Stack: Prometheus + Grafana + Loki + Jaeger + OpenCost
   - Which expertise skill patterns apply to each component?
3. **AI Orchestration**:
   - Compose accumulated knowledge into deployment
   - Validate against spec: "Can I see traces? Can I query logs? Is SLO dashboard working?"
4. **Convergence Loop**:
   - Iteration 1: Deploy stack, identify gaps
   - Iteration 2: Add custom metrics, refine dashboards
   - Final: Working observability for production AI application
5. **Skill Finalization**:
   - Test skill with novel scenario
   - Verify skill produces correct configs from lessons learned

**Maps to Evals**: SC-001, SC-002, SC-003, SC-004, SC-005, SC-006, SC-007 (ALL)

**Key Concepts from Expertise Skill**:
- Complete TaskManager observability example
- All patterns integrated

**Estimated Word Count**: 3500-4000

**"Reflect on Your Skill" Section**: Final skill assessment - can it deploy full observability stack?

---

## V. Skill Dependencies

**Skill Dependency Graph**:
```
Three Pillars (L01) ─┬─► Prometheus (L02) ─► Grafana (L03)
                     ├─► OpenTelemetry (L04)
                     └─► Loki (L05)

SRE Foundations (L06) ─► Alerting (L07)

Cost Engineering (L08) [independent]

Dapr Observability (L09) ─► requires Ch53 Dapr Core knowledge

Capstone (L10) ─► requires ALL previous lessons
```

**Cross-Chapter Dependencies**:
- **Ch49**: Docker basics for container resource understanding
- **Ch50**: Kubernetes for deploying observability stack
- **Ch51**: Helm for installing kube-prometheus-stack, Loki, Jaeger
- **Ch53**: Dapr for L09 (Dapr observability)
- **Ch54**: GitOps for declarative stack deployment

**Validation**: Chapter-index.md shows Ch49, Ch50, Ch51, Ch52, Ch54 implemented. Ch53 (Dapr Core) planned but not blocking - L09 can teach Dapr observability assuming Ch53 content.

---

## VI. Content Dependencies and Ordering

| Lesson | Depends On | Produces |
|--------|------------|----------|
| L00 | skills-lab | `observability-cost-engineer` skill |
| L01 | None | Conceptual foundation |
| L02 | L01 | Prometheus deployed, PromQL knowledge |
| L03 | L02 | Grafana dashboards |
| L04 | L01 | OpenTelemetry instrumentation, Jaeger |
| L05 | L01 | Loki logging, LogQL |
| L06 | L02, L03 | SLO definitions, error budgets |
| L07 | L06 | Alerting rules, runbooks |
| L08 | None | Cost visibility, FinOps |
| L09 | L02, L04, Ch53 | Dapr observability |
| L10 | ALL | Integrated observability stack |

---

## VII. Assessment Plan

### Formative Assessments (During Lessons)
- **L01**: Scenario quiz - which pillar for which problem?
- **L02**: PromQL exercise - write queries for 4 golden signals
- **L03**: Dashboard creation - build Task API SLO panel
- **L04**: Instrumentation exercise - add custom span to Task API
- **L05**: LogQL exercise - query errors with trace correlation
- **L06**: Error budget calculation - given SLO, calculate budget
- **L07**: Alert creation - multi-burn-rate for Task API
- **L08**: Cost identification - find top 3 cost drivers

### Summative Assessment (End of Chapter)
- **L10 Capstone**: Deploy complete observability stack for Task API
  - Success: All SC-001 through SC-007 achieved
  - Observable: Traces in Jaeger, metrics in Prometheus, logs in Loki, costs in OpenCost, SLO dashboard working

---

## VIII. Validation Checklist

**Chapter-Level Validation**:
- [x] Chapter type identified: Technical/Code-Focused
- [x] Concept density analysis documented: 12 core concepts
- [x] Lesson count justified: 11 lessons (L00-L10) for 12 concepts at B1
- [x] All evals from spec covered by lessons
- [x] All lessons map to at least one eval

**Stage Progression Validation**:
- [x] L00: Skill-First Pattern (meta-skill)
- [x] L01: Layer 1 (Conceptual, no AI collaboration)
- [x] L02-L05: Layer 2 (AI Collaboration with Three Roles)
- [x] L06-L09: Layer 2/L3 (AI Collaboration + Intelligence building)
- [x] L10: Layer 4 (Spec-Driven Integration)
- [x] No spec-first before Layer 4

**Cognitive Load Validation**:
- [x] Each lesson's concept count within B1 limits (2-4 new concepts)
- [x] L01: 4 concepts (pillars + golden signals)
- [x] L02: 4 concepts (architecture, PromQL, client, ServiceMonitor)
- [x] L03: 3 concepts (JSON, panels, variables)
- [x] L04: 4 concepts (traces, propagation, SDK, sampling)
- [x] L05: 3 concepts (architecture, LogQL, correlation)
- [x] L06: 4 concepts (SLI, SLO, SLA, error budget)
- [x] L07: 3 concepts (burn-rate, PrometheusRule, runbooks)
- [x] L08: 4 concepts (FinOps, OpenCost, allocation, right-sizing)
- [x] L09: 3 concepts (Dapr metrics, tracing, actor obs)
- [x] L10: 1 concept (integration - reinforcement of all prior)

**Three Roles Validation** (Layer 2 lessons):
- [x] Each Layer 2 lesson (L02-L09) demonstrates AI as Teacher
- [x] Each Layer 2 lesson demonstrates AI as Student
- [x] Each Layer 2 lesson demonstrates AI as Co-Worker (convergence)

**Expertise Skill Alignment**:
- [x] Key patterns from `.claude/skills/building-with-observability/SKILL.md` mapped to lessons
- [x] Safety notes (alert fatigue, sampling, 100% SLO) incorporated
- [x] Decision logic (when to use each tool) used in L01
- [x] Code examples (PromQL, LogQL, YAML) sourced from skill

**DACA Source Mapping**:
- [x] L02: Install-Prometheus-Grafana.md
- [x] L03: Install-Prometheus-Grafana.md (Grafana sections)
- [x] L04: Enable-Dapr-Metrics-and-Tracing.md (tracing)
- [x] L05: Centralized-Logging-with-Loki-or-EFK.md
- [x] L06: Define-SLOs-and-SLAs.md
- [x] L08: Resource-Sizing-Guidelines.md + Use-of-Spot-Instances.md
- [x] L09: Enable-Dapr-Metrics-and-Tracing.md

---

## IX. Word Count Summary

| Lesson | Target Words | Time (min) |
|--------|--------------|------------|
| L00 | 800-1000 | 15 |
| L01 | 2000-2500 | 20 |
| L02 | 3000-3500 | 30 |
| L03 | 2500-3000 | 25 |
| L04 | 3000-3500 | 30 |
| L05 | 2500-3000 | 25 |
| L06 | 3000-3500 | 30 |
| L07 | 2500-3000 | 25 |
| L08 | 3000-3500 | 30 |
| L09 | 2500-3000 | 25 |
| L10 | 3500-4000 | 40 |
| **Total** | **28,300-34,000** | **295** |

---

## X. Implementation Notes

### Running Example Consistency
All lessons use **Task API** as the running example, consistent with Part 7 pattern:
- Prometheus metrics for Task API endpoints
- OpenTelemetry traces for task creation/completion
- Structured logs for task events
- SLOs for Task API availability and latency
- Cost allocation for task-api namespace

### Quality Reference
Match quality standard of:
- `/Users/mjs/Documents/code/panaversity-official/tutorsgpt/p7-c/apps/learn-app/docs/07-AI-Cloud-Native-Development/54-cicd-gitops-argocd/01-cicd-concepts-automated-pipeline.md`

### Key Differentiators from Generic Observability Tutorials
1. **AI-Native Focus**: Observability for AI agent systems (LLM latency, token costs)
2. **Skill-First**: Students own `observability-cost-engineer` skill by chapter end
3. **Dapr Integration**: Unique coverage of Dapr sidecar observability
4. **FinOps for AI**: Cost engineering specifically for AI workloads (API costs, model costs)
5. **Three Roles Throughout**: Every L2 lesson shows bidirectional AI collaboration

---

**Plan Status**: Ready for Implementation
**Next Step**: Invoke content-implementer subagent for L00, then L01-L10 sequentially
