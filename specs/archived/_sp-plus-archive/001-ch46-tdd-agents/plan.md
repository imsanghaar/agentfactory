# Chapter 46: TDD for Agents — Implementation Plan

**Generated by**: chapter-planner v2.0.0 (Reasoning-Activated)
**Source Spec**: specs/001-ch46-tdd-agents/spec.md
**Expertise Skill**: .claude/skills/testing-ai-agents/SKILL.md
**Created**: 2025-12-30
**Constitution**: v6.0.0 (Reasoning Mode)

---

## I. Chapter Analysis

### Chapter Type

**Technical/Code-Focused** — Recognition signals:
- Learning objectives use "apply/create/implement" verbs
- Heavy code examples required (pytest, fixtures, mocking)
- Hands-on exercises with measurable outputs (tests pass, coverage reports)
- Running example (Task API) requires working code

### Concept Density Analysis

**Core Concepts** (from spec): 12 concepts

1. TDD philosophy and distinction from Evals
2. pytest-asyncio configuration and patterns
3. Async test fixtures and event loops
4. httpx AsyncClient with ASGITransport
5. FastAPI dependency injection overrides
6. In-memory SQLite with StaticPool
7. SQLModel operation testing
8. respx HTTP mocking for LLM APIs
9. Mock response structures (OpenAI/Anthropic)
10. Agent tool isolation testing
11. Integration test patterns (full pipeline)
12. CI/CD test automation (GitHub Actions)

**Complexity Assessment**: Complex (12 interconnected concepts across multiple domains)

**Proficiency Tier**: B1-B2 (from spec and chapter-index.md Part 6 Phase 4)
- B1 for foundational patterns (L00-L02)
- B2 for advanced patterns (L05-L08)

**Justified Lesson Count**: 9 lessons (L00-L08)

| Layer | Lessons | Purpose |
|-------|---------|---------|
| L1 (Manual Foundation) | L00, L01, L02 | Skill creation, philosophy, pytest basics |
| L2 (AI Collaboration) | L03, L04, L05 | Endpoint testing, DB testing, LLM mocking |
| L2/L3 (Collaboration/Skill) | L06 | Tool isolation testing |
| L3 (Intelligence Design) | L07 | Integration test patterns |
| L4 (Spec-Driven Capstone) | L08 | Full test suite |

**Reasoning**: 9 lessons justified by:
- 12 core concepts at B1-B2 proficiency require spreading across multiple lessons
- Each lesson targets 2-3 new concepts (respecting B1 limit of 7-10 concepts cumulative)
- Skill-First pattern requires dedicated L00
- Integration + Capstone require separate lessons for proper scope
- Matches spec lesson structure exactly

---

## II. Success Evals (from Spec)

**Predefined Success Criteria** (evals-first requirement):

| ID | Criterion | Measurable Outcome | Business Goal |
|----|-----------|-------------------|---------------|
| SC-001 | Fast tests | Students can run `pytest` with all tests passing in <10 seconds | Rapid feedback loop |
| SC-002 | Coverage | Students achieve 80%+ code coverage | Production quality |
| SC-003 | Test velocity | Students write tests for new endpoints in <5 minutes | Developer efficiency |
| SC-004 | Zero API calls | Zero LLM API calls during test execution | Cost control |
| SC-005 | Skill ownership | Students have reusable `agent-tdd` skill | Digital FTE component |
| SC-006 | CI/CD automation | Tests run automatically on every PR | Continuous quality |
| SC-007 | TDD vs Evals distinction | Students explain the difference clearly | Conceptual clarity |
| SC-008 | Mock flexibility | Students can mock any LLM API response | Debugging capability |
| SC-009 | Failure diagnosis | Students diagnose why tests fail | Debugging skill |
| SC-010 | Transferability | Students add tests to any FastAPI + SQLModel project | Portable skill |

**All lessons below map to these evals.**

---

## III. Lesson Sequence

### Lesson 0: Build Your Testing Skill (Layer 1: Manual Foundation)

**Learning Objective**: Create an `agent-tdd` skill using natural conversation with Claude

**Stage**: 1 (Manual Foundation — Skill-First Pattern)

**CEFR Proficiency**: B1

**New Concepts** (count: 1):
1. Skill-First Learning Pattern (create skill before learning content)

**Cognitive Load Validation**: 1 concept << 10 limit (B1) -> WITHIN LIMIT

**Maps to Evals**: SC-005 (Skill ownership)

**Content Elements**:

1. **Step 1: Get the Skills Lab**
   - Clone `panaversity/claude-code-skills-lab`
   - Launch Claude Code in the directory

2. **Step 2: Create Your Skill**
   - Prompt template for skill creation:
     ```
     Using your skill creator skill create a new skill for testing AI agent code
     with pytest. I will use it to test FastAPI endpoints, mock LLM calls, and
     test agent pipelines from hello world to production test suites.
     Use context7 skill to study official pytest-asyncio and respx documentation
     and then build it so no self assumed knowledge.
     ```
   - Claude fetches official docs via Context7
   - Claude asks clarifying questions (testing patterns, mocking preferences)
   - Skill created at `.claude/skills/agent-tdd/`

3. **Done Statement**: "You now own an agent-tdd skill built from official documentation. The rest of this chapter teaches you what it knows—and how to make it better."

**Prerequisites**: Chapter 5 (skill-creator, fetching-library-docs)

**Estimated Time**: 15 minutes

**Output File**: `00-build-your-testing-skill.md`

---

### Lesson 1: TDD Philosophy for Agent Development (Layer 1: Manual Foundation)

**Learning Objective**: Explain why TDD matters for agent development and distinguish TDD from Evals

**Stage**: 1 (Manual Foundation)

**CEFR Proficiency**: B1

**New Concepts** (count: 3):
1. TDD philosophy (write test first, then implementation)
2. TDD vs Evals distinction (deterministic vs probabilistic)
3. Cost/speed benefits of mocked tests vs real LLM calls

**Cognitive Load Validation**: 3 concepts <= 10 limit (B1) -> WITHIN LIMIT

**Maps to Evals**: SC-007 (TDD vs Evals distinction), SC-004 (Zero API calls)

**Content Elements**:

1. **Opening Narrative**
   - The $50 test suite problem: real LLM calls cost money
   - "Tests that cost money don't run often"
   - The false confidence of manual testing

2. **TDD vs Evals Comparison Table** (from expertise skill)

| Aspect | TDD (This Chapter) | Evals (Chapter 47) |
|--------|-------------------|-------------------|
| Question | Does the code work correctly? | Does the LLM reason well? |
| Nature | Deterministic | Probabilistic |
| Output | Pass/Fail | Scores (0-1) |
| Tests | Functions, APIs, DB operations | Response quality, faithfulness |
| Speed | Fast (mocked LLM) | Slow (real LLM calls) |
| Cost | Zero (no API calls) | High (API calls required) |

3. **What TDD Tests for Agents**
   - Code correctness: Does the function return correct output?
   - API behavior: Does the endpoint handle errors?
   - Database integrity: Are constraints enforced?
   - Tool execution: Does the tool function work?
   - Pipeline flow: Do components connect correctly?

4. **What TDD Does NOT Test**
   - LLM reasoning quality (that's evals)
   - Response helpfulness (that's evals)
   - Output safety (that's evals)
   - Response faithfulness (that's evals)

5. **Try With AI Section** (3 prompts)

   **Prompt 1: Distinguish Test Types**
   ```
   I have an agent that processes customer support tickets.
   Help me identify which behaviors should be TDD tests
   and which should be evals. Don't write the tests—just
   categorize these behaviors: ticket creation, response
   quality, database updates, response tone, error handling.
   ```
   **What you're learning:** Categorizing behaviors is the first step in test planning.

   **Prompt 2: Calculate Test Cost**
   ```
   My agent test suite makes 50 LLM calls per run. Each call
   averages 2000 tokens at $0.002 per 1K tokens. If I run tests
   20 times per day, what's my monthly testing cost? How much
   would I save with mocked tests?
   ```
   **What you're learning:** Quantifying the cost of unmocked tests justifies the investment in proper test infrastructure.

   **Prompt 3: Domain Application**
   ```
   In my [your domain] application, I have an agent that [describe
   your agent's function]. Help me identify 3 things to test with
   TDD and 2 things to evaluate with evals.
   ```
   **What you're learning:** Applying the TDD/Evals distinction to your own domain solidifies understanding.

6. **Reflect on Your Skill Section**

   **Test Your Skill:**
   ```
   Using my agent-tdd skill, explain when to use TDD vs Evals.
   Does my skill correctly distinguish deterministic tests from
   probabilistic evaluations?
   ```

   **Identify Gaps:**
   - Does my skill explain the cost implications of real LLM calls?
   - Does it include the comparison table?

   **Improve Your Skill:**
   ```
   My agent-tdd skill needs clearer TDD vs Evals distinction.
   Update it to include the comparison table and cost analysis.
   ```

**Prerequisites**: None (Chapter 46 opening lesson)

**Estimated Time**: 20 minutes

**Output File**: `01-tdd-philosophy-for-agents.md`

---

### Lesson 2: pytest Fundamentals for Async Code (Layer 1: Manual Foundation)

**Learning Objective**: Configure pytest-asyncio and write async test functions with proper event loop management

**Stage**: 1 (Manual Foundation)

**CEFR Proficiency**: B1

**New Concepts** (count: 4):
1. pytest-asyncio configuration (`asyncio_mode = "auto"`)
2. Async test functions (`@pytest.mark.asyncio`)
3. Event loop scope management (session-scoped)
4. Async fixtures (`@pytest.fixture` with async)

**Cognitive Load Validation**: 4 concepts <= 10 limit (B1) -> WITHIN LIMIT

**Maps to Evals**: SC-001 (Fast tests), SC-003 (Test velocity)

**Content Elements**:

1. **Opening Narrative**
   - "Agent code is async. Your tests must be async too."
   - Why `async def` matters for testing agent APIs
   - The event loop coordination problem

2. **Project Setup** (code from expertise skill)
   ```bash
   uv add --dev pytest pytest-asyncio httpx respx pytest-cov
   ```

3. **pytest Configuration**
   ```toml
   # pyproject.toml
   [tool.pytest.ini_options]
   asyncio_mode = "auto"
   asyncio_default_fixture_loop_scope = "function"
   testpaths = ["tests"]
   ```

4. **First Async Test**
   ```python
   import pytest

   @pytest.mark.asyncio
   async def test_async_function():
       """Test an async function."""
       result = await some_async_function()
       assert result == expected
   ```

5. **Session-Scoped Event Loop** (for shared fixtures)
   ```python
   @pytest.fixture(scope="session")
   def event_loop():
       """Create event loop for session-scoped fixtures."""
       import asyncio
       loop = asyncio.get_event_loop_policy().new_event_loop()
       yield loop
       loop.close()
   ```

6. **Async Fixtures**
   ```python
   @pytest.fixture
   async def async_data():
       """Async fixture for test data."""
       data = await fetch_test_data()
       yield data
       await cleanup(data)
   ```

7. **Hands-On Exercise**
   - Create `tests/conftest.py` with event loop fixture
   - Write 3 async test functions
   - Run `pytest tests/ -v`

8. **Common Mistakes**
   - Forgetting `@pytest.mark.asyncio` decorator
   - Mixing sync and async fixtures incorrectly
   - Not using `yield` in async fixtures

9. **Try With AI Section** (3 prompts)

   **Prompt 1: Debug Event Loop Error**
   ```
   My test fails with "Event loop is closed". Here's my test:

   @pytest.mark.asyncio
   async def test_something():
       await my_async_function()

   What's causing this and how do I fix it?
   ```
   **What you're learning:** Event loop errors are the most common async testing issue.

   **Prompt 2: Fixture Scope Decision**
   ```
   I have an async database connection. Should this be a
   function-scoped or session-scoped fixture? Explain the
   trade-offs for test isolation vs performance.
   ```
   **What you're learning:** Fixture scope affects both isolation and speed.

   **Prompt 3: Convert Sync Tests**
   ```
   I have 10 synchronous tests that work. Now my code is async.
   What's the minimum change to make them async-compatible?
   Walk me through the pattern.
   ```
   **What you're learning:** Migrating existing tests to async.

10. **Reflect on Your Skill Section**

    **Test Your Skill:**
    ```
    Using my agent-tdd skill, show me how to configure pytest-asyncio.
    Does my skill include the pyproject.toml configuration and
    event loop fixture pattern?
    ```

    **Identify Gaps:**
    - Does my skill explain fixture scopes?
    - Does it cover common event loop errors?

    **Improve Your Skill:**
    ```
    My agent-tdd skill needs pytest-asyncio patterns. Add the
    pyproject.toml configuration, event loop fixture, and
    common error handling.
    ```

**Prerequisites**: L00, L01, Part 5 asyncio chapter

**Estimated Time**: 25 minutes

**Output File**: `02-pytest-async-fundamentals.md`

---

### Lesson 3: Testing FastAPI Endpoints (Layer 2: AI Collaboration)

**Learning Objective**: Test FastAPI endpoints using httpx AsyncClient with ASGITransport and dependency overrides

**Stage**: 2 (AI Collaboration — Three Roles)

**CEFR Proficiency**: B1

**New Concepts** (count: 4):
1. httpx AsyncClient for async testing
2. ASGITransport pattern for FastAPI
3. Dependency injection overrides
4. Testing all HTTP methods (GET, POST, PUT, PATCH, DELETE)

**Cognitive Load Validation**: 4 concepts <= 10 limit (B1) -> WITHIN LIMIT

**Maps to Evals**: SC-001 (Fast tests), SC-003 (Test velocity), SC-010 (Transferability)

**Three Roles Demonstrations** (REQUIRED for L2):

1. **AI as Teacher**
   - **Scenario**: Student needs to test authenticated endpoint
   - **AI suggests**: Dependency override pattern for `get_current_user`
   - **What student learns**: How to mock authentication in tests without hitting real auth

2. **AI as Student**
   - **Scenario**: AI generates complex fixture setup
   - **Student responds**: "I need simpler setup—just mock the user, don't mock the whole auth chain"
   - **AI adapts**: Simplified `get_test_user()` function

3. **AI as Co-Worker**
   - **Scenario**: Testing PUT vs PATCH semantics
   - **Iteration 1**: AI suggests testing both with same test structure
   - **Student feedback**: "PATCH should allow partial updates"
   - **Iteration 2**: AI generates separate tests showing partial update behavior
   - **Convergence**: Complete test coverage for update semantics

**Content Elements**:

1. **Opening Narrative**
   - "You built the Task API in Chapter 40. Now you test it."
   - Why httpx instead of TestClient for async

2. **httpx AsyncClient Pattern** (from expertise skill)
   ```python
   from httpx import ASGITransport, AsyncClient
   from app.main import app

   @pytest.fixture
   async def client():
       async with AsyncClient(
           transport=ASGITransport(app=app),
           base_url="http://test",
       ) as ac:
           yield ac
   ```

3. **Dependency Override Pattern**
   ```python
   from app.database import get_session
   from app.auth import get_current_user

   @pytest.fixture
   async def client():
       app.dependency_overrides[get_session] = get_test_session
       app.dependency_overrides[get_current_user] = get_test_user

       async with AsyncClient(...) as ac:
           yield ac

       app.dependency_overrides.clear()
   ```

4. **Testing All HTTP Methods**
   - GET (list, single)
   - POST (create)
   - PUT (full update)
   - PATCH (partial update)
   - DELETE (remove)

5. **Testing Error Responses**
   - 404 for missing resources
   - 422 for validation errors
   - 401 for unauthorized

6. **Complete conftest.py** (from expertise skill Pattern 1)

7. **Hands-On Exercise**
   - Clone Task API from Chapter 40
   - Create `tests/conftest.py` with full setup
   - Write tests for `/tasks` endpoints

8. **Try With AI Section** (3 prompts)

   **Prompt 1: Generate Test for New Endpoint** (AI as Teacher)
   ```
   I added a new endpoint to my Task API:

   @app.get("/tasks/{task_id}/subtasks")
   async def list_subtasks(task_id: int, session: AsyncSession = Depends(get_session)):
       ...

   Generate a test for this endpoint. Include happy path and error cases.
   ```
   **What you're learning:** AI generates test boilerplate; you verify correctness.

   **Prompt 2: Refine Generated Test** (AI as Student)
   ```
   Your generated test uses a raw ID. I want to use a fixture that
   creates a task first. Update the test to depend on a `sample_task`
   fixture instead of hardcoding ID 1.
   ```
   **What you're learning:** You teach AI your testing conventions.

   **Prompt 3: Iterate on Edge Cases** (AI as Co-Worker)
   ```
   Let's identify edge cases for the subtasks endpoint together.
   I'm thinking: empty list, pagination, invalid parent task.
   What else should we test?
   ```
   **What you're learning:** Collaborative test case discovery.

9. **Reflect on Your Skill Section**

    **Test Your Skill:**
    ```
    Using my agent-tdd skill, generate a test for a FastAPI endpoint
    with authentication. Does my skill include the dependency override
    pattern and httpx AsyncClient setup?
    ```

    **Identify Gaps:**
    - Does my skill handle ASGITransport correctly?
    - Does it include conftest.py template?

    **Improve Your Skill:**
    ```
    My agent-tdd skill needs FastAPI testing patterns. Add the
    httpx AsyncClient with ASGITransport, dependency overrides,
    and complete conftest.py template.
    ```

**Prerequisites**: L02, Chapter 40 (Task API)

**Estimated Time**: 30 minutes

**Output File**: `03-testing-fastapi-endpoints.md`

---

### Lesson 4: Testing SQLModel Operations (Layer 2: AI Collaboration)

**Learning Objective**: Test SQLModel database operations using in-memory SQLite with StaticPool

**Stage**: 2 (AI Collaboration)

**CEFR Proficiency**: B1

**New Concepts** (count: 3):
1. In-memory SQLite with StaticPool configuration
2. Model-level tests (separate from API tests)
3. Cascade and constraint testing

**Cognitive Load Validation**: 3 concepts <= 10 limit (B1) -> WITHIN LIMIT

**Maps to Evals**: SC-001 (Fast tests), SC-002 (Coverage)

**Three Roles Demonstrations**:

1. **AI as Teacher**
   - **Scenario**: Student doesn't understand StaticPool
   - **AI explains**: "StaticPool keeps a single connection open, required for in-memory SQLite to persist across transactions"
   - **What student learns**: Why standard connection pooling breaks in-memory databases

2. **AI as Student**
   - **Scenario**: AI suggests PostgreSQL-specific test
   - **Student responds**: "We're using SQLite for tests—JSONB won't work"
   - **AI adapts**: Changes to JSON field with SQLite-compatible assertions

3. **AI as Co-Worker**
   - **Iteration 1**: AI generates basic model test
   - **Student feedback**: "Add cascade delete test"
   - **Iteration 2**: AI adds cascade test
   - **Student feedback**: "Also test unique constraint"
   - **Convergence**: Complete model test suite

**Content Elements**:

1. **Opening Narrative**
   - "API tests go through HTTP. Model tests go straight to the database."
   - Why both layers need tests
   - The speed advantage of model tests

2. **In-Memory SQLite Setup** (from expertise skill Pattern 1)
   ```python
   TEST_DATABASE_URL = "sqlite+aiosqlite:///:memory:"

   test_engine = create_async_engine(
       TEST_DATABASE_URL,
       echo=False,
       poolclass=StaticPool,
       connect_args={"check_same_thread": False},
   )
   ```

3. **Database Fixture with Fresh Tables**
   ```python
   @pytest.fixture(autouse=True)
   async def setup_database():
       """Create tables before each test, drop after."""
       async with test_engine.begin() as conn:
           await conn.run_sync(SQLModel.metadata.create_all)
       yield
       async with test_engine.begin() as conn:
           await conn.run_sync(SQLModel.metadata.drop_all)
   ```

4. **Model Tests** (from expertise skill Pattern 3)
   - Create operation
   - Relationship tests
   - Cascade delete
   - Constraint violations

5. **Edge Case: PostgreSQL vs SQLite**
   - JSONB -> JSON compatibility
   - Array types workaround
   - When to use PostgreSQL test container

6. **Hands-On Exercise**
   - Create `tests/test_models.py`
   - Test Task model CRUD
   - Test cascade delete behavior
   - Test unique constraint violation

7. **Try With AI Section** (3 prompts)

   **Prompt 1: Model Test Generation** (AI as Teacher)
   ```
   Here's my Task model:

   class Task(SQLModel, table=True):
       id: int | None = Field(default=None, primary_key=True)
       title: str
       description: str | None = None
       status: str = "pending"
       project_id: int | None = Field(foreign_key="project.id")

   Generate model tests for: create, update, relationship with Project.
   ```
   **What you're learning:** AI generates model-level tests separate from API tests.

   **Prompt 2: SQLite Compatibility** (AI as Student)
   ```
   Your test uses JSONB. I'm testing with SQLite. Fix the test
   to work with both SQLite and PostgreSQL.
   ```
   **What you're learning:** You catch database-specific issues AI misses.

   **Prompt 3: Constraint Testing** (AI as Co-Worker)
   ```
   Let's add constraint tests. I have a unique constraint on
   (project_id, title). How do we test this raises the right error?
   ```
   **What you're learning:** Collaborative constraint test design.

8. **Reflect on Your Skill Section**

    **Test Your Skill:**
    ```
    Using my agent-tdd skill, show me how to test SQLModel cascade
    deletes. Does my skill include in-memory SQLite setup with StaticPool?
    ```

    **Identify Gaps:**
    - Does my skill handle SQLite vs PostgreSQL differences?
    - Does it include constraint violation testing?

    **Improve Your Skill:**
    ```
    My agent-tdd skill needs SQLModel testing patterns. Add in-memory
    SQLite setup, cascade testing, and constraint violation patterns.
    ```

**Prerequisites**: L03, Chapter 40 (SQLModel setup)

**Estimated Time**: 25 minutes

**Output File**: `04-testing-sqlmodel-operations.md`

---

### Lesson 5: Mocking LLM Calls (Layer 2: AI Collaboration)

**Learning Objective**: Mock LLM API calls using respx to achieve zero API calls in tests

**Stage**: 2 (AI Collaboration)

**CEFR Proficiency**: B2 (advanced topic)

**New Concepts** (count: 4):
1. respx HTTP mocking library
2. OpenAI API response structure mocking
3. Tool call response mocking
4. Error response mocking (429, 500, timeout)

**Cognitive Load Validation**: 4 concepts at B2 -> WITHIN LIMIT (no ceiling for B2)

**Maps to Evals**: SC-004 (Zero API calls), SC-008 (Mock flexibility)

**Three Roles Demonstrations**:

1. **AI as Teacher**
   - **Scenario**: Student doesn't know respx
   - **AI explains**: respx mocks httpx requests at transport layer, so any code using httpx (including OpenAI SDK) gets mocked
   - **What student learns**: Why respx works for LLM mocking

2. **AI as Student**
   - **Scenario**: AI mocks response with wrong structure
   - **Student responds**: "OpenAI returns `choices[0].message.content`, not `response.text`"
   - **AI adapts**: Fixes response structure to match real API

3. **AI as Co-Worker**
   - **Iteration 1**: AI generates basic mock
   - **Student feedback**: "Add tool call mock for agent that uses tools"
   - **Iteration 2**: AI adds tool call structure
   - **Student feedback**: "Now add streaming mock"
   - **Convergence**: Complete mock suite for all scenarios

**Content Elements**:

1. **Opening Narrative**
   - "Every LLM call costs money. Every mock costs nothing."
   - The key insight: respx intercepts httpx, OpenAI SDK uses httpx
   - "Zero API calls" as success criterion

2. **respx Basics** (from expertise skill Pattern 4)
   ```python
   import respx
   import httpx

   @pytest.mark.asyncio
   @respx.mock
   async def test_openai_completion():
       respx.post("https://api.openai.com/v1/chat/completions").mock(
           return_value=httpx.Response(
               200,
               json={
                   "choices": [{
                       "message": {
                           "role": "assistant",
                           "content": "Hello!"
                       }
                   }],
                   "usage": {"total_tokens": 50}
               }
           )
       )

       result = await call_openai("Say hello")
       assert "Hello" in result
   ```

3. **Mocking Tool Calls** (from expertise skill)
   ```python
   respx.post("https://api.openai.com/v1/chat/completions").mock(
       return_value=httpx.Response(200, json={
           "choices": [{
               "message": {
                   "role": "assistant",
                   "tool_calls": [{
                       "id": "call_123",
                       "function": {
                           "name": "create_task",
                           "arguments": '{"title": "New Task"}'
                       }
                   }]
               }
           }]
       })
   )
   ```

4. **Mocking Error Responses**
   - Rate limit (429)
   - Server error (500)
   - Timeout (TimeoutException)

5. **Verifying Mock Was Called**
   ```python
   assert respx.calls.call_count == 1
   ```

6. **Anthropic API Mocking**
   ```python
   respx.post("https://api.anthropic.com/v1/messages").mock(...)
   ```

7. **Hands-On Exercise**
   - Create `tests/test_agent.py`
   - Mock OpenAI completion
   - Mock tool call response
   - Mock 429 rate limit
   - Verify all tests pass with zero API calls

8. **Try With AI Section** (3 prompts)

   **Prompt 1: Generate Mock for Your Agent** (AI as Teacher)
   ```
   My agent calls OpenAI and uses these tools: create_task, search_tasks.
   Generate respx mocks that return tool calls for both tools.
   ```
   **What you're learning:** AI generates complex mock structures.

   **Prompt 2: Fix Mock Structure** (AI as Student)
   ```
   Your mock returns `message: {tool_calls: [...]}` but my code expects
   `choices[0].message.tool_calls`. Fix the mock structure.
   ```
   **What you're learning:** You validate AI's understanding of API structures.

   **Prompt 3: Add Error Scenarios** (AI as Co-Worker)
   ```
   My tests only cover happy path. Let's add error mocks:
   - Rate limit (429) with retry logic
   - Server error (500) with fallback
   - Timeout with graceful degradation

   Which should we tackle first?
   ```
   **What you're learning:** Collaborative error scenario planning.

9. **Reflect on Your Skill Section**

    **Test Your Skill:**
    ```
    Using my agent-tdd skill, show me how to mock an OpenAI API call
    with respx. Does my skill include tool call mocking and error mocking?
    ```

    **Identify Gaps:**
    - Does my skill include Anthropic API mocking?
    - Does it cover streaming response mocking?

    **Improve Your Skill:**
    ```
    My agent-tdd skill needs comprehensive LLM mocking patterns.
    Add respx mocks for OpenAI tool calls, Anthropic messages,
    error responses (429, 500), and timeouts.
    ```

**Prerequisites**: L04, Chapter 34-36 (agent SDKs)

**Estimated Time**: 30 minutes

**Output File**: `05-mocking-llm-calls.md`

---

### Lesson 6: Testing Agent Tools (Layer 2/L3: Collaboration/Skill Design)

**Learning Objective**: Test agent tool functions in isolation before testing agent orchestration

**Stage**: 2/3 (Transition — AI Collaboration to Skill Design)

**CEFR Proficiency**: B2

**New Concepts** (count: 3):
1. Tool isolation (testing function without agent)
2. Input validation testing (security)
3. External dependency mocking in tools

**Cognitive Load Validation**: 3 concepts at B2 -> WITHIN LIMIT

**Maps to Evals**: SC-002 (Coverage), SC-009 (Failure diagnosis)

**Three Roles + Skill Design Elements**:

1. **AI as Teacher**: Shows how to extract tool function for isolated testing
2. **AI as Student**: Adapts to student's security requirements for input validation
3. **AI as Co-Worker**: Iterates on tool test completeness
4. **Skill Design**: Students identify reusable patterns for their `agent-tdd` skill

**Content Elements**:

1. **Opening Narrative**
   - "When an agent fails, is it the LLM or the tool?"
   - Isolation as debugging strategy
   - Tools are just functions with special signatures

2. **Tool Isolation Pattern** (from expertise skill Pattern 6)
   ```python
   # tests/test_tools.py
   from app.tools import search_database, validate_input

   @pytest.mark.asyncio
   async def test_search_tool():
       """Test database search tool function."""
       results = await search_database(query="python")

       assert isinstance(results, list)
       assert all("python" in r["title"].lower() for r in results)
   ```

3. **Input Validation Testing**
   ```python
   def test_validate_input_rejects_injection():
       """Test input validation blocks SQL injection."""
       malicious = "'; DROP TABLE users; --"

       with pytest.raises(ValidationError):
           validate_input(malicious)
   ```

4. **Mocking External Dependencies in Tools**
   ```python
   @pytest.mark.asyncio
   @respx.mock
   async def test_tool_with_external_api():
       respx.get("https://api.example.com/data").mock(
           return_value=httpx.Response(200, json={"result": "data"})
       )

       result = await fetch_external_data("query")
       assert result == "data"
   ```

5. **Testing Tool Error Handling**
   - Tool timeout
   - Invalid response from external API
   - Database connection failure

6. **Hands-On Exercise**
   - Create `tests/test_tools.py`
   - Test each tool function in isolation
   - Add input validation tests
   - Mock external API calls

7. **Skill Design Reflection**
   - What patterns emerged from tool testing?
   - What should be added to `agent-tdd` skill?

8. **Try With AI Section** (3 prompts)

   **Prompt 1: Extract Tool for Testing** (AI as Teacher)
   ```
   My agent has this tool:

   @function_tool
   async def create_task(title: str, priority: str = "medium"):
       ...

   How do I test this function without the @function_tool decorator?
   Show me the isolation pattern.
   ```
   **What you're learning:** Separating tool logic from agent framework.

   **Prompt 2: Security Testing** (AI as Student)
   ```
   Your test doesn't check for SQL injection. My tool accepts user
   input that goes to a database query. Add security-focused tests.
   ```
   **What you're learning:** You teach AI your security requirements.

   **Prompt 3: Design Reusable Pattern** (AI as Co-Worker / Skill Design)
   ```
   I've written tool tests for 5 different tools. What patterns
   repeat? Let's design a reusable template for my agent-tdd skill.
   ```
   **What you're learning:** Extracting patterns for skill improvement.

9. **Reflect on Your Skill Section**

    **Test Your Skill:**
    ```
    Using my agent-tdd skill, show me how to test an agent tool
    in isolation. Does my skill include security testing patterns?
    ```

    **Identify Gaps:**
    - Does my skill separate tool logic from framework decorators?
    - Does it include input validation patterns?

    **Improve Your Skill:**
    ```
    My agent-tdd skill needs tool testing patterns. Add tool
    isolation, input validation security tests, and external
    dependency mocking for tools.
    ```

**Prerequisites**: L05, Chapter 34-36 (agent tools)

**Estimated Time**: 25 minutes

**Output File**: `06-testing-agent-tools.md`

---

### Lesson 7: Integration Test Patterns (Layer 3: Intelligence Design)

**Learning Objective**: Test complete agent pipelines with mocked LLM responses

**Stage**: 3 (Intelligence Design — Create Reusable Patterns)

**CEFR Proficiency**: B2

**New Concepts** (count: 3):
1. Multi-turn conversation testing
2. Database state verification after agent actions
3. Error handling path testing

**Cognitive Load Validation**: 3 concepts at B2 -> WITHIN LIMIT

**Maps to Evals**: SC-002 (Coverage), SC-009 (Failure diagnosis), SC-010 (Transferability)

**Intelligence Design Elements**:

Students create reusable integration test patterns for their `agent-tdd` skill:
- Multi-step mock sequences
- State verification patterns
- Error scenario templates

**Content Elements**:

1. **Opening Narrative**
   - "Unit tests verify parts. Integration tests verify the whole."
   - The full pipeline: user message -> LLM decides tool -> tool executes -> LLM responds
   - All with mocked LLM responses

2. **Multi-Turn Mock Sequence** (from expertise skill Pattern 7)
   ```python
   @pytest.mark.asyncio
   @respx.mock
   async def test_complete_agent_flow(client: AsyncClient):
       respx.post("https://api.openai.com/v1/chat/completions").mock(
           side_effect=[
               # First call: LLM decides to use tool
               httpx.Response(200, json={
                   "choices": [{
                       "message": {
                           "role": "assistant",
                           "tool_calls": [{
                               "id": "call_1",
                               "function": {
                                   "name": "create_task",
                                   "arguments": '{"title": "New Task"}'
                               }
                           }]
                       }
                   }]
               }),
               # Second call: LLM responds with result
               httpx.Response(200, json={
                   "choices": [{
                       "message": {
                           "role": "assistant",
                           "content": "I created the task 'New Task' for you."
                       }
                   }]
               })
           ]
       )

       response = await client.post(
           "/api/agent/chat",
           json={"message": "Create a task called 'New Task'"}
       )

       assert response.status_code == 200
       assert "created" in response.json()["response"].lower()
   ```

3. **Database State Verification**
   ```python
   # Verify task was actually created in DB
   tasks = await client.get("/api/tasks")
   assert any(t["title"] == "New Task" for t in tasks.json())
   ```

4. **Error Handling Path Testing** (from expertise skill Pattern 8)
   - LLM timeout handling
   - Malformed response handling
   - Tool execution failure

5. **Test Organization Pattern**
   ```
   tests/
   ├── conftest.py              # Shared fixtures
   ├── unit/
   │   ├── test_models.py       # SQLModel tests
   │   ├── test_tools.py        # Agent tool tests
   │   └── test_utils.py        # Utility function tests
   ├── integration/
   │   ├── test_api.py          # FastAPI endpoint tests
   │   └── test_agent.py        # Agent pipeline tests
   └── e2e/
       └── test_flows.py        # End-to-end flows
   ```

6. **Hands-On Exercise**
   - Create `tests/integration/test_agent_pipeline.py`
   - Test multi-turn conversation
   - Verify database state changes
   - Test timeout handling

7. **Skill Pattern Extraction**
   - Extract multi-step mock pattern
   - Create state verification template
   - Document error scenario patterns

8. **Try With AI Section** (3 prompts)

   **Prompt 1: Multi-Turn Sequence Design**
   ```
   My agent has a 3-step flow:
   1. User asks to create project
   2. Agent asks for project name
   3. User provides name
   4. Agent creates project

   Design the respx side_effect sequence for this flow.
   ```
   **What you're learning:** Complex mock sequences for realistic flows.

   **Prompt 2: State Verification Pattern**
   ```
   After my agent creates a task, I want to verify:
   - Task exists in database
   - Task has correct owner
   - Task appears in user's task list

   Design a state verification helper function.
   ```
   **What you're learning:** Reusable verification patterns.

   **Prompt 3: Extract Skill Pattern**
   ```
   I've written 5 integration tests. What patterns should I
   extract for my agent-tdd skill? Focus on: mock sequences,
   state verification, error scenarios.
   ```
   **What you're learning:** Creating reusable intelligence.

9. **Reflect on Your Skill Section**

    **Test Your Skill:**
    ```
    Using my agent-tdd skill, show me how to test a multi-turn
    agent conversation. Does my skill include state verification
    and error path testing?
    ```

    **Identify Gaps:**
    - Does my skill have multi-step mock patterns?
    - Does it include the test organization structure?

    **Improve Your Skill:**
    ```
    My agent-tdd skill needs integration testing patterns. Add
    multi-turn mock sequences, database state verification, error
    path testing, and recommended test organization structure.
    ```

**Prerequisites**: L06, All previous lessons

**Estimated Time**: 30 minutes

**Output File**: `07-integration-test-patterns.md`

---

### Lesson 8: Capstone — Full Test Suite (Layer 4: Spec-Driven Integration)

**Learning Objective**: Implement a comprehensive test suite for Task API using specification-first approach

**Stage**: 4 (Spec-Driven Integration)

**CEFR Proficiency**: B2

**New Concepts** (count: 3):
1. Test coverage reporting (pytest-cov)
2. Test factories for data creation
3. CI/CD integration (GitHub Actions)

**Cognitive Load Validation**: 3 concepts at B2 -> WITHIN LIMIT

**Maps to Evals**: ALL (SC-001 through SC-010)

**Spec-Driven Integration Elements**:

1. **Specification First**: Students write test specification before implementation
2. **Component Composition**: Compose all patterns from L00-L07
3. **AI Orchestration**: AI implements test suite from spec
4. **Validation**: Verify against spec success criteria

**Content Elements**:

1. **Opening Narrative**
   - "You've learned the patterns. Now compose them into a production test suite."
   - Specification-first: define what tests you need before writing them
   - Goal: 80%+ coverage, <10s runtime, zero API calls

2. **Test Specification Template**
   ```markdown
   # Task API Test Suite Specification

   ## Success Criteria
   - [ ] 80%+ code coverage
   - [ ] All tests pass in <10 seconds
   - [ ] Zero LLM API calls
   - [ ] CI/CD runs on every PR

   ## Test Categories
   1. Unit Tests (models, tools, utils)
   2. Integration Tests (API endpoints)
   3. Agent Tests (pipeline with mocked LLM)

   ## Required Tests
   - Task CRUD (create, read, update, delete)
   - Authentication flow
   - Error responses (404, 422, 401)
   - Agent chat endpoint
   - Tool execution
   ```

3. **Test Factories** (from expertise skill)
   ```python
   # tests/factories.py
   from app.models import Task, User

   async def create_test_user(session, **overrides):
       defaults = {"email": "test@example.com", "name": "Test User"}
       user = User(**{**defaults, **overrides})
       session.add(user)
       await session.commit()
       return user

   async def create_test_task(session, user_id, **overrides):
       defaults = {"title": "Test Task", "status": "pending"}
       task = Task(**{**defaults, "user_id": user_id, **overrides})
       session.add(task)
       await session.commit()
       return task
   ```

4. **Coverage Configuration**
   ```toml
   # pyproject.toml
   [tool.coverage.run]
   source = ["app"]
   omit = ["tests/*", "migrations/*"]

   [tool.coverage.report]
   fail_under = 80
   ```

5. **Running with Coverage**
   ```bash
   pytest --cov=app --cov-report=html
   ```

6. **CI/CD Integration** (from expertise skill)
   ```yaml
   # .github/workflows/test.yml
   name: Tests
   on: [push, pull_request]
   jobs:
     test:
       runs-on: ubuntu-latest
       steps:
         - uses: actions/checkout@v4
         - uses: astral-sh/setup-uv@v5
         - run: uv sync --all-extras
         - run: uv run pytest --cov --cov-report=xml
         - uses: codecov/codecov-action@v4
   ```

7. **Complete Test Suite Structure**
   ```
   tests/
   ├── conftest.py              # All shared fixtures
   ├── factories.py             # Test data factories
   ├── unit/
   │   ├── test_models.py       # 5 tests
   │   └── test_tools.py        # 4 tests
   ├── integration/
   │   ├── test_tasks.py        # 10 tests
   │   ├── test_auth.py         # 6 tests
   │   └── test_agent.py        # 4 tests
   └── e2e/
       └── test_flows.py        # 3 tests
   ```

8. **Hands-On Exercise: Build the Suite**
   - Write test specification first
   - Use AI to implement from spec
   - Run coverage report
   - Achieve 80%+ coverage
   - Set up GitHub Actions

9. **Verification Checklist**
   - [ ] `pytest` passes all tests
   - [ ] `pytest --cov` shows 80%+ coverage
   - [ ] Runtime <10 seconds
   - [ ] No network calls (respx.mock active)
   - [ ] GitHub Actions workflow runs

10. **Try With AI Section** (3 prompts)

    **Prompt 1: Implement from Spec** (Spec-Driven)
    ```
    Here's my test specification:

    [paste your test spec]

    Implement the test suite following this specification.
    Use the patterns from my agent-tdd skill.
    ```
    **What you're learning:** Specification-first test implementation.

    **Prompt 2: Achieve Coverage Target**
    ```
    My coverage report shows 72%. The uncovered lines are:

    [paste uncovered lines]

    Generate tests to cover these paths.
    ```
    **What you're learning:** Using coverage to guide test completeness.

    **Prompt 3: Optimize Test Speed**
    ```
    My test suite takes 25 seconds. Profile suggests database
    setup is slow. How can I optimize without sacrificing
    isolation?
    ```
    **What you're learning:** Performance optimization for test suites.

11. **Final Skill Reflection**

    **Test Your Complete Skill:**
    ```
    Using my agent-tdd skill, generate a complete test suite
    specification for a new FastAPI agent project. Verify my
    skill includes: pytest-asyncio setup, httpx client, respx
    mocking, SQLModel testing, tool isolation, integration
    patterns, factories, and CI/CD.
    ```

    **Finalize Your Skill:**
    ```
    Review my agent-tdd skill for completeness. Add any missing
    patterns from this chapter:
    - Test specification template
    - Factory pattern
    - Coverage configuration
    - GitHub Actions workflow
    ```

**Prerequisites**: All previous lessons (L00-L07)

**Estimated Time**: 35 minutes

**Output File**: `08-capstone-task-api-test-suite.md`

---

## IV. Skill Dependencies

**Skill Dependency Graph**:

```
[TDD Philosophy] (no prerequisites) -> L01
         |
         v
[pytest-asyncio] (requires TDD Philosophy) -> L02
         |
         v
[FastAPI Testing] (requires pytest-asyncio) -> L03
         |
         +--------------------+
         |                    |
         v                    v
[SQLModel Testing]      [LLM Mocking]
    (L04)                  (L05)
         |                    |
         +--------------------+
                   |
                   v
         [Tool Testing] (requires L04 + L05) -> L06
                   |
                   v
         [Integration Patterns] (requires all above) -> L07
                   |
                   v
         [Capstone] (requires all patterns) -> L08
```

**Cross-Chapter Dependencies**:
- Chapter 40: FastAPI for Agents (Task API codebase) - REQUIRED
- Part 5: Python fundamentals (basic pytest) - REQUIRED
- Ch34-36: Agent SDK patterns (code to test) - REQUIRED
- Validation: All prerequisites implemented per chapter-index.md -> VERIFIED

---

## V. Assessment Plan

### Formative Assessments (During Lessons)

| Lesson | Assessment | Method |
|--------|------------|--------|
| L00 | Skill creation | Student has `.claude/skills/agent-tdd/` |
| L01 | TDD vs Evals quiz | Can categorize 5 behaviors correctly |
| L02 | Async test writing | 3 async tests pass |
| L03 | Endpoint tests | Task API endpoint tests pass |
| L04 | Model tests | SQLModel tests with cascade pass |
| L05 | LLM mocking | Zero API calls in test run |
| L06 | Tool isolation | Tool tests independent of agent |
| L07 | Integration tests | Multi-turn flow test passes |
| L08 | Coverage target | 80%+ coverage achieved |

### Summative Assessment (End of Chapter)

**Chapter Quiz** (`quiz.md`):
- 10 questions covering TDD vs Evals distinction, pytest patterns, mocking
- Mix of multiple choice and code completion
- Success: 70%+ correct

**Capstone Validation**:
- `pytest --cov` achieves 80%+ coverage
- All tests pass in <10 seconds
- GitHub Actions workflow succeeds

---

## VI. Validation Checklist

**Chapter-Level Validation**:
- [x] Chapter type identified (Technical/Code-Focused)
- [x] Concept density analysis documented (12 concepts)
- [x] Lesson count justified (9 lessons, spec-aligned)
- [x] All evals from spec covered by lessons
- [x] All lessons map to at least one eval

**Stage Progression Validation**:
- [x] L00-L02: Layer 1 (Manual — skill creation, philosophy, pytest basics)
- [x] L03-L05: Layer 2 (AI Collaboration with Three Roles)
- [x] L06: Layer 2/3 (Transition — tool testing, skill extraction)
- [x] L07: Layer 3 (Intelligence Design — integration patterns)
- [x] L08: Layer 4 (Spec-Driven Capstone)
- [x] No spec-first before Layer 4

**Cognitive Load Validation**:
- [x] L00: 1 concept (skill creation)
- [x] L01: 3 concepts (TDD philosophy)
- [x] L02: 4 concepts (pytest-asyncio)
- [x] L03: 4 concepts (FastAPI testing)
- [x] L04: 3 concepts (SQLModel)
- [x] L05: 4 concepts (LLM mocking)
- [x] L06: 3 concepts (tool testing)
- [x] L07: 3 concepts (integration)
- [x] L08: 3 concepts (capstone)
- [x] All within B1 (<=10) or B2 (no limit) thresholds

**Three Roles Validation** (Layer 2 lessons):
- [x] L03: AI as Teacher (dependency override), Student (simplify fixtures), Co-Worker (PUT vs PATCH)
- [x] L04: AI as Teacher (StaticPool), Student (SQLite compat), Co-Worker (constraint tests)
- [x] L05: AI as Teacher (respx), Student (response structure), Co-Worker (error mocks)
- [x] L06: AI as Teacher (isolation), Student (security), Co-Worker (patterns)

**Skill-First Pattern Validation**:
- [x] L00 creates `agent-tdd` skill before content
- [x] Every lesson ends with "Reflect on Your Skill" section
- [x] Skill progressively improves throughout chapter

**Canonical Source Alignment**:
- [x] Skill format matches `.claude/skills/*/SKILL.md` canonical pattern
- [x] Testing patterns from expertise skill `.claude/skills/testing-ai-agents/SKILL.md`

---

## VII. Output Files

```
apps/learn-app/docs/06-AI-Native-Software-Development/46-tdd-for-agents/
├── 00-build-your-testing-skill.md     # L00 (15 min)
├── 01-tdd-philosophy-for-agents.md    # L01 (20 min)
├── 02-pytest-async-fundamentals.md    # L02 (25 min)
├── 03-testing-fastapi-endpoints.md    # L03 (30 min)
├── 04-testing-sqlmodel-operations.md  # L04 (25 min)
├── 05-mocking-llm-calls.md            # L05 (30 min)
├── 06-testing-agent-tools.md          # L06 (25 min)
├── 07-integration-test-patterns.md    # L07 (30 min)
├── 08-capstone-task-api-test-suite.md # L08 (35 min)
├── quiz.md                            # Chapter quiz
└── README.md                          # Chapter overview
```

**Total Duration**: 235 minutes (~4 hours)

---

## VIII. Implementation Tasks

### Per-Lesson Task Checklist

#### L00: Build Your Testing Skill (15 min)
- [ ] Write YAML frontmatter with skills metadata
- [ ] Document Step 1: Get the Skills Lab
- [ ] Write skill creation prompt
- [ ] Include Done statement and Next link
- [ ] Quality reference: Ch34 L00 pattern

#### L01: TDD Philosophy for Agents (20 min)
- [ ] Write compelling opening narrative
- [ ] Create TDD vs Evals comparison table
- [ ] Document what TDD tests / does NOT test
- [ ] Write 3 "Try With AI" prompts with learning explanations
- [ ] Write "Reflect on Your Skill" section

#### L02: pytest Fundamentals for Async Code (25 min)
- [ ] Document project setup with uv
- [ ] Write pytest configuration (pyproject.toml)
- [ ] Show async test examples
- [ ] Document event loop fixture
- [ ] Create hands-on exercise
- [ ] Write 3 "Try With AI" prompts
- [ ] Write "Reflect on Your Skill" section

#### L03: Testing FastAPI Endpoints (30 min)
- [ ] Write opening narrative connecting to Chapter 40
- [ ] Document httpx AsyncClient pattern
- [ ] Show dependency override pattern
- [ ] Include conftest.py template
- [ ] Document Three Roles demonstrations
- [ ] Create hands-on exercise
- [ ] Write 3 "Try With AI" prompts
- [ ] Write "Reflect on Your Skill" section

#### L04: Testing SQLModel Operations (25 min)
- [ ] Document in-memory SQLite with StaticPool
- [ ] Show database fixture with fresh tables
- [ ] Include model test examples
- [ ] Document SQLite vs PostgreSQL differences
- [ ] Document Three Roles demonstrations
- [ ] Create hands-on exercise
- [ ] Write 3 "Try With AI" prompts
- [ ] Write "Reflect on Your Skill" section

#### L05: Mocking LLM Calls (30 min)
- [ ] Write opening narrative about cost savings
- [ ] Document respx basics
- [ ] Show tool call mocking
- [ ] Document error response mocking
- [ ] Document Three Roles demonstrations
- [ ] Create hands-on exercise
- [ ] Write 3 "Try With AI" prompts
- [ ] Write "Reflect on Your Skill" section

#### L06: Testing Agent Tools (25 min)
- [ ] Document tool isolation pattern
- [ ] Show input validation testing
- [ ] Document external dependency mocking
- [ ] Include skill pattern extraction
- [ ] Create hands-on exercise
- [ ] Write 3 "Try With AI" prompts
- [ ] Write "Reflect on Your Skill" section

#### L07: Integration Test Patterns (30 min)
- [ ] Document multi-turn mock sequence
- [ ] Show database state verification
- [ ] Document error handling path testing
- [ ] Include test organization structure
- [ ] Document skill pattern extraction
- [ ] Create hands-on exercise
- [ ] Write 3 "Try With AI" prompts
- [ ] Write "Reflect on Your Skill" section

#### L08: Capstone — Full Test Suite (35 min)
- [ ] Write test specification template
- [ ] Document test factories
- [ ] Show coverage configuration
- [ ] Include GitHub Actions workflow
- [ ] Document spec-driven implementation
- [ ] Create verification checklist
- [ ] Write 3 "Try With AI" prompts
- [ ] Write final skill reflection

#### Chapter Quiz (quiz.md)
- [ ] Write 10 questions covering key concepts
- [ ] Include TDD vs Evals distinction questions
- [ ] Include pytest pattern questions
- [ ] Include mocking questions
- [ ] Provide answer key

#### README.md
- [ ] Write chapter overview
- [ ] Document Skill-First arc
- [ ] Create lesson structure table
- [ ] Document prerequisites
- [ ] Include what students will own

---

## IX. Quality Reference

**Match quality of**:
- Chapter 34 L00 (`34-openai-agents-sdk/00-build-your-openai-agents-skill.md`)
- Chapter 40 L02 (`40-fastapi-for-agents/02-pytest-fundamentals.md`)

**Key quality indicators**:
1. Full YAML frontmatter with skills metadata
2. Compelling narrative opening
3. Code examples from expertise skill
4. Three "Try With AI" prompts with "What you're learning" explanations
5. "Reflect on Your Skill" section at lesson end
6. Clear hands-on exercises

---

**Plan Version**: 1.0.0
**Constitution Alignment**: v6.0.0
**Ready for Implementation**: Yes
