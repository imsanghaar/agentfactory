# Implementation Plan: Chapter 42 - Apps SDK (Building Interactive ChatGPT Apps)

**Generated by**: chapter-planner (Reasoning-Activated)
**Source Spec**: specs/048-chapter-42-openai-apps-sdk/spec.md
**Expertise Skill**: .claude/skills/building-chatgpt-apps/SKILL.md
**Created**: 2025-12-28
**Status**: Ready for Implementation

---

## I. Chapter Analysis

### Chapter Type

**Technical/Code-Focused** - This chapter teaches developers to build ChatGPT Apps with interactive widgets. Learning objectives use "apply/create/implement" verbs, and code examples are essential throughout.

### Position in Learning Journey

- **Part**: 6 (AI Native Software Development)
- **Proficiency Level**: B1 (Intermediate)
- **Assumed Knowledge**: Chapters 33-34 (agent concepts, OpenAI Agents SDK), Chapters 37-38 (MCP fundamentals), Chapter 40 (FastAPI patterns)

### Concept Density Analysis

**Core Concepts** (from spec + skill): 8 distinct concepts

1. Three-layer architecture (ChatGPT UI, Widget iframe, MCP Server)
2. Widget resource registration (`text/html+skybridge` MIME type)
3. `window.openai` API for widget-host communication
4. Tool metadata (`openai/outputTemplate`, `openai/widgetAccessible`)
5. Response payload design (`structuredContent` vs `_meta`)
6. Widget state management (`widgetState`, `setWidgetState`)
7. Display modes (inline, pip, fullscreen)
8. Development workflow (ngrok, Developer Mode)

**Complexity Assessment**: Standard (8 concepts with moderate interconnections)

**Proficiency Tier**: B1 (from spec) - max 10 concepts per lesson

**Justified Lesson Count**: 8 lessons

- Layer 1 (Manual Foundation): 1 lesson (architecture understanding)
- Layer 2 (AI Collaboration): 4 lessons (implementation with Claude Code)
- Layer 3 (Pattern Application): 1 lesson (TaskManager widget)
- Layer 4 (Capstone): 1 lesson (full implementation + extensions)
- Assessment: 1 lesson (chapter quiz)

**Reasoning**: 8 lessons appropriate for 8 core concepts. The chapter builds on extensive prior knowledge (MCP servers, FastAPI, agent patterns) so fewer lessons needed than Chapter 34. Each Layer 2 lesson introduces 2-3 new concepts, staying within B1 limits.

---

## II. Success Evals (from Spec)

**Predefined Success Criteria** (evals-first):

1. **SC-001**: Learners can create a working ChatGPT App with widget from scratch in under 60 minutes
2. **SC-002**: Learners can explain the three-layer architecture and data flow without referring to notes
3. **SC-003**: TaskManager capstone demonstrates 4+ interactive features (add, list, complete, delete tasks)
4. **SC-004**: All code examples execute successfully when copied from the lessons
5. **SC-005**: Learners can extend the TaskManager with one additional feature independently
6. **SC-006**: 90% of learners successfully complete the ngrok + Developer Mode setup on first attempt

**All lessons below map to these evals.**

---

## III. Lesson Sequence

### Lesson 1: Apps SDK Architecture (Layer 1: Manual Foundation)

**Learning Objective**: Explain the three-layer architecture (ChatGPT UI, Widget iframe, MCP Server) and trace data flow from user prompt to widget render

**Stage**: 1 (Manual Foundation)

**CEFR Proficiency**: B1

**New Concepts** (count: 4):
1. Three-layer architecture (ChatGPT UI -> Widget iframe -> MCP Server)
2. Widget vs standard MCP tool differences
3. Data flow: prompt -> model -> tool call -> server response -> widget render -> model narration
4. Official examples repository structure (kitchen_sink)

**Cognitive Load Validation**: 4 concepts <= 10 limit -> WITHIN LIMIT

**Maps to Evals**: SC-002 (architecture understanding)

**Content Elements**:
- Architecture diagram explanation (from skill)
- Comparison table: Apps SDK vs Agents SDK vs standard MCP
- Walkthrough of kitchen_sink example structure
- Manual diagram drawing exercise (no AI yet)
- **No AI assistance** - build mental model first

**Prerequisites**:
- Chapter 33: Agent taxonomy and architecture
- Chapter 34: OpenAI Agents SDK (tools, handoffs)
- Chapter 37-38: MCP fundamentals, FastMCP server building

**Estimated Time**: 45 minutes

**File Name**: `01-apps-sdk-architecture.md`

---

### Lesson 2: Your First ChatGPT App (Layer 2: AI Collaboration)

**Learning Objective**: Build a working ChatGPT App that displays a "Hello World" widget when invoked

**Stage**: 2 (AI Collaboration with Three Roles)

**CEFR Proficiency**: B1

**New Concepts** (count: 5):
1. Project setup with FastMCP for ChatGPT Apps
2. `text/html+skybridge` MIME type for widget resources
3. Tool definition with `openai/outputTemplate` metadata
4. ngrok tunnel setup for local development
5. ChatGPT Developer Mode registration

**Cognitive Load Validation**: 5 concepts <= 10 limit -> WITHIN LIMIT

**Maps to Evals**: SC-001 (create working app), SC-004 (code executes), SC-006 (setup success)

**Three Roles Demonstrations** (REQUIRED):

1. **AI as Teacher**:
   - Scenario: Student writes basic FastMCP server without widget metadata
   - AI suggests: "For ChatGPT Apps, tools need special metadata. Add `_meta` with `openai/outputTemplate` pointing to your widget resource. Here's why it's different from standard MCP..."
   - Student learns: The critical metadata pattern for widget rendering

2. **AI as Student**:
   - Scenario: AI generates comprehensive widget with React hooks
   - Student responds: "Too complex for first example. Just use vanilla HTML/JS."
   - AI adapts: Simplifies to minimal HTML widget without build tooling

3. **AI as Co-Worker**:
   - Scenario: ngrok URL keeps changing, app disconnects
   - Iteration 1: AI suggests hardcoding URL in config
   - Student feedback: "That breaks when ngrok restarts"
   - Iteration 2: AI suggests environment variable pattern
   - Convergence: Together they establish a robust dev workflow

**Content Elements**:
- Project structure template (from skill)
- FastMCP server with widget resource (Python)
- Minimal widget HTML template
- Step-by-step ngrok + Developer Mode guide
- Troubleshooting section for common setup issues

**Prerequisites**: Lesson 1 (architecture understanding)

**Estimated Time**: 60 minutes

**File Name**: `02-first-chatgpt-app.md`

---

### Lesson 3: Widget Interactivity (Layer 2: AI Collaboration)

**Learning Objective**: Add interactive buttons to widgets that trigger `sendFollowUpMessage` and `callTool` actions

**Stage**: 2 (AI Collaboration with Three Roles)

**CEFR Proficiency**: B1

**New Concepts** (count: 4):
1. `window.openai` API availability checks (optional chaining)
2. `sendFollowUpMessage` for action buttons
3. `callTool` for tool chaining from widgets
4. `openai/widgetAccessible` metadata for enabling callTool

**Cognitive Load Validation**: 4 concepts <= 10 limit -> WITHIN LIMIT

**Maps to Evals**: SC-001 (working interactive app), SC-003 (interactive features)

**Three Roles Demonstrations** (REQUIRED):

1. **AI as Teacher**:
   - Scenario: Student tries `window.openai.sendFollowUpMessage()` directly
   - AI suggests: "Always use optional chaining: `window.openai?.sendFollowUpMessage()`. Here's why - the widget might load outside ChatGPT during development..."
   - Student learns: Defensive API usage pattern

2. **AI as Student**:
   - Scenario: AI implements callTool for every button
   - Student responds: "sendFollowUpMessage is better for suggestions, callTool for data refresh"
   - AI adapts: Restructures code to use appropriate method per action type

3. **AI as Co-Worker**:
   - Scenario: callTool works in browser console but fails in widget
   - Iteration 1: AI checks code syntax
   - Student feedback: "Syntax is fine, something about permissions"
   - Iteration 2: AI identifies missing `openai/widgetAccessible` in tool metadata
   - Convergence: Both learn the metadata-widget connection

**Content Elements**:
- Complete `window.openai` API reference table (from skill)
- Action button implementation patterns
- Tool chaining example
- API availability check patterns

**Prerequisites**: Lesson 2 (working widget)

**Estimated Time**: 50 minutes

**File Name**: `03-widget-interactivity.md`

---

### Lesson 4: Response Payload Design (Layer 2: AI Collaboration)

**Learning Objective**: Design tool responses that correctly separate model-visible data (`structuredContent`) from widget-only data (`_meta`)

**Stage**: 2 (AI Collaboration with Three Roles)

**CEFR Proficiency**: B1

**New Concepts** (count: 3):
1. `structuredContent` - concise JSON for model narration
2. `_meta` - large/sensitive data hidden from model
3. `content` - optional markdown/text for model

**Cognitive Load Validation**: 3 concepts <= 10 limit -> WITHIN LIMIT

**Maps to Evals**: SC-004 (code executes correctly), SC-005 (extend independently)

**Three Roles Demonstrations** (REQUIRED):

1. **AI as Teacher**:
   - Scenario: Student puts all data in structuredContent
   - AI suggests: "The model sees structuredContent and might include it in responses. For large datasets or sensitive info, use `_meta` - the model never sees it. Here's the separation principle..."
   - Student learns: When to use each payload field

2. **AI as Student**:
   - Scenario: AI puts minimal data in structuredContent
   - Student responds: "The model needs enough context to narrate properly"
   - AI adapts: Adds summary fields while keeping details in `_meta`

3. **AI as Co-Worker**:
   - Scenario: Widget shows wrong data - mismatch between structuredContent and _meta
   - Iteration 1: AI checks field names for typos
   - Student feedback: "Names match, but widget reads from wrong source"
   - Iteration 2: AI traces data flow from server to widget
   - Convergence: Establish clear naming convention for payload fields

**Content Elements**:
- Response payload structure diagram (from skill)
- Python/FastMCP response patterns
- Visibility comparison table
- Best practices for payload design

**Prerequisites**: Lesson 3 (widget interactivity)

**Estimated Time**: 40 minutes

**File Name**: `04-response-payload-design.md`

---

### Lesson 5: State and Display Modes (Layer 2: AI Collaboration)

**Learning Objective**: Implement widget state persistence using `widgetState`/`setWidgetState` and control display modes (inline, pip, fullscreen)

**Stage**: 2 (AI Collaboration with Three Roles)

**CEFR Proficiency**: B1

**New Concepts** (count: 5):
1. `widgetState` - reading persisted UI state
2. `setWidgetState` - synchronous state persistence
3. React hooks: `useWidgetState`, `useOpenAiGlobal` (optional advanced)
4. Display modes: inline, pip, fullscreen
5. `requestDisplayMode` API

**Cognitive Load Validation**: 5 concepts <= 10 limit -> WITHIN LIMIT

**Maps to Evals**: SC-003 (interactive features), SC-005 (extend independently)

**Three Roles Demonstrations** (REQUIRED):

1. **AI as Teacher**:
   - Scenario: Student's widget resets on every follow-up message
   - AI suggests: "Use `setWidgetState` to persist user selections. When widget reloads, read from `window.openai.widgetState`. Here's the pattern for maintaining state across tool calls..."
   - Student learns: State persistence lifecycle in widgets

2. **AI as Student**:
   - Scenario: AI implements React with full hook setup
   - Student responds: "For this lesson, keep it vanilla JS. Show React hooks as optional advanced topic."
   - AI adapts: Restructures with vanilla JS primary, React as sidebar

3. **AI as Co-Worker**:
   - Scenario: State persists but display mode doesn't work on mobile
   - Iteration 1: AI adds fullscreen button with requestDisplayMode
   - Student feedback: "Works on desktop but mobile shows pip instead of fullscreen"
   - Iteration 2: AI explains platform coercion behavior
   - Convergence: Document platform-specific display mode behavior

**Content Elements**:
- State management patterns (vanilla JS)
- React hooks reference (from skill)
- Display mode API usage
- Platform-specific behavior notes

**Prerequisites**: Lesson 4 (response payloads)

**Estimated Time**: 50 minutes

**File Name**: `05-state-display-modes.md`

---

### Lesson 6: Building TaskManager Widget (Layer 3: Pattern Application)

**Learning Objective**: Apply all learned patterns to build a TaskManager widget with task list display, add/complete/delete functionality, and state persistence

**Stage**: 3 (Pattern Application)

**CEFR Proficiency**: B1

**Patterns Applied** (from Lessons 1-5):
1. Three-layer architecture (Lesson 1)
2. FastMCP server with widget resources (Lesson 2)
3. Interactive buttons with sendFollowUpMessage/callTool (Lesson 3)
4. Proper structuredContent vs _meta separation (Lesson 4)
5. State persistence with widgetState (Lesson 5)

**Maps to Evals**: SC-001 (working app), SC-003 (4+ features), SC-004 (code executes)

**Content Elements**:
- TaskManager specification (spec-driven approach)
- MCP server with 4 tools: list_tasks, add_task, complete_task, delete_task
- Task list widget with interactive buttons
- State persistence for task selections
- Testing with Claude Code as pair programmer

**Reusable Artifact Created**: TaskManager ChatGPT App template

**Prerequisites**: Lessons 1-5 (all patterns)

**Estimated Time**: 75 minutes

**File Name**: `06-taskmanager-widget.md`

---

### Lesson 7: TaskManager Capstone (Layer 4: Spec-Driven Integration)

**Learning Objective**: Complete and polish the TaskManager implementation with testing, debugging, deployment considerations, and extension planning

**Stage**: 4 (Spec-Driven Integration)

**CEFR Proficiency**: B1

**Capstone Deliverables**:
1. Complete TaskManager implementation review
2. Debug and fix common issues
3. Deployment considerations (authentication, production URLs)
4. Future extensions (categories, due dates, OAuth)

**Maps to Evals**: SC-003 (4+ features), SC-005 (extend independently)

**Content Elements**:
1. **Implementation Review**:
   - Complete code walkthrough
   - Architecture validation against spec
2. **Testing and Debugging**:
   - Common issues from skill's troubleshooting guide
   - Caching problems and solutions
3. **Deployment Considerations**:
   - Moving from ngrok to production
   - OAuth 2.1 authentication (brief mention, not hands-on)
4. **Extensions**:
   - Adding task categories (guided exercise)
   - Adding due dates (independent challenge)
   - Authentication patterns (reference for future)

**Prerequisites**: Lesson 6 (TaskManager core)

**Estimated Time**: 60 minutes

**File Name**: `07-taskmanager-capstone.md`

---

### Lesson 8: Chapter Quiz (Assessment)

**Learning Objective**: Validate understanding of all chapter concepts through 50-question assessment

**Stage**: Assessment

**CEFR Proficiency**: B1

**Quiz Coverage**:
- Architecture concepts (10 questions)
- Widget development (15 questions)
- window.openai API (10 questions)
- Response payloads (5 questions)
- State and display modes (5 questions)
- Integration patterns (5 questions)

**Maps to Evals**: All (summative assessment)

**Content Elements**:
- 50-question Quiz component with randomized batching
- Coverage across all 7 lessons
- Immediate feedback on each question

**Prerequisites**: Lessons 1-7

**Estimated Time**: 45 minutes

**File Name**: `quiz.md`

---

## IV. Skill Dependencies

### Skill Dependency Graph

```
Chapter 33 (Agent concepts)
     |
     v
Chapter 34 (OpenAI Agents SDK - tools, runners)
     |
     v
Chapter 37-38 (MCP fundamentals, FastMCP)
     |
     v
Chapter 40 (FastAPI patterns)
     |
     v
Chapter 42 Lesson 1 (Architecture)
     |
     v
Lesson 2 (First App - builds on MCP + FastMCP)
     |
     v
Lesson 3 (Interactivity - builds on widget basics)
     |
     v
Lesson 4 (Response Payloads - builds on tool responses)
     |
     v
Lesson 5 (State/Display - builds on interactivity)
     |
     v
Lesson 6 (TaskManager - composes all patterns)
     |
     v
Lesson 7 (Capstone - polishes implementation)
     |
     v
Lesson 8 (Quiz - assesses all)
```

### Cross-Chapter Dependencies

| Prerequisite | Concepts Required | Status |
|--------------|-------------------|--------|
| Chapter 33 | Agent taxonomy, architecture patterns | Planned (in Part 6) |
| Chapter 34 | OpenAI Agents SDK - agents, tools, handoffs | Implemented |
| Chapter 37-38 | MCP fundamentals, FastMCP server building | Planned (in Part 6) |
| Chapter 40 | FastAPI patterns, HTTP endpoints | Planned (in Part 6) |

**Validation**: Cross-chapter dependencies are in same Part 6, expected to be implemented before Chapter 42.

---

## V. Assessment Plan

### Formative Assessments (During Lessons)

| Lesson | Assessment Type | Purpose |
|--------|-----------------|---------|
| Lesson 1 | Architecture diagram drawing | Validate mental model |
| Lesson 2 | Working "Hello World" widget | Confirm development setup |
| Lesson 3 | Button triggering follow-up message | Test API understanding |
| Lesson 4 | Correct data in widget vs model | Verify payload design |
| Lesson 5 | State persisting across follow-ups | Confirm state management |
| Lesson 6 | TaskManager with 4 operations | Integration test |
| Lesson 7 | Independent extension added | Mastery confirmation |

### Summative Assessment (End of Chapter)

- **Lesson 8**: 50-question quiz covering all concepts
- **Scoring**: Pass threshold 80% (40/50 questions)
- **Retry**: Unlimited attempts with question randomization

---

## VI. Content Dependencies on Expertise Skill

The `building-chatgpt-apps` skill provides:

### Architecture Patterns (Lesson 1)
- Three-layer architecture diagram
- Flow explanation: prompt -> widget render

### Code Templates (Lessons 2-6)
- FastMCP server template with widget resources
- Widget HTML template with styling
- Tool definition patterns with metadata
- Response payload structure

### API Reference (Lessons 3-5)
- Complete window.openai API table
- React hooks implementations
- Display mode methods

### Troubleshooting (Lessons 2, 7)
- Common issues and solutions table
- Caching problems
- ngrok disconnection handling

### Safety Notes (All lessons)
- Never embed secrets in payloads
- Always check window.openai existence
- Token validation patterns (for OAuth section)

---

## VII. Pedagogical Arc

```
Foundation           Practice              Integration           Mastery
    |                   |                      |                    |
 Lesson 1            Lessons 2-5            Lesson 6            Lesson 7
 (L1: Manual)        (L2: AI Collab)        (L3: Apply)         (L4: Capstone)
    |                   |                      |                    |
 Architecture        Build skills           Compose all          Polish &
 understanding       with Claude Code       into TaskManager     extend
```

### Learning Flow

1. **Lesson 1 (Foundation)**: Build mental model of three-layer architecture without AI assistance
2. **Lessons 2-5 (Skill Building)**: Develop each capability with Claude Code as pair programmer
3. **Lesson 6 (Integration)**: Apply all patterns to build real application
4. **Lesson 7 (Mastery)**: Polish, debug, and extend independently
5. **Lesson 8 (Validation)**: Confirm understanding through comprehensive quiz

---

## VIII. "Try With AI" Section Requirements

Each Layer 2 lesson (2-5) must include 3 "Try With AI" prompts:

### Template per Lesson

```markdown
## Try With AI

### Prompt 1: [Skill Exploration]
```
[Copyable prompt targeting one specific skill from the lesson]
```
**What you're learning:** [Explanation of learning goal]

### Prompt 2: [Extension Challenge]
```
[Prompt that extends the lesson concept to new scenario]
```
**What you're learning:** [Explanation of learning goal]

### Prompt 3: [Domain Connection]
```
[Prompt connecting lesson to reader's own domain/project]
```
**What you're learning:** [Explanation of learning goal]
```

### Example from Lesson 3 (Widget Interactivity)

**Prompt 1** (Skill Exploration):
```
Add a "Refresh Data" button to my widget that calls the refresh_list tool using window.openai.callTool(). Include proper API availability checking.
```
**What you're learning:** Using callTool for widget-to-server communication

**Prompt 2** (Extension Challenge):
```
Create a widget with 3 buttons: "Summarize", "Expand", and "Translate". Each button should use sendFollowUpMessage with appropriate prompts. Add visual feedback when buttons are clicked.
```
**What you're learning:** Building multi-action widgets with user feedback

**Prompt 3** (Domain Connection):
```
For my [domain] business, design a widget that displays [data type] and lets users take 2-3 common actions. What would the sendFollowUpMessage prompts be for each action?
```
**What you're learning:** Applying widget patterns to your specific use case

---

## IX. Validation Checklist

### Chapter-Level Validation

- [x] Chapter type identified: Technical/Code-Focused
- [x] Concept density analysis documented: 8 core concepts
- [x] Lesson count justified: 8 lessons (not arbitrary)
- [x] All evals from spec covered by lessons
- [x] All lessons map to at least one eval
- [x] Expertise skill leveraged for patterns and examples

### Stage Progression Validation

- [x] Lesson 1: Layer 1 (Manual, no AI) - architecture understanding
- [x] Lessons 2-5: Layer 2 (AI Collab with Three Roles)
- [x] Lesson 6: Layer 3 (Pattern Application)
- [x] Lesson 7: Layer 4 (Spec-Driven Integration/Capstone)
- [x] Lesson 8: Assessment
- [x] No spec-first before Layer 4

### Cognitive Load Validation

- [x] Each lesson's concept count <= B1 tier limit (10)
- [x] Lesson 1: 4 concepts (PASS)
- [x] Lesson 2: 5 concepts (PASS)
- [x] Lesson 3: 4 concepts (PASS)
- [x] Lesson 4: 3 concepts (PASS)
- [x] Lesson 5: 5 concepts (PASS)
- [x] Lessons 6-7: Pattern application/synthesis (different cognitive mode)

### Three Roles Validation (Layer 2 Lessons)

- [x] Each Layer 2 lesson (2-5) demonstrates AI as Teacher
- [x] Each Layer 2 lesson (2-5) demonstrates AI as Student
- [x] Each Layer 2 lesson (2-5) demonstrates AI as Co-Worker (convergence)

### Spec Alignment Validation

- [x] FR-001: 8 lessons following L1->L2->L3->L4 progression
- [x] FR-002: Lesson 1 explains architecture without AI
- [x] FR-003: Lessons 2-5 use Claude Code as pair programmer
- [x] FR-004: Lessons 6-7 build complete TaskManager App
- [x] FR-005: Each Layer 2 lesson includes 3 "Try With AI" prompts
- [x] FR-006: All code verified against skill's patterns
- [x] FR-007: Does NOT repeat Chapters 34, 37-38, 40 concepts
- [x] FR-008: References and builds upon MCP concepts
- [x] FR-009: All widgets use `text/html+skybridge` MIME type
- [x] FR-010: All window.openai usage includes availability checks

---

## X. File Structure

```
apps/learn-app/docs/06-AI-Native-Software-Development/42-openai-apps-sdk/
├── README.md                           # Chapter intro
├── 01-apps-sdk-architecture.md         # L1: Architecture
├── 02-first-chatgpt-app.md            # L2: First Widget
├── 03-widget-interactivity.md         # L2: Interactivity
├── 04-response-payload-design.md      # L2: Payloads
├── 05-state-display-modes.md          # L2: State/Display
├── 06-taskmanager-widget.md           # L3: TaskManager Build
├── 07-taskmanager-capstone.md         # L4: Capstone
└── quiz.md                            # Assessment
```

---

## XI. Implementation Notes

### What NOT to Repeat from Prior Chapters

| Topic | Covered In | How to Reference |
|-------|-----------|------------------|
| Agent architecture patterns | Chapter 33 | "As you learned in Chapter 33..." |
| OpenAI Agents SDK (tools, runners) | Chapter 34 | "Building on the SDK patterns from Chapter 34..." |
| MCP server fundamentals | Chapter 37 | "Using your MCP knowledge from Chapter 37..." |
| FastMCP server building | Chapter 38 | "The FastMCP patterns you learned apply here..." |
| FastAPI HTTP patterns | Chapter 40 | "HTTP concepts from Chapter 40 translate directly..." |

### What MUST be Explained Fresh

| Topic | Why New | Source |
|-------|---------|--------|
| Widget iframe sandbox | Apps SDK specific | Skill: architecture section |
| `window.openai` API | Apps SDK specific | Skill: API reference |
| `text/html+skybridge` MIME type | Apps SDK specific | Skill: resource registration |
| Tool metadata for widgets | Apps SDK specific | Skill: tool definition section |
| `structuredContent` vs `_meta` | Apps SDK specific | Skill: response structure |
| Developer Mode workflow | Apps SDK specific | Skill: development setup |

---

**Plan Version**: 1.0.0
**Ready for Implementation**: Yes
**Quality Reference**: apps/learn-app/docs/06-AI-Native-Software-Development/34-openai-agents-sdk/
