# Chapter 43: Vector Databases & RAG — Implementation Plan

**Generated by**: chapter-planner (Reasoning-Activated)
**Source Spec**: `specs/chapter-43-vector-databases-rag-langchain/spec.md`
**Expertise Skill**: `.claude/skills/building-rag-systems/SKILL.md`
**Quality Reference**: `apps/learn-app/docs/06-AI-Native-Software-Development/40-fastapi-for-agents/`
**Created**: 2025-12-30
**Constitution**: v6.0.0 (Reasoning Mode)

---

## I. Chapter Analysis

### Chapter Type

**Technical/Code-Focused** — This chapter teaches RAG implementation with hands-on code. Every lesson beyond L00-L02 requires code examples, API interactions, and practical exercises. The spec explicitly states "code patterns" for L03-L08.

**Rationale**: Learning objectives use "implement", "build", "create", "integrate" (Apply/Create Bloom levels). All lessons after conceptual foundation require working code.

### Concept Density Analysis

**Core Concepts** (from spec): 12 concepts

1. Parametric vs non-parametric memory
2. Embeddings as semantic representations
3. Cosine similarity / vector distance
4. Document loading (loaders)
5. Text splitting / chunking
6. Qdrant vector store setup
7. Retriever pattern
8. Retrieval chains (LCEL)
9. Semantic search endpoints
10. RAG evaluation metrics (RAGAS)
11. LangSmith tracing
12. RAG architecture patterns (8 patterns, teaching 2)

**Complexity Assessment**: Standard-to-Complex (B1 proficiency, 12 interconnected concepts)

**Proficiency Tier**: B1 (Intermediate) — from spec and chapter-index.md

**Justified Lesson Count**: 9 lessons (L00-L08)

- Layer 1 (Manual Foundation): 3 lessons (L00, L01, L02 — skill creation + conceptual)
- Layer 2 (AI Collaboration): 4 lessons (L03, L04, L05, L07 — document processing, vector store, chains, evaluation)
- Layer 3 (Skill Integration): 1 lesson (L06 — Task API extension)
- Layer 4 (Capstone): 1 lesson (L08 — RAG architecture patterns)

**Total**: 9 lessons

**Reasoning**: The spec already defines 9 lessons (L00-L08) which aligns with concept density analysis. The chapter covers 12 core concepts across 9 lessons, averaging ~1.3 concepts per lesson — within B1 cognitive load limits (max 10 new concepts per lesson).

---

## II. Success Evals (from Spec)

**Predefined Success Criteria** (evals-first requirement):

1. **E1**: Students can explain why agents need external knowledge (parametric vs non-parametric)
2. **E2**: Students can describe how vector embeddings represent semantic meaning
3. **E3**: Students can use LangChain for document loading, text splitting, and embedding
4. **E4**: Students can set up and interact with Qdrant vector store through LangChain
5. **E5**: Students can build retrieval chains and QA chains for semantic search
6. **E6**: Students can integrate RAG with Task API for semantic task search
7. **E7**: Students can evaluate RAG quality using LangSmith and RAGAS metrics
8. **E8**: Students can design RAG systems using appropriate architecture patterns

**All lessons below map to these evals.**

---

## III. Pedagogical Arc

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                      CHAPTER 43: RAG LEARNING ARC                           │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  FOUNDATION (L00-L02)           WHY? What problem does RAG solve?           │
│  └── L00: Build skill           Skill-first: create rag-deployment skill   │
│  └── L01: Why external memory   Conceptual: parametric vs non-parametric   │
│  └── L02: Embeddings model      Conceptual: vectors represent meaning      │
│                                                                             │
│  PRACTICE (L03-L05)             HOW? Core RAG components                    │
│  └── L03: Document processing   Technical: loaders, splitters, chunking    │
│  └── L04: Qdrant vector store   Technical: setup, indexing, search         │
│  └── L05: Retrieval chains      Technical: LCEL, QA chains                 │
│                                                                             │
│  INTEGRATION (L06-L07)          APPLY: Production patterns                 │
│  └── L06: Task API semantic     Integration: extend Chapter 40 API         │
│  └── L07: Evaluation            Quality: LangSmith + RAGAS metrics         │
│                                                                             │
│  MASTERY (L08)                  DESIGN: Architecture decisions             │
│  └── L08: RAG patterns          Capstone: implement 2 of 8 patterns        │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## IV. Lesson Sequence

### Lesson 0: Build Your RAG Skill
**Layer**: L1 (Manual Foundation) | **Duration**: 20 min | **CEFR**: B1

**Learning Objective**: Create a `rag-deployment` skill using Claude and official documentation before learning RAG concepts.

**Stage**: 1 (Manual Foundation — Skill-First Pattern)

**New Concepts** (count: 1):
1. Skill-first learning approach for RAG

**Cognitive Load Validation**: 1 concept <= 10 B1 limit -> WITHIN LIMIT

**Maps to Evals**: Foundation for all evals (E1-E8)

**Content Elements**:
- Step 1: Get skills-lab (clone fresh)
- Step 2: Fetch LangChain RAG docs via `/fetching-library-docs`
- Step 3: Create skill via `/skill-creator`
- Step 4: Verify skill responds to RAG prompts

**Prerequisites**: Chapter 5 (skill-creator, fetching-library-docs)

**Skill Output**: `.claude/skills/rag-deployment/SKILL.md`

**No "Reflect on Your Skill"** — this IS the skill creation lesson.

---

### Lesson 1: Why Agents Need External Knowledge
**Layer**: L1 (Conceptual) | **Duration**: 25 min | **CEFR**: A2

**Learning Objective**: Explain why agents need external knowledge systems and distinguish parametric from non-parametric memory.

**Stage**: 1 (Manual Foundation — Conceptual)

**New Concepts** (count: 4):
1. Parametric memory (training data, weights)
2. Non-parametric memory (retrieval systems)
3. Knowledge cutoff problem
4. Hallucination risk and grounding

**Cognitive Load Validation**: 4 concepts <= 7 A2 limit -> WITHIN LIMIT

**Maps to Evals**: E1 (explain why agents need external knowledge)

**Content Elements**:
- **Hook**: "Your Task API has hundreds of tasks. A user asks: 'Show me Docker deployment tasks.' Currently you can only filter by exact fields. RAG enables semantic matching."
- **Mental Model**: Brain (parametric) vs Library Card Catalog (non-parametric)
- **Knowledge Cutoff**: LLMs trained on historical data, don't know yesterday's news
- **Hallucination**: Making up facts vs grounding in retrieved documents
- **RAG Solution**: Query -> Retrieve -> Augment -> Generate

**No code in this lesson** — pure conceptual foundation per spec.

**Three "Try With AI" Prompts**:
1. "My agent answered a question about [current event]. Was this grounded or hallucinated?"
2. "Compare parametric and non-parametric memory. When would you use each?"
3. "I want my Task API agent to answer questions about tasks semantically. Design the retrieval flow."

**"Reflect on Your Skill" Section**: Test if skill explains RAG purpose; identify gaps in knowledge-cutoff explanation.

**Estimated Time**: 25 minutes

---

### Lesson 2: Vector Embeddings Mental Model
**Layer**: L1 (Conceptual) | **Duration**: 30 min | **CEFR**: A2

**Learning Objective**: Describe how vector embeddings represent semantic meaning and calculate similarity.

**Stage**: 1 (Manual Foundation — Conceptual with demonstration)

**New Concepts** (count: 4):
1. Embeddings as numerical meaning representations
2. Semantic similarity (similar meanings = close vectors)
3. Embedding models (text-embedding-3-small, 1536 dimensions)
4. Cosine similarity for comparison

**Cognitive Load Validation**: 4 concepts <= 7 A2 limit -> WITHIN LIMIT

**Maps to Evals**: E2 (describe how vector embeddings represent meaning)

**Content Elements**:
- **Visual**: 2D projection of embedding space with clustered concepts
- **Demonstration**: Generate embeddings for sample sentences, compare scores
- **Mental Model**: Words as coordinates in meaning space
- **Practical**: "king - man + woman = queen" analogy (semantic arithmetic)

**Light code demonstration** (conceptual, not production):

```python
from openai import OpenAI
import numpy as np

client = OpenAI()

def get_embedding(text: str) -> list[float]:
    response = client.embeddings.create(
        input=text,
        model="text-embedding-3-small"
    )
    return response.data[0].embedding

# Semantic similarity demo
e1 = get_embedding("Deploy Docker container")
e2 = get_embedding("Run containerized application")
e3 = get_embedding("Cook pasta for dinner")

def cosine_sim(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

print(f"Deploy/Run containers: {cosine_sim(e1, e2):.3f}")  # ~0.85 (similar)
print(f"Deploy/Cook pasta: {cosine_sim(e1, e3):.3f}")      # ~0.15 (different)
```

**Three "Try With AI" Prompts**:
1. "Generate embeddings for 'urgent bug fix' and 'critical issue repair'. Are they semantically close?"
2. "Why are embeddings better than keyword matching for semantic search?"
3. "My domain is [your domain]. What concepts would cluster together in embedding space?"

**"Reflect on Your Skill" Section**: Test if skill generates embedding code; improve embedding model selection guidance.

**Estimated Time**: 30 minutes

---

### Lesson 3: LangChain Document Processing
**Layer**: L2 (AI Collaboration) | **Duration**: 40 min | **CEFR**: B1

**Learning Objective**: Use LangChain document loaders and text splitters to prepare documents for RAG.

**Stage**: 2 (AI Collaboration with Three Roles)

**New Concepts** (count: 4):
1. Document loaders (WebBaseLoader, PyPDFLoader, TextLoader)
2. Text splitters (RecursiveCharacterTextSplitter)
3. Chunking parameters (chunk_size, chunk_overlap, separators)
4. Metadata preservation

**Cognitive Load Validation**: 4 concepts <= 10 B1 limit -> WITHIN LIMIT

**Maps to Evals**: E3 (use LangChain for document loading, text splitting, embedding)

**Three Roles Demonstrations** (REQUIRED):

**Role 1: AI as Teacher**
- **Scenario**: Student asks about chunk size
- **AI suggests**: "1000 characters with 200 overlap is a good default. Here's why: too small loses context, too large exceeds token limits. NVIDIA benchmarks show 400 tokens optimal for retrieval quality."
- **What student learns**: Evidence-based chunking strategy

**Role 2: AI as Student**
- **Scenario**: AI proposes fixed 500-character chunks
- **Student responds**: "My documents have code blocks. Fixed splitting breaks them mid-function."
- **AI adapts**: "For code-heavy content, use semantic boundaries. Split on `## ` headers for Markdown, or use language-aware chunkers. Here's RecursiveCharacterTextSplitter with custom separators..."
- **What AI learns**: Domain-specific chunking requirements

**Role 3: AI as Co-Worker**
- **Scenario**: Optimizing chunk overlap for Task API documentation
- **Iteration 1**: AI suggests 10% overlap
- **Student feedback**: "Context loss between chunks. Tasks reference each other."
- **Iteration 2**: AI suggests 20% overlap with metadata linking
- **Convergence**: Both agree on 15% overlap + chunk ID linking for context expansion

**Code Pattern** (from expertise skill):

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import DirectoryLoader, TextLoader

# Load documents
loader = DirectoryLoader(
    "./docs/",
    glob="**/*.md",
    loader_cls=TextLoader,
    loader_kwargs={"encoding": "utf-8"},
)
docs = loader.load()

# Split with semantic awareness
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,           # Target ~400 tokens
    chunk_overlap=200,         # 20% overlap
    add_start_index=True,      # Track position
    separators=["\n\n", "\n", " ", ""],  # Split hierarchy
)
splits = text_splitter.split_documents(docs)

print(f"Loaded {len(docs)} docs -> {len(splits)} chunks")
```

**Three "Try With AI" Prompts**:
1. "My documents are technical specifications with code blocks. Suggest a chunking strategy that preserves code integrity."
2. "I loaded 50 markdown files and got 3000 chunks. Is this too many? How do I evaluate chunk quality?"
3. "Show me how to preserve document metadata (source, page, section) through the splitting process."

**"Reflect on Your Skill" Section**: Test if skill handles different document types; add chunking parameter guidance.

**Prerequisites**: L00 (skill), L02 (embeddings concept)

**Estimated Time**: 40 minutes

---

### Lesson 4: Qdrant Vector Store with LangChain
**Layer**: L2 (AI Collaboration) | **Duration**: 45 min | **CEFR**: B1

**Learning Objective**: Set up and interact with Qdrant vector store through LangChain for document indexing and similarity search.

**Stage**: 2 (AI Collaboration with Three Roles)

**New Concepts** (count: 5):
1. Running Qdrant with Docker
2. QdrantVectorStore initialization patterns
3. Adding documents with embeddings
4. Similarity search modes (dense, sparse, hybrid)
5. Metadata filtering

**Cognitive Load Validation**: 5 concepts <= 10 B1 limit -> WITHIN LIMIT

**Maps to Evals**: E4 (set up and interact with Qdrant)

**Three Roles Demonstrations** (REQUIRED):

**Role 1: AI as Teacher**
- **Scenario**: Student unfamiliar with vector databases
- **AI suggests**: "Qdrant stores vectors as points in high-dimensional space. When you search, it finds the nearest neighbors to your query vector. The distance metric (cosine/euclidean/dot) determines 'nearness'."
- **What student learns**: Vector DB fundamentals before implementation

**Role 2: AI as Student**
- **Scenario**: AI suggests in-memory Qdrant for production
- **Student responds**: "This resets on restart. I need persistence."
- **AI adapts**: "For production, use Docker Qdrant with volume persistence:
  `docker run -v qdrant_storage:/qdrant/storage -p 6333:6333 qdrant/qdrant`
  This persists your vectors across restarts."
- **What AI learns**: Production persistence requirements

**Role 3: AI as Co-Worker**
- **Scenario**: Designing Task API collection schema
- **Iteration 1**: AI suggests minimal payload (task_id only)
- **Student feedback**: "Need to filter by status, priority, assignee"
- **Iteration 2**: AI adds indexed payload fields
- **Convergence**: Schema with dense vectors + indexed metadata for filtered retrieval

**Code Pattern** (from expertise skill):

```python
from langchain_qdrant import QdrantVectorStore
from langchain_openai import OpenAIEmbeddings
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams

# Initialize embeddings
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# Option 1: Docker (production)
vector_store = QdrantVectorStore.from_documents(
    splits,
    embeddings,
    url="http://localhost:6333",
    collection_name="task_docs",
)

# Option 2: In-memory (testing)
client = QdrantClient(":memory:")
client.create_collection(
    collection_name="task_docs",
    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),
)
vector_store = QdrantVectorStore(
    client=client,
    collection_name="task_docs",
    embedding=embeddings,
)

# Similarity search
results = vector_store.similarity_search(
    "How do I complete a task?",
    k=4
)
for doc in results:
    print(f"Source: {doc.metadata.get('source')}")
    print(doc.page_content[:200])
```

**Safety Notes**:
- Never commit `.env` files with OPENAI_API_KEY
- Use environment variables: `os.getenv("OPENAI_API_KEY")`
- For Docker: persist volumes to avoid data loss

**Three "Try With AI" Prompts**:
1. "My Qdrant similarity search returns irrelevant results. How do I debug retrieval quality?"
2. "I need to filter search results by task status. Show me metadata filtering with Qdrant."
3. "What's the difference between dense, sparse, and hybrid search? When would I use each?"

**"Reflect on Your Skill" Section**: Test if skill produces Qdrant setup code; add collection configuration guidance.

**Prerequisites**: Docker installed, L03 (document splits)

**Estimated Time**: 45 minutes

---

### Lesson 5: Building Retrieval Chains
**Layer**: L2 (AI Collaboration) | **Duration**: 45 min | **CEFR**: B1

**Learning Objective**: Build retrieval chains and QA chains using LangChain Expression Language (LCEL) for semantic search.

**Stage**: 2 (AI Collaboration with Three Roles)

**New Concepts** (count: 4):
1. Retriever pattern (`vector_store.as_retriever()`)
2. Simple QA chain (retriever -> prompt -> LLM)
3. Chain composition with LCEL
4. Document formatting for context

**Cognitive Load Validation**: 4 concepts <= 10 B1 limit -> WITHIN LIMIT

**Maps to Evals**: E5 (build retrieval chains and QA chains)

**Three Roles Demonstrations** (REQUIRED):

**Role 1: AI as Teacher**
- **Scenario**: Student asks about LCEL syntax
- **AI suggests**: "LCEL uses the pipe operator `|` to compose components. Think of it as data flowing through a pipeline: `input | component1 | component2 | output`. Each component transforms the data."
- **What student learns**: Declarative chain composition

**Role 2: AI as Student**
- **Scenario**: AI generates chain without source citations
- **Student responds**: "Users need to know where answers come from for trust."
- **AI adapts**: "You're right. Here's how to include sources in the response:
  ```python
  def format_docs_with_sources(docs):
      return '\n\n'.join(
          f'[{doc.metadata.get("source")}]: {doc.page_content}'
          for doc in docs
      )
  ```"
- **What AI learns**: User trust requires citation

**Role 3: AI as Co-Worker**
- **Scenario**: Optimizing retrieval for Task API
- **Iteration 1**: AI suggests k=4 retrieval
- **Student feedback**: "Some queries need more context, others less"
- **Iteration 2**: AI proposes MMR retrieval for diversity
- **Convergence**: Use MMR with k=6, fetch_k=20, lambda_mult=0.5 for relevance + diversity balance

**Code Pattern** (from expertise skill):

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")
retriever = vector_store.as_retriever(
    search_type="mmr",  # Maximum Marginal Relevance
    search_kwargs={"k": 4, "fetch_k": 20}
)

prompt = ChatPromptTemplate.from_template("""
Answer the question based only on the following context:

{context}

Question: {question}

Provide a concise answer and cite your sources.
""")

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# LCEL chain composition
chain = (
    {"context": retriever | format_docs, "question": lambda x: x}
    | prompt
    | llm
)

response = chain.invoke("How do I mark a task as complete?")
print(response.content)
```

**Three "Try With AI" Prompts**:
1. "My QA chain gives correct answers but they're too verbose. How do I control response length?"
2. "Build a chain that returns both the answer AND the retrieved documents so users can verify."
3. "My chain sometimes says 'I don't know' when the answer IS in the documents. How do I improve retrieval?"

**"Reflect on Your Skill" Section**: Test if skill produces LCEL chains; add prompt engineering guidance for QA.

**Prerequisites**: L04 (Qdrant vector store)

**Estimated Time**: 45 minutes

---

### Lesson 6: RAG for Task API
**Layer**: L3 (Skill Integration) | **Duration**: 50 min | **CEFR**: B1

**Learning Objective**: Integrate RAG with the Task API from Chapter 40 to add semantic search capabilities.

**Stage**: 3 (Skill Integration — builds reusable component)

**New Concepts** (count: 4):
1. Indexing task descriptions
2. Semantic search endpoint design
3. Combining structured filters with vector similarity
4. Returning ranked results with relevance scores

**Cognitive Load Validation**: 4 concepts <= 10 B1 limit -> WITHIN LIMIT

**Maps to Evals**: E6 (integrate RAG with Task API for semantic task search)

**Content Elements**:
- **Chapter 40 Connection**: Extend Task API with `/tasks/search/semantic` endpoint
- **Integration Pattern**: Index task descriptions -> Semantic search -> Return ranked results
- **Production Concern**: Incremental indexing on task create/update

**Code Pattern** (extending Chapter 40 Task API):

```python
from fastapi import APIRouter, Depends
from langchain_qdrant import QdrantVectorStore
from langchain_openai import OpenAIEmbeddings
from pydantic import BaseModel

router = APIRouter()

# Initialize vector store (shared across requests)
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vector_store = QdrantVectorStore.from_existing_collection(
    embedding=embeddings,
    collection_name="task_descriptions",
    url="http://localhost:6333",
)

class SemanticSearchRequest(BaseModel):
    query: str
    k: int = 5
    status_filter: str | None = None

class SemanticSearchResult(BaseModel):
    task_id: int
    title: str
    relevance: float
    snippet: str

@router.post("/tasks/search/semantic", response_model=list[SemanticSearchResult])
async def semantic_search(request: SemanticSearchRequest):
    """Search tasks by semantic meaning."""
    # Build filter if status provided
    filter_condition = None
    if request.status_filter:
        from qdrant_client import models
        filter_condition = models.Filter(
            must=[models.FieldCondition(
                key="metadata.status",
                match=models.MatchValue(value=request.status_filter)
            )]
        )

    # Semantic search with optional filter
    results = vector_store.similarity_search_with_score(
        request.query,
        k=request.k,
        filter=filter_condition
    )

    return [
        SemanticSearchResult(
            task_id=doc.metadata["task_id"],
            title=doc.metadata["title"],
            relevance=float(score),
            snippet=doc.page_content[:200]
        )
        for doc, score in results
    ]

# Index task on creation (call from task create endpoint)
async def index_task(task_id: int, title: str, description: str, status: str):
    """Index task description for semantic search."""
    from langchain_core.documents import Document

    doc = Document(
        page_content=f"{title}\n\n{description}",
        metadata={
            "task_id": task_id,
            "title": title,
            "status": status
        }
    )
    vector_store.add_documents([doc], ids=[f"task_{task_id}"])
```

**Three "Try With AI" Prompts**:
1. "When should I re-index tasks? On every update or with a batch job?"
2. "My semantic search returns old task versions. How do I handle updates?"
3. "Design an endpoint that combines keyword filters (status, assignee) with semantic search."

**"Reflect on Your Skill" Section**: Test if skill generates FastAPI + RAG integration; add endpoint design patterns.

**Prerequisites**: Chapter 40 (Task API), L05 (retrieval chains)

**Estimated Time**: 50 minutes

---

### Lesson 7: Evaluating RAG Quality
**Layer**: L2 (AI Collaboration) | **Duration**: 45 min | **CEFR**: B1

**Learning Objective**: Evaluate RAG quality using LangSmith tracing and RAGAS metrics.

**Stage**: 2 (AI Collaboration — evaluation as collaborative practice)

**New Concepts** (count: 5):
1. Four evaluation dimensions (Correctness, Relevance, Groundedness, Retrieval Quality)
2. LLM-as-Judge pattern
3. LangSmith tracing and visualization
4. RAGAS metrics (faithfulness, answer_relevancy, context_precision, context_recall)
5. Evaluation datasets

**Cognitive Load Validation**: 5 concepts <= 10 B1 limit -> WITHIN LIMIT

**Maps to Evals**: E7 (evaluate RAG quality using LangSmith and RAGAS)

**Three Roles Demonstrations** (REQUIRED):

**Role 1: AI as Teacher**
- **Scenario**: Student asks what makes a "good" RAG system
- **AI suggests**: "Four dimensions matter: (1) Faithfulness — is the answer grounded in retrieved context? (2) Relevance — does the answer address the question? (3) Context Precision — are retrieved docs relevant? (4) Context Recall — do retrieved docs cover the answer?"
- **What student learns**: Multi-dimensional quality framework

**Role 2: AI as Student**
- **Scenario**: AI suggests evaluating with 5 test questions
- **Student responds**: "5 questions won't catch edge cases. My Task API has many query types."
- **AI adapts**: "You're right. For reliable evaluation, you need 20-30 diverse examples covering: factual queries, procedural queries, edge cases (ambiguous, multi-hop). Here's how to build a comprehensive test set..."
- **What AI learns**: Statistical reliability requires diverse test coverage

**Role 3: AI as Co-Worker**
- **Scenario**: Designing evaluation for Task API RAG
- **Iteration 1**: AI suggests faithfulness metric only
- **Student feedback**: "I also need to know if retrieval is working"
- **Iteration 2**: AI adds context_precision
- **Convergence**: Full evaluation with faithfulness + context_precision + answer_relevancy

**Code Pattern** (from expertise skill):

```python
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)
from datasets import Dataset
from langsmith import traceable, Client

# Enable LangSmith tracing
import os
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "task-api-rag-eval"

# Create evaluation dataset
eval_data = {
    "question": [
        "How do I create a new task?",
        "How do I mark a task complete?",
        "What tasks are assigned to me?",
    ],
    "answer": [],  # Filled by RAG pipeline
    "contexts": [],  # Filled by retrieval
    "ground_truth": [
        "Use POST /tasks with title and description in the request body.",
        "Use PATCH /tasks/{id} with status='completed' in the body.",
        "Use GET /tasks?assignee={your_id} to filter tasks by assignee.",
    ]
}

@traceable(name="task_api_rag")
def run_rag_pipeline(question: str) -> dict:
    """RAG pipeline with tracing."""
    docs = retriever.invoke(question)
    context = "\n\n".join(doc.page_content for doc in docs)
    answer = chain.invoke(question)
    return {
        "question": question,
        "answer": answer.content,
        "contexts": [doc.page_content for doc in docs],
    }

# Run evaluation
for q, gt in zip(eval_data["question"], eval_data["ground_truth"]):
    result = run_rag_pipeline(q)
    eval_data["answer"].append(result["answer"])
    eval_data["contexts"].append(result["contexts"])

dataset = Dataset.from_dict(eval_data)
results = evaluate(
    dataset,
    metrics=[faithfulness, answer_relevancy, context_precision, context_recall]
)
print(results)
# {'faithfulness': 0.92, 'answer_relevancy': 0.88, 'context_precision': 0.85, 'context_recall': 0.90}
```

**Metric Thresholds** (from expertise skill):

| Metric | Minimum | Good | Excellent |
|--------|---------|------|-----------|
| Faithfulness | 0.70 | 0.85 | 0.95+ |
| Answer Relevancy | 0.70 | 0.80 | 0.90+ |
| Context Precision | 0.60 | 0.75 | 0.85+ |

**Three "Try With AI" Prompts**:
1. "My faithfulness score is 0.65. What's causing hallucination and how do I fix it?"
2. "Create a test dataset for my [domain] RAG system with 20 diverse questions."
3. "How do I integrate RAGAS evaluation into CI/CD so quality is checked on every deploy?"

**"Reflect on Your Skill" Section**: Test if skill generates evaluation code; add metric interpretation guidance.

**Prerequisites**: L05 (retrieval chains), LangSmith account (free tier)

**Estimated Time**: 45 minutes

---

### Lesson 8: RAG Architecture Patterns (Capstone)
**Layer**: L4 (Orchestration) | **Duration**: 60 min | **CEFR**: B1

**Learning Objective**: Design RAG systems using appropriate architecture patterns and implement two patterns for the Task API.

**Stage**: 4 (Spec-Driven Integration — Capstone)

**Eight RAG Patterns** (from expertise skill):

| Pattern | When to Use | Complexity |
|---------|-------------|------------|
| 1. Simple RAG | FAQ, known scope | Low |
| 2. Simple RAG + Memory | Conversations | Low |
| 3. Branched RAG | Multi-domain | Medium |
| 4. HyDE | Vague queries | Medium |
| 5. Adaptive RAG | Variable complexity | Medium |
| 6. Corrective RAG (CRAG) | High accuracy | High |
| 7. Self-RAG | Research | High |
| 8. Agentic RAG | Complex reasoning | High |

**Concepts Covered** (overview of all 8, implementation of 2):
1. Architecture pattern selection criteria
2. Simple RAG implementation
3. One advanced pattern (student choice: HyDE, CRAG, or Agentic)
4. Pattern composition

**Cognitive Load Validation**: 4 implementation concepts (8 patterns in overview mode) <= 10 B1 limit -> WITHIN LIMIT

**Maps to Evals**: E8 (design RAG systems using appropriate architecture patterns)

**Capstone Project**:

**Specification Writing** (PRIMARY SKILL — Spec FIRST):

```markdown
# Task API RAG Enhancement Spec

## Intent
Add advanced RAG capabilities to Task API for production use.

## Requirements
1. Implement Simple RAG (baseline) for task documentation search
2. Implement ONE advanced pattern (choose HyDE, CRAG, or Agentic)
3. Evaluate both patterns with RAGAS metrics
4. Document pattern selection rationale

## Success Criteria
- Simple RAG achieves >0.80 faithfulness on test set
- Advanced pattern shows measurable improvement over baseline
- Implementation includes evaluation code
```

**Component Composition**:
- Skills from L00-L07: document processing, Qdrant, retrieval chains, evaluation
- Task API integration from L06
- Pattern selection from decision logic

**Implementation Pattern** (Simple RAG baseline from expertise skill):

```python
# Simple RAG (baseline)
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")
retriever = vector_store.as_retriever(search_kwargs={"k": 4})

prompt = ChatPromptTemplate.from_template("""
Answer based on this context:
{context}

Question: {question}
""")

simple_rag_chain = (
    {"context": retriever | format_docs, "question": lambda x: x}
    | prompt
    | llm
)
```

**Advanced Pattern Example** (HyDE from expertise skill):

```python
# HyDE (Hypothetical Document Embeddings)
hyde_prompt = ChatPromptTemplate.from_template("""
Write a detailed passage that would answer this question:
{question}

Passage:""")

def hyde_retrieve(question: str, k: int = 4):
    # Generate hypothetical answer
    hypothetical = (hyde_prompt | llm).invoke({"question": question})

    # Use hypothetical for retrieval (better for vague queries)
    docs = vector_store.similarity_search(hypothetical.content, k=k)
    return docs

hyde_chain = (
    {"context": lambda q: format_docs(hyde_retrieve(q)), "question": lambda x: x}
    | prompt
    | llm
)
```

**Three "Try With AI" Prompts**:
1. "Compare Simple RAG vs HyDE for my Task API. When would each pattern perform better?"
2. "My users ask vague questions like 'help me with deployment'. Which RAG pattern handles ambiguity best?"
3. "Design a RAG architecture for [your domain] that handles both simple FAQ and complex multi-step queries."

**"Reflect on Your Skill" Section**: Test if skill recommends appropriate patterns; add pattern selection decision tree.

**Prerequisites**: All previous lessons (L00-L07)

**Estimated Time**: 60 minutes

---

## V. Skill Dependencies

**Skill Dependency Graph**:

```
Chapter 5: skill-creator, fetching-library-docs
    └── L00: Build rag-deployment skill
        └── L01-L02: Conceptual foundation (no code dependencies)
            └── L03: Document processing (requires L02 embeddings concept)
                └── L04: Qdrant vector store (requires L03 splits)
                    └── L05: Retrieval chains (requires L04 vector store)
                        └── L06: Task API integration (requires L05 chains + Chapter 40)
                        └── L07: Evaluation (requires L05 chains)
                            └── L08: Architecture patterns (requires L06 + L07)
```

**Cross-Chapter Dependencies**:

| Prerequisite | Status | Validation |
|--------------|--------|------------|
| Chapter 5 (Skills/Commands) | Implemented | Required for L00 |
| Chapters 34-36 (Agent SDKs) | Implemented | Agents call tools context |
| Chapter 40 (FastAPI) | Implemented | Required for L06 Task API integration |
| Docker | External | Required for Qdrant (L04+) |

---

## VI. Assessment Plan

### Formative Assessments (During Lessons)

| Lesson | Assessment | Type |
|--------|------------|------|
| L01 | Explain parametric vs non-parametric memory | Conceptual quiz |
| L02 | Generate embeddings and compare similarity | Code exercise |
| L03 | Split documents with appropriate parameters | Hands-on exercise |
| L04 | Set up Qdrant and run similarity search | Hands-on exercise |
| L05 | Build QA chain and get correct answers | Hands-on exercise |
| L06 | Extend Task API with semantic search | Integration exercise |
| L07 | Evaluate RAG with RAGAS metrics | Evaluation exercise |

### Summative Assessment (End of Chapter)

**L08 Capstone Project**: Implement two RAG patterns with comparative evaluation

**Success Criteria**:
1. Simple RAG baseline implemented and tested
2. One advanced pattern implemented (HyDE, CRAG, or Agentic)
3. RAGAS evaluation shows metrics for both patterns
4. Pattern selection documented with rationale
5. Code runs without errors

---

## VII. Content Dependencies and Ordering

**Strict Order** (lessons build on each other):

```
L00 (Skill) → L01 (Why RAG) → L02 (Embeddings) → L03 (Documents) → L04 (Qdrant) → L05 (Chains) → L06 (Task API) + L07 (Evaluation) → L08 (Patterns)
```

**Parallel Possible**: L06 and L07 could theoretically run in parallel (both depend on L05), but L08 requires both.

**External Dependencies**:

| Dependency | When Needed | Setup Instruction |
|------------|-------------|-------------------|
| OpenAI API key | L02+ | `export OPENAI_API_KEY=...` |
| Docker | L04+ | `docker run qdrant/qdrant` |
| LangSmith account | L07 | Free tier at smith.langchain.com |
| Chapter 40 Task API | L06 | Clone from previous chapter |

---

## VIII. Images/Videos to Include

From spec, create these diagrams:

| Lesson | Diagram | Purpose |
|--------|---------|---------|
| L02 | Embedding space visualization (2D projection) | Show semantic clustering |
| L05 | Retrieval chain flow diagram | Visualize LCEL pipeline |
| L08 | RAG architecture comparison table | Pattern selection guide |

**Diagram Specifications**:

**L02 Embedding Space**:
- 2D scatter plot with labeled clusters
- Clusters: "Docker tasks", "Bug fixes", "Meetings"
- Show query point and nearest neighbors
- Tool: Create with matplotlib or use existing stock diagram

**L05 Chain Flow**:
```
Query → Retriever → Docs → Format → Prompt → LLM → Answer
```
- Boxes for each component
- Arrows showing data flow
- Labels for inputs/outputs

**L08 Architecture Table**:
- 8 patterns in rows
- Columns: Pattern, When to Use, Complexity, Task API Fit
- Color coding: Green (implemented), Yellow (covered conceptually)

---

## IX. Validation Checklist

**Chapter-Level Validation**:
- [x] Chapter type identified (Technical/Code-Focused)
- [x] Concept density analysis documented (12 concepts -> 9 lessons)
- [x] Lesson count justified (matches spec, concept density confirms)
- [x] All evals from spec covered by lessons (E1-E8)
- [x] All lessons map to at least one eval

**Stage Progression Validation**:
- [x] Lessons 0-2: Layer 1 (Manual Foundation — Skill creation + Conceptual)
- [x] Lessons 3-5, 7: Layer 2 (AI Collaboration with Three Roles)
- [x] Lesson 6: Layer 3 (Skill Integration — Task API extension)
- [x] Lesson 8: Layer 4 (Spec-Driven Capstone)
- [x] No spec-first before Layer 4 (spec-first only in L08 capstone)

**Cognitive Load Validation**:
- [x] L00: 1 concept <= 10 B1 limit
- [x] L01: 4 concepts <= 7 A2 limit
- [x] L02: 4 concepts <= 7 A2 limit
- [x] L03: 4 concepts <= 10 B1 limit
- [x] L04: 5 concepts <= 10 B1 limit
- [x] L05: 4 concepts <= 10 B1 limit
- [x] L06: 4 concepts <= 10 B1 limit
- [x] L07: 5 concepts <= 10 B1 limit
- [x] L08: 4 implementation concepts <= 10 B1 limit

**Dependency Validation**:
- [x] Skill dependencies satisfied by lesson order
- [x] Cross-chapter dependencies validated (Chapter 5, 40 implemented)
- [x] External dependencies documented (Docker, OpenAI key, LangSmith)

**Three Roles Validation** (Layer 2 lessons):
- [x] L03: AI as Teacher (chunking strategy), Student (code-aware splitting), Co-Worker (overlap optimization)
- [x] L04: AI as Teacher (vector DB fundamentals), Student (persistence), Co-Worker (schema design)
- [x] L05: AI as Teacher (LCEL syntax), Student (source citations), Co-Worker (retrieval tuning)
- [x] L07: AI as Teacher (quality dimensions), Student (test coverage), Co-Worker (metric selection)

**Quality Requirements** (from spec):
- [x] Full YAML frontmatter planned for all lessons
- [x] Compelling narrative opening planned (Task API connection)
- [x] Three "Try With AI" prompts per lesson
- [x] "Reflect on Your Skill" section for L01-L08
- [x] Safety notes for API key handling

---

## X. Expertise Skill Integration

**Skill Reference**: `.claude/skills/building-rag-systems/SKILL.md`

**Key Patterns Used from Skill**:

| Lesson | Skill Pattern | Reference Section |
|--------|---------------|-------------------|
| L03 | Semantic chunking | `references/ingestion-patterns.md` |
| L04 | Qdrant setup + hybrid search | `references/langchain-patterns.md` |
| L05 | LCEL chain composition | `references/langchain-patterns.md` |
| L06 | Filtered retrieval | `references/retrieval-patterns.md` |
| L07 | RAGAS evaluation | `references/evaluation-patterns.md` |
| L08 | 8 RAG architectures | `references/rag-architectures.md` |

**Anti-Patterns from Skill** (include in lessons):

| Lesson | Anti-Pattern | Correct Pattern |
|--------|--------------|-----------------|
| L03 | Fixed character chunking | Semantic boundaries (## headers) |
| L04 | In-memory for production | Docker Qdrant with volumes |
| L05 | No source citations | Include metadata in responses |
| L07 | Evaluating with 5 examples | Minimum 20-30 diverse examples |

---

## XI. Running Example Integration

**Task API Extension Arc**:

| Lesson | Task API Change |
|--------|-----------------|
| L04 | Index task descriptions into Qdrant collection |
| L06 | Add `/tasks/search/semantic` endpoint |
| L08 | Add advanced RAG patterns for task recommendations |

**Narrative Continuity**: "You built a Task API in Chapter 40 with CRUD operations. Users can filter by status and priority. But what if they ask: 'Show me tasks about Docker deployment'? That requires semantic understanding. RAG enables your API to find semantically similar tasks."

---

## XII. Implementation Notes for Content Implementer

**Quality Reference Match**: Follow structure from `apps/learn-app/docs/06-AI-Native-Software-Development/40-fastapi-for-agents/`:

1. **L00**: Match `00-build-your-fastapi-skill.md` structure (15 min, concise steps)
2. **L01-L08**: Match `01-hello-fastapi.md` structure:
   - Full YAML frontmatter with skills, learning_objectives, cognitive_load, differentiation
   - Compelling hook connecting to prior chapters
   - Code with outputs shown
   - "Try With AI" section with 3 prompts and learning explanations
   - "Reflect on Your Skill" section at end
   - "Common Mistakes" where appropriate

**File Naming Convention**:
```
00-build-your-rag-skill.md
01-why-agents-need-external-knowledge.md
02-vector-embeddings-mental-model.md
03-langchain-document-processing.md
04-qdrant-vector-store.md
05-building-retrieval-chains.md
06-rag-for-task-api.md
07-evaluating-rag-quality.md
08-rag-architecture-patterns.md
```

**Output Directory**: `apps/learn-app/docs/06-AI-Native-Software-Development/43-vector-databases-rag/`

---

**Plan Complete. Ready for implementation.**
