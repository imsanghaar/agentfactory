# Part 8: LLMOps Proprietary Intelligence — Chapter Plans (61-66)

**Generated by**: chapter-planner v2.0.0 (Reasoning-Activated)
**Source Spec**: /Users/mjs/Documents/code/panaversity-official/tutorsgpt/mem/specs/001-part8-llmops/spec.md
**Version**: 1.0.0
**Status**: Draft
**Created**: 2026-01-01
**Constitution**: v7.0.0 (Agent Factory Paradigm)
**Scope**: Chapters 61-66 (Stage 1: Concepts/Setup + Stage 2: Data/Training)

---

## Overview

This plan covers the first 6 chapters of Part 8, spanning:
- **Stage 1: Concepts & Setup** (Chapters 61-62)
- **Stage 2: Data & Training** (Chapters 63-66)

Remaining chapters (67-72) will be planned in a subsequent phase.

### Running Example: Task API Domain

All chapters use the **Task API** from Chapter 40 as the running example:
- Domain: Task management (create, update, complete tasks)
- Fine-tuning goal: Create a Task Assistant that understands task management workflows
- Progression: Simple instruction-following → Persona tuning → Agentic tool-calling

### Hardware Constraints

All content compatible with:
- **Google Colab Free Tier**: T4 GPU (15GB VRAM), 12GB RAM
- **4-bit Quantization**: BitsAndBytes NF4 for all training
- **Local Inference**: Ollama on 8GB+ RAM (quantized models)

---

## Chapter 61: Introduction to LLMOps

### Chapter Analysis

**Chapter Type**: Conceptual/Hybrid — Establishes mental models for when/why to fine-tune, introduces LLM lifecycle, ROI analysis. Some code examples but primarily conceptual.

**Proficiency Tier**: B2 (Upper-Intermediate) — Students have completed Parts 1-7, understand agents, need strategic framework for custom models.

**Concept Density**: 8 core concepts

1. LLMOps lifecycle (data → train → eval → deploy → monitor)
2. When to fine-tune vs when to prompt engineer
3. Foundation models vs specialized models tradeoff
4. Training taxonomy (pretraining, SFT, DPO)
5. Proprietary intelligence as competitive advantage
6. Cost-benefit analysis (API costs vs training investment)
7. Digital FTE production through custom models
8. Platform landscape (open-source vs managed)

**Justified Lesson Count**: 7 lessons
- Layer 1 (Conceptual): 4 lessons (lifecycle, decision framework, taxonomy, economics)
- Layer 2 (AI Collab): 2 lessons (use-case analysis with AI)
- Layer 3 (Intelligence): 1 lesson (decision framework skill)

---

### Lesson Sequence

#### Lesson 1: The LLMOps Revolution

**Learning Objective**: Explain what LLMOps is and why proprietary intelligence creates competitive advantage

**Stage**: Layer 1 (Conceptual Foundation)

**Duration**: 35 min

**New Concepts** (count: 4):
1. LLMOps definition and scope
2. Proprietary intelligence concept
3. Foundation model limitations (generic knowledge)
4. Custom model value proposition

**Cognitive Load Validation**: 4 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:
- What is LLMOps? (training, deploying, operating custom models)
- The $650M Casetext example (encoded legal expertise)
- Foundation models: powerful but generic
- Your domain expertise → Proprietary intelligence → Digital FTE
- Real-world examples: Healthcare, Legal, Finance custom models

**Maps to Evals**: Foundation for all subsequent chapters

---

#### Lesson 2: The LLM Lifecycle

**Learning Objective**: Identify the five stages of the LLM lifecycle and their interdependencies

**Stage**: Layer 1 (Conceptual Foundation)

**Duration**: 40 min

**New Concepts** (count: 5):
1. Data curation stage
2. Training/fine-tuning stage
3. Evaluation stage
4. Deployment stage
5. Monitoring and iteration stage

**Cognitive Load Validation**: 5 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:
```
Data Curation → Training → Evaluation → Deployment → Monitoring
     ↑                                                    |
     └────────────── Feedback Loop ───────────────────────┘
```
- Each stage: inputs, outputs, tools, decisions
- Where things go wrong (data quality, eval gaps, drift)
- Spec-first thinking applies: Define success before training
- Running example: Task API lifecycle planning

**Maps to Evals**: SC-001 (complete workflow understanding)

---

#### Lesson 3: When to Fine-Tune (Decision Framework)

**Learning Objective**: Apply decision framework to determine whether fine-tuning creates value for a given use case

**Stage**: Layer 1 (Conceptual Foundation)

**Duration**: 45 min

**New Concepts** (count: 4):
1. Prompt engineering ceiling
2. Fine-tuning indicators (consistent format, domain knowledge, reduced latency)
3. Fine-tuning anti-indicators (rapidly changing knowledge, general tasks)
4. Decision tree for fine-tune vs prompt

**Cognitive Load Validation**: 4 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:

**When to Fine-Tune**:
| Signal | Example | Why Fine-Tune |
|--------|---------|---------------|
| Consistent output format | Always return JSON with specific schema | Training enforces format better than prompts |
| Domain-specific language | Medical terminology, legal citations | Prompt context limits exceeded |
| Reduced latency needed | <100ms response time | Smaller specialized model faster |
| Cost optimization | High-volume task | Custom 7B model cheaper than GPT-4 |

**When NOT to Fine-Tune**:
| Signal | Example | Why Prompt Instead |
|--------|---------|-------------------|
| Rapidly changing knowledge | Current events, stock prices | Training data becomes stale |
| General reasoning | Complex multi-step problems | Foundation models excel |
| Low volume | <100 requests/day | Training cost not justified |

**Running Example**: Task API — analyze whether task creation assistant should be fine-tuned

**Maps to Evals**: User Story 1 (domain expert creates custom model)

---

#### Lesson 4: Training Taxonomy

**Learning Objective**: Distinguish between pretraining, SFT, and alignment methods (DPO/RLHF)

**Stage**: Layer 1 (Conceptual Foundation)

**Duration**: 40 min

**New Concepts** (count: 4):
1. Pretraining (massive unlabeled data → base capabilities)
2. Supervised Fine-Tuning (instruction pairs → task performance)
3. Alignment (preference data → safety, style, behavior)
4. PEFT methods overview (LoRA, QLoRA, adapters)

**Cognitive Load Validation**: 4 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:
```
Pretraining           SFT                    Alignment
(Foundational)        (Task-Specific)        (Behavioral)
     |                     |                      |
     v                     v                      v
"Learn language"    "Follow instructions"   "Be helpful & safe"
     |                     |                      |
Trillions tokens     Thousands pairs       Preference pairs
Millions $           $1-100 (our focus)     $10-1000
```

- What WE teach: SFT and DPO (practitioner-accessible)
- What we DON'T teach: Pretraining (requires cluster), RLHF (too complex)
- PEFT makes SFT accessible on consumer hardware

**Maps to Evals**: Foundation for Chapters 64-66

---

#### Lesson 5: Economics of Custom Models

**Learning Objective**: Calculate ROI for fine-tuning investment including training costs, inference savings, and time-to-value

**Stage**: Layer 1 (Conceptual Foundation)

**Duration**: 45 min

**New Concepts** (count: 4):
1. Training cost components (compute, data preparation, iteration)
2. Inference cost comparison (API vs self-hosted)
3. Break-even analysis
4. Hidden costs (maintenance, monitoring, updates)

**Cognitive Load Validation**: 4 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:

**Cost Comparison Table**:
| Approach | Monthly Cost | Setup Cost | Best For |
|----------|-------------|------------|----------|
| GPT-4 API | $10-50K (high volume) | $0 | Rapid prototyping |
| Custom 7B (self-hosted) | $500-2K | $100-500 training | Production scale |
| Custom 7B (cloud GPU) | $1-5K | $100-500 training | Medium scale |

**Break-even formula**:
```
Break-even requests = Training cost / (API cost per request - Inference cost per request)
```

**Running Example**: Task API economics
- Current: GPT-4o-mini at $0.003/request
- Projected: 10,000 requests/day = $900/month
- Custom model: $200 training + $300/month serving = 3-month payback

**Maps to Evals**: User Story 2 (replace expensive API calls)

---

#### Lesson 6: Use Case Analysis with AI

**Learning Objective**: Analyze a domain use case with AI assistance to determine fine-tuning viability

**Stage**: Layer 2 (AI Collaboration)

**Duration**: 50 min

**New Concepts** (count: 2):
1. Use case specification for fine-tuning
2. Data requirements estimation

**Cognitive Load Validation**: 2 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Three Roles Demonstration**:

**AI as Teacher**: AI suggests questions to ask about your domain
- "What output formats are required?"
- "What domain-specific terminology exists?"
- "What volume of requests do you expect?"

**AI as Student**: Student teaches AI their specific domain constraints
- "Our Task API needs to understand project management terminology"
- "Response time must be under 200ms"
- "We have 500 labeled examples"

**AI as Co-Worker**: Converge on fine-tuning recommendation
- Joint analysis produces: "SFT with LoRA recommended, 500 examples sufficient, estimate 2 hours training on T4"

**Content Elements**:
- Structured use case template
- AI-assisted gap analysis
- Data requirements estimation
- Feasibility assessment checklist

**Maps to Evals**: User Story 1, SC-007 (80%+ complete Part 8 with deployable model)

---

#### Lesson 7: Build Your LLMOps Decision Skill

**Learning Objective**: Create reusable skill for LLMOps decision-making

**Stage**: Layer 3 (Intelligence Design)

**Duration**: 45 min

**New Concepts** (count: 0 — synthesis):
All concepts from L1-L6 encoded as reusable skill

**Content Elements**:

**Skill Structure**:
```markdown
# Skill: llmops-decision-framework

## Persona
Think like an ML engineering lead evaluating whether to invest in custom model development.

## Analysis Questions
1. What's the current solution's cost and performance?
2. What specific improvements would fine-tuning provide?
3. What data exists or can be created?
4. What's the break-even timeline?

## Decision Principles
1. Start with prompt engineering — fine-tune only when ceiling reached
2. Data quality > model size
3. Plan for the full lifecycle, not just training
4. Spec success criteria BEFORE training
```

**Deliverable**: Students create `.claude/skills/llmops-decision-framework/SKILL.md`

**Maps to Evals**: SC-008 (students graduate with llmops skill)

---

## Chapter 62: LLM Architecture & Compute Reality

### Chapter Analysis

**Chapter Type**: Technical/Conceptual — Deep dive into how LLMs work internally, focused on practical implications for fine-tuning decisions. Lab exercises on inference.

**Proficiency Tier**: B2 (Upper-Intermediate)

**Concept Density**: 10 core concepts

1. Transformer architecture (attention, layers, parameters)
2. Tokenization (BPE, vocabulary, context length)
3. Parameter counting (embedding, attention, FFN)
4. VRAM requirements calculation
5. Quantization theory (FP16 → INT8 → INT4)
6. BitsAndBytes and NF4 quantization
7. Gradient checkpointing
8. Batch size and memory tradeoffs
9. Inference optimization (KV cache, speculative decoding)
10. T4 GPU characteristics and constraints

**Justified Lesson Count**: 8 lessons
- Layer 1 (Conceptual): 4 lessons (architecture, tokenization, memory, quantization)
- Layer 2 (Hands-on): 3 lessons (VRAM calculation, quantization lab, inference lab)
- Layer 3 (Skill): 1 lesson (compute planning skill)

---

### Lesson Sequence

#### Lesson 1: Transformer Architecture Essentials

**Learning Objective**: Explain transformer architecture components relevant to fine-tuning decisions

**Stage**: Layer 1 (Conceptual Foundation)

**Duration**: 45 min

**New Concepts** (count: 4):
1. Attention mechanism (query, key, value)
2. Multi-head attention
3. Feed-forward network layers
4. Layer normalization and residual connections

**Cognitive Load Validation**: 4 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:
- Transformer block diagram (simplified for practitioners)
- Attention: "What should I focus on?" mechanism
- Multi-head: Different "perspectives" on the input
- FFN: Knowledge storage layers
- **Practical insight**: FFN layers store "knowledge" — this is what we modify in fine-tuning

**NOT Covering** (research-level):
- Positional encoding math
- Softmax temperature details
- Flash attention implementation

**Maps to Evals**: Foundation for understanding LoRA targeting

---

#### Lesson 2: Tokenization and Context

**Learning Objective**: Explain how tokenization affects model behavior and training data preparation

**Stage**: Layer 1 (Conceptual Foundation)

**Duration**: 40 min

**New Concepts** (count: 4):
1. BPE tokenization algorithm
2. Vocabulary size tradeoffs
3. Context length limits
4. Special tokens (BOS, EOS, PAD)

**Cognitive Load Validation**: 4 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:
```python
# Tokenization in action
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3-8B")
text = "Create a new task called 'Review PR #123'"
tokens = tokenizer.encode(text)
print(f"Text: {text}")
print(f"Tokens: {tokens}")
print(f"Token count: {len(tokens)}")
```

- Why tokenization matters for training data
- Context length: Llama-3 (8K), Mistral (32K)
- **Practical insight**: Training data format affects token efficiency

**Maps to Evals**: Foundation for Chapter 63 (data engineering)

---

#### Lesson 3: Parameter Counting and Model Sizes

**Learning Objective**: Calculate parameter count and understand model size implications

**Stage**: Layer 1 (Conceptual Foundation)

**Duration**: 40 min

**New Concepts** (count: 3):
1. Parameter counting formula (embedding + attention + FFN)
2. Model size tiers (7B, 13B, 70B) and capabilities
3. Scaling laws (bigger ≠ always better for specialized tasks)

**Cognitive Load Validation**: 3 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:

**Model Size Reference**:
| Model | Parameters | VRAM (FP16) | VRAM (4-bit) | Fits T4? |
|-------|-----------|-------------|--------------|----------|
| Llama-3-8B | 8B | 16GB | 5GB | ✅ Training |
| Mistral-7B | 7B | 14GB | 4.5GB | ✅ Training |
| Llama-3-70B | 70B | 140GB | 40GB | ❌ |

**Key insight**: For specialized tasks, 7B fine-tuned often beats 70B prompted

**Maps to Evals**: SC-002 (<5% OOM rate), foundation for VRAM planning

---

#### Lesson 4: VRAM Budget Planning

**Learning Objective**: Calculate VRAM requirements for training and inference

**Stage**: Layer 2 (Hands-on with AI)

**Duration**: 50 min

**New Concepts** (count: 4):
1. VRAM components (model weights, gradients, optimizer states, activations)
2. Training multiplier (~4x weights for FP16, less with quantization)
3. Inference requirements (weights + KV cache)
4. Batch size impact on memory

**Cognitive Load Validation**: 4 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Three Roles Demonstration**:

**AI as Teacher**: AI explains VRAM formula
```
Training VRAM ≈ Weights + Gradients + Optimizer + Activations
For full precision: ~4x model size
For QLoRA: ~1.5x model size (quantized base + full precision adapters)
```

**AI as Student**: Student specifies their GPU constraints
- "I have T4 with 15GB VRAM"
- "I want to fine-tune Llama-3-8B"

**AI as Co-Worker**: Calculate together
```python
# QLoRA VRAM estimation
base_model_quantized = 5  # GB (4-bit Llama-3-8B)
lora_adapters = 0.1  # GB (full precision, small)
gradients = 0.1  # GB (only LoRA parameters)
optimizer = 0.2  # GB (only LoRA parameters)
activations = 4  # GB (depends on batch size, seq length)
# Total: ~9.4 GB — fits T4!
```

**Maps to Evals**: SC-002 (OOM prevention)

---

#### Lesson 5: Quantization Deep Dive

**Learning Objective**: Apply quantization techniques to reduce model memory requirements

**Stage**: Layer 2 (Hands-on with AI)

**Duration**: 55 min

**New Concepts** (count: 4):
1. Precision formats (FP32, FP16, BF16, INT8, INT4)
2. BitsAndBytes library
3. NF4 (NormalFloat4) quantization
4. Double quantization

**Cognitive Load Validation**: 4 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:
```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3-8B",
    quantization_config=bnb_config,
    device_map="auto",
)
```

**Precision Comparison**:
| Format | Bits per Param | 8B Model Size | Quality Loss |
|--------|---------------|---------------|--------------|
| FP32 | 32 | 32 GB | 0% (baseline) |
| FP16 | 16 | 16 GB | ~0% |
| INT8 | 8 | 8 GB | <1% |
| INT4/NF4 | 4 | 4 GB | 1-3% |

**Maps to Evals**: FR-005 (4-bit quantization required)

---

#### Lesson 6: Memory Optimization Techniques

**Learning Objective**: Apply gradient checkpointing and batch size tuning to fit training in limited VRAM

**Stage**: Layer 2 (Hands-on)

**Duration**: 45 min

**New Concepts** (count: 3):
1. Gradient checkpointing (trade compute for memory)
2. Batch size and gradient accumulation
3. Mixed precision training

**Cognitive Load Validation**: 3 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:
```python
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-3-8b-bnb-4bit",
    max_seq_length=2048,
    load_in_4bit=True,
)

# Enable gradient checkpointing
model.gradient_checkpointing_enable()

# Training args with memory optimization
training_args = TrainingArguments(
    per_device_train_batch_size=1,  # Small batch
    gradient_accumulation_steps=8,   # Effective batch = 8
    fp16=True,                       # Mixed precision
    ...
)
```

**When OOM Happens Checklist**:
1. Reduce batch size (1 is minimum)
2. Enable gradient checkpointing
3. Reduce sequence length
4. Use gradient accumulation
5. Check for memory leaks (del, gc.collect())

**Maps to Evals**: Edge case handling (OOM on T4)

---

#### Lesson 7: Lab — Inference on T4

**Learning Objective**: Run inference on quantized model using Colab T4 GPU

**Stage**: Layer 2 (Hands-on Lab)

**Duration**: 60 min

**New Concepts** (count: 2):
1. Colab GPU setup
2. Inference pipeline construction

**Cognitive Load Validation**: 2 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Lab Objectives**:
1. Set up Colab with T4 GPU
2. Load quantized Llama-3-8B
3. Run inference with Task API domain prompts
4. Measure latency and memory usage

**Lab Code**:
```python
# Colab setup
!pip install -q unsloth transformers accelerate

# Check GPU
import torch
print(f"GPU: {torch.cuda.get_device_name(0)}")
print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

# Load model
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-3-8b-Instruct-bnb-4bit",
    max_seq_length=2048,
    load_in_4bit=True,
)

# Task API inference
prompt = """You are a helpful task assistant.

User: Create a task for reviewing the Q4 budget spreadsheet by Friday.
Assistant:"""

inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

**Maps to Evals**: FR-004 (Colab Free Tier compatible)

---

#### Lesson 8: Build Your Compute Planning Skill

**Learning Objective**: Create reusable skill for LLMOps compute planning

**Stage**: Layer 3 (Intelligence Design)

**Duration**: 45 min

**New Concepts** (count: 0 — synthesis):
All concepts from L1-L7 encoded as reusable skill

**Skill Structure**:
```markdown
# Skill: llmops-compute-planner

## Persona
Think like a GPU infrastructure engineer sizing training jobs.

## Analysis Questions
1. What model are you fine-tuning? (parameters, architecture)
2. What GPU do you have? (VRAM, compute capability)
3. What's your sequence length and batch size target?
4. What quantization level is acceptable for your use case?

## Decision Principles
1. Calculate before you run — OOM wastes time
2. Start with maximum quantization, increase precision if quality suffers
3. Gradient checkpointing is almost always worth the compute tradeoff
4. Effective batch size = per_device_batch * gradient_accumulation * gpus
```

**Deliverable**: Students extend `.claude/skills/llmops-compute-planner/SKILL.md`

**Maps to Evals**: SC-008 (skill accumulation)

---

## Chapter 63: Data Engineering for Training

### Chapter Analysis

**Chapter Type**: Technical — Hands-on data preparation, quality pipelines, synthetic data generation. Heavy code focus.

**Proficiency Tier**: B2 (Upper-Intermediate)

**Concept Density**: 9 core concepts

1. Dataset formats (Alpaca, ShareGPT/conversations)
2. JSONL structure and validation
3. Data quality dimensions (accuracy, consistency, coverage)
4. Deduplication and cleaning
5. Train/validation/test splits
6. Synthetic data generation with LLMs
7. Quality filtering pipelines
8. Cost management for data generation
9. Privacy and PII handling

**Justified Lesson Count**: 8 lessons
- L00 (Skill-First): 1 lesson (build data-engineering skill)
- Layer 1 (Conceptual): 2 lessons (formats, quality dimensions)
- Layer 2 (Hands-on): 4 lessons (JSONL, cleaning, synthetic, pipeline)
- Layer 3 (Skill): 1 lesson (data quality skill refinement)

---

### Lesson Sequence

#### Lesson 0: Build Your Data Engineering Skill

**Learning Objective**: Create data-engineering skill from official documentation before learning the details

**Stage**: L00 (Skill-First Pattern)

**Duration**: 30 min

**Content Elements**:

**Skill-First Workflow**:
1. Clone skills-lab fresh: `git clone https://github.com/panaversity/skills-lab`
2. Write LEARNING-SPEC.md:
```markdown
# Learning Spec: LLMOps Data Engineering

## What I Want to Learn
- How to prepare training datasets for fine-tuning
- Dataset formats (Alpaca, ShareGPT)
- Quality validation and filtering

## Why It Matters
Training data quality determines model quality.

## Success Criteria
- I can create a 500-row training dataset
- I can validate dataset quality
- I can generate synthetic data cost-effectively
```
3. Invoke `/fetching-library-docs unsloth` for official patterns
4. Create initial skill: `.claude/skills/llmops-data-engineer/SKILL.md`

**Skill Starting Point**:
```markdown
# Skill: llmops-data-engineer

## Persona
Think like a data engineer preparing training datasets for LLM fine-tuning.

## Core Knowledge (from official docs)
- Alpaca format: instruction, input, output
- ShareGPT format: conversations array with role/content
- Unsloth dataset formatting utilities

## Analysis Questions
1. What format does your base model expect?
2. What quality dimensions matter for your task?
3. How will you validate data before training?
```

**Maps to Evals**: SC-008 (skill accumulation), FR-003 (L00 pattern)

---

#### Lesson 1: Dataset Formats

**Learning Objective**: Distinguish between Alpaca and ShareGPT dataset formats and choose appropriately

**Stage**: Layer 1 (Conceptual Foundation)

**Duration**: 40 min

**New Concepts** (count: 3):
1. Alpaca format (instruction/input/output)
2. ShareGPT/conversations format
3. Format selection criteria

**Cognitive Load Validation**: 3 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:

**Alpaca Format** (Single-turn):
```json
{
  "instruction": "Create a task with the following details",
  "input": "title: Review budget, due: Friday, priority: high",
  "output": "Created task 'Review budget' with due date Friday and high priority."
}
```

**ShareGPT Format** (Multi-turn):
```json
{
  "conversations": [
    {"from": "human", "value": "Create a new task for reviewing the budget"},
    {"from": "gpt", "value": "I'll create that task for you. What's the due date?"},
    {"from": "human", "value": "Friday"},
    {"from": "gpt", "value": "Created task 'Review budget' due Friday."}
  ]
}
```

**When to Use Which**:
| Format | Use Case | Examples |
|--------|----------|----------|
| Alpaca | Single-turn Q&A, completions | Code generation, classification |
| ShareGPT | Multi-turn conversations | Chatbots, assistants, agents |

**Running Example**: Task API uses ShareGPT (multi-turn task management conversations)

**Maps to Evals**: FR-018 (dataset formats match industry standards)

---

#### Lesson 2: Data Quality Dimensions

**Learning Objective**: Identify quality dimensions and their impact on model performance

**Stage**: Layer 1 (Conceptual Foundation)

**Duration**: 40 min

**New Concepts** (count: 4):
1. Accuracy (correct labels/outputs)
2. Consistency (similar inputs → similar outputs)
3. Coverage (all use cases represented)
4. Diversity (variety in examples)

**Cognitive Load Validation**: 4 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:

**Quality Dimensions Framework**:
| Dimension | Definition | Detection | Impact |
|-----------|------------|-----------|--------|
| Accuracy | Outputs are correct | Human review, automated checks | Wrong outputs |
| Consistency | Same format/style | Template validation | Unpredictable behavior |
| Coverage | All scenarios included | Gap analysis | Missing capabilities |
| Diversity | Varied phrasings | Embedding clustering | Poor generalization |

**Quality Checklist for Task API Dataset**:
- [ ] All task operations covered (create, update, complete, delete)
- [ ] Various phrasing styles ("create task", "add task", "new task")
- [ ] Edge cases (empty title, past due date, invalid priority)
- [ ] Error handling examples

**Maps to Evals**: Foundation for quality pipeline

---

#### Lesson 3: Creating JSONL Datasets

**Learning Objective**: Create and validate JSONL training datasets

**Stage**: Layer 2 (Hands-on)

**Duration**: 50 min

**New Concepts** (count: 3):
1. JSONL file structure
2. Schema validation with Pydantic
3. Dataset inspection utilities

**Cognitive Load Validation**: 3 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:
```python
import json
from pydantic import BaseModel, field_validator
from typing import Literal

class Conversation(BaseModel):
    from_: Literal["human", "gpt"] = Field(alias="from")
    value: str

    @field_validator("value")
    @classmethod
    def value_not_empty(cls, v: str) -> str:
        if not v.strip():
            raise ValueError("value cannot be empty")
        return v

class TrainingExample(BaseModel):
    conversations: list[Conversation]

    @field_validator("conversations")
    @classmethod
    def valid_conversation(cls, v: list) -> list:
        if len(v) < 2:
            raise ValueError("need at least 2 turns")
        if v[0].from_ != "human":
            raise ValueError("conversation must start with human")
        return v

def validate_dataset(filepath: str) -> tuple[int, list[str]]:
    """Validate JSONL dataset, return (valid_count, errors)."""
    valid = 0
    errors = []
    with open(filepath, 'r') as f:
        for i, line in enumerate(f, 1):
            try:
                data = json.loads(line)
                TrainingExample.model_validate(data)
                valid += 1
            except Exception as e:
                errors.append(f"Line {i}: {e}")
    return valid, errors
```

**Running Example**: Create 50-row Task API dataset manually

**Maps to Evals**: FR-018 (JSONL format), foundation for synthetic generation

---

#### Lesson 4: Data Cleaning Pipeline

**Learning Objective**: Build pipeline to deduplicate, normalize, and clean training data

**Stage**: Layer 2 (Hands-on)

**Duration**: 50 min

**New Concepts** (count: 4):
1. Deduplication (exact and fuzzy)
2. Text normalization
3. Length filtering
4. Quality scoring

**Cognitive Load Validation**: 4 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:
```python
import pandas as pd
from datasketch import MinHash, MinHashLSH

def deduplicate_fuzzy(examples: list[dict], threshold: float = 0.8) -> list[dict]:
    """Remove near-duplicate examples using MinHash LSH."""
    lsh = MinHashLSH(threshold=threshold, num_perm=128)
    unique = []

    for i, ex in enumerate(examples):
        text = ex["conversations"][0]["value"]  # Use first turn
        m = MinHash(num_perm=128)
        for word in text.lower().split():
            m.update(word.encode('utf8'))

        if not lsh.query(m):  # No similar item found
            lsh.insert(str(i), m)
            unique.append(ex)

    return unique

def filter_by_length(examples: list[dict],
                     min_tokens: int = 10,
                     max_tokens: int = 2048) -> list[dict]:
    """Filter examples by total conversation length."""
    def count_tokens(ex):
        total = sum(len(c["value"].split()) for c in ex["conversations"])
        return total

    return [ex for ex in examples
            if min_tokens <= count_tokens(ex) <= max_tokens]
```

**Cleaning Pipeline**:
```
Raw Data → Dedup (exact) → Dedup (fuzzy) → Normalize → Filter Length → Validate Schema
```

**Maps to Evals**: Edge case (corrupted data handling)

---

#### Lesson 5: Synthetic Data Generation

**Learning Objective**: Generate training data using LLMs with quality control

**Stage**: Layer 2 (AI Collaboration)

**Duration**: 60 min

**New Concepts** (count: 4):
1. Synthetic data generation strategy
2. Prompt engineering for data generation
3. Self-consistency filtering
4. Cost management

**Cognitive Load Validation**: 4 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Three Roles Demonstration**:

**AI as Teacher**: Suggests generation prompts
```
Generate a training example for a task management assistant.
The user should ask to create a task with specific details.
The assistant should confirm the task creation.
Include natural language variation.
```

**AI as Student**: Student specifies domain constraints
- "Tasks must have title, due_date, priority, description"
- "Assistant should ask clarifying questions if info missing"
- "Include error cases (past due date, invalid priority)"

**AI as Co-Worker**: Generate and validate together

**Content Elements**:
```python
import openai

def generate_training_examples(
    client: openai.OpenAI,
    n_examples: int = 100,
    model: str = "gpt-4o-mini"
) -> list[dict]:
    """Generate synthetic training examples."""

    system_prompt = """Generate a training conversation for a task management assistant.

    Requirements:
    - User asks to create/update/complete/delete a task
    - Include natural language variation
    - Assistant confirms actions taken
    - Format as JSON with 'conversations' array
    """

    examples = []
    for i in range(n_examples):
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Generate example {i+1}. Topic: {random.choice(topics)}"}
            ],
            response_format={"type": "json_object"}
        )
        examples.append(json.loads(response.choices[0].message.content))

    return examples

# Cost estimation
# GPT-4o-mini: ~$0.15/1M input tokens, ~$0.60/1M output tokens
# 500 examples × ~500 tokens each = 250K tokens = ~$0.15
```

**Maps to Evals**: SC-010 (<$0.15 per student for 500 rows)

---

#### Lesson 6: Quality Filtering Pipeline

**Learning Objective**: Build automated quality filtering with LLM-as-judge

**Stage**: Layer 2 (Hands-on with AI)

**Duration**: 55 min

**New Concepts** (count: 3):
1. LLM-as-judge pattern
2. Confidence scoring
3. Multi-criteria filtering

**Cognitive Load Validation**: 3 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:
```python
def llm_quality_judge(
    example: dict,
    client: openai.OpenAI,
    criteria: list[str]
) -> dict:
    """Score training example quality using LLM."""

    prompt = f"""Rate this training example on each criterion (1-5):

Example:
{json.dumps(example, indent=2)}

Criteria:
{chr(10).join(f"- {c}" for c in criteria)}

Respond with JSON: {{"scores": {{"criterion": score}}, "reasoning": "..."}}
"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"}
    )

    return json.loads(response.choices[0].message.content)

# Quality criteria for Task API
criteria = [
    "natural_language: User message sounds natural",
    "accuracy: Assistant response is correct",
    "completeness: Conversation includes all needed info",
    "format: Follows expected task structure"
]

# Filter: Keep examples with average score >= 4
filtered = [ex for ex in examples
            if llm_quality_judge(ex, client, criteria)["avg_score"] >= 4]
```

**Maps to Evals**: Edge case (corrupted data validation)

---

#### Lesson 7: Lab — Build 500-Row Task API Dataset

**Learning Objective**: Create complete training dataset for Task API fine-tuning

**Stage**: Layer 2 (Hands-on Lab)

**Duration**: 90 min

**New Concepts** (count: 0 — synthesis):
Apply all L1-L6 concepts to create real dataset

**Lab Objectives**:
1. Generate 600 synthetic examples (buffer for filtering)
2. Apply cleaning pipeline (dedup, normalize)
3. Quality filter to 500 high-quality examples
4. Create train/val/test splits (400/50/50)
5. Validate final dataset

**Lab Deliverable**:
```
data/
├── task_api_train.jsonl  (400 examples)
├── task_api_val.jsonl    (50 examples)
├── task_api_test.jsonl   (50 examples)
└── data_report.md        (quality metrics)
```

**Cost Target**: <$0.15 total (GPT-4o-mini)

**Maps to Evals**: SC-010 (cost target), FR-004 (Colab compatible)

---

#### Lesson 8: Refine Your Data Engineering Skill

**Learning Objective**: Update data engineering skill with lessons learned

**Stage**: Layer 3 (Intelligence Refinement)

**Duration**: 30 min

**Content Elements**:

Update `.claude/skills/llmops-data-engineer/SKILL.md`:
```markdown
# Skill: llmops-data-engineer (v2.0)

## Persona
Think like a data engineer preparing training datasets for LLM fine-tuning.

## Analysis Questions
1. What format does your base model expect? (Alpaca vs ShareGPT)
2. What quality dimensions matter for your task?
3. What's your deduplication strategy? (exact + fuzzy)
4. How will you validate data before training?
5. What's your cost budget for synthetic generation?

## Decision Principles
1. Quality > Quantity: 500 clean examples beat 5000 noisy ones
2. Start with real data if available, augment with synthetic
3. Always split before training: train/val/test
4. LLM-as-judge for scalable quality filtering
5. Budget: ~$0.30/1000 synthetic examples with GPT-4o-mini

## Workflow
1. Define schema (Pydantic validation)
2. Collect/generate examples
3. Clean: dedup → normalize → filter
4. Quality score with LLM-as-judge
5. Split: 80/10/10 or similar
6. Validate schema on all splits
```

**Maps to Evals**: SC-008 (skill accumulation)

---

## Chapter 64: Supervised Fine-Tuning (SFT)

### Chapter Analysis

**Chapter Type**: Technical — Core hands-on chapter teaching LoRA/QLoRA fine-tuning with Unsloth. Heavy code, lab-focused.

**Proficiency Tier**: B2 (Upper-Intermediate)

**Concept Density**: 11 core concepts

1. LoRA theory (low-rank adaptation)
2. QLoRA (quantized LoRA)
3. Adapter targeting (which layers to modify)
4. Unsloth framework
5. Training arguments (lr, epochs, batch size)
6. Loss curves and convergence
7. Checkpointing and resume
8. Hyperparameter tuning
9. Overfitting detection
10. Adapter merging
11. Model export

**Justified Lesson Count**: 9 lessons
- L00 (Skill-First): 1 lesson
- Layer 1 (Conceptual): 2 lessons (LoRA theory, hyperparameters)
- Layer 2 (Hands-on): 5 lessons (setup, training, monitoring, tuning, export)
- Layer 3 (Capstone): 1 lesson (Digital FTE: Task Assistant)

---

### Lesson Sequence

#### Lesson 0: Build Your Fine-Tuning Skill

**Learning Objective**: Create fine-tuning skill from official Unsloth documentation

**Stage**: L00 (Skill-First Pattern)

**Duration**: 30 min

**Content Elements**:

**Skill-First Workflow**:
1. Fresh skills-lab clone
2. LEARNING-SPEC.md for fine-tuning goals
3. `/fetching-library-docs unsloth` for official patterns
4. Create: `.claude/skills/llmops-fine-tuner/SKILL.md`

**Initial Skill**:
```markdown
# Skill: llmops-fine-tuner

## Persona
Think like an ML engineer fine-tuning LLMs for production deployment.

## Core Knowledge (from Unsloth docs)
- FastLanguageModel for efficient loading
- LoRA adapters with PEFT
- 4-bit quantization for T4 GPU
- SFTTrainer from trl

## Key Patterns
- Load: FastLanguageModel.from_pretrained()
- Configure: get_peft_model() with LoRA config
- Train: SFTTrainer with dataset
- Save: model.save_pretrained()
```

**Maps to Evals**: SC-008 (skill), FR-003 (L00 pattern)

---

#### Lesson 1: LoRA Theory

**Learning Objective**: Explain how LoRA reduces fine-tuning parameters while preserving quality

**Stage**: Layer 1 (Conceptual Foundation)

**Duration**: 45 min

**New Concepts** (count: 4):
1. Low-rank matrix decomposition
2. Rank (r) parameter and its impact
3. Alpha scaling factor
4. Target modules selection

**Cognitive Load Validation**: 4 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:

**LoRA Intuition**:
```
Original weight matrix W: (4096 × 4096) = 16M parameters
LoRA adapters: A (4096 × 16) + B (16 × 4096) = 131K parameters

We train the small adapters, keep base weights frozen.
Result: 99.2% parameter reduction, ~95% quality retention
```

**Key Parameters**:
| Parameter | Range | Impact |
|-----------|-------|--------|
| rank (r) | 8-64 | Higher = more capacity, more memory |
| alpha | 16-64 | Scaling factor, typically alpha = 2*r |
| target_modules | q_proj, v_proj, etc. | Which layers to adapt |

**Maps to Evals**: Foundation for training

---

#### Lesson 2: Hyperparameters Explained

**Learning Objective**: Configure training hyperparameters for stable fine-tuning

**Stage**: Layer 1 (Conceptual Foundation)

**Duration**: 45 min

**New Concepts** (count: 5):
1. Learning rate and schedulers
2. Epochs and steps
3. Batch size and gradient accumulation
4. Weight decay and regularization
5. Warmup ratio

**Cognitive Load Validation**: 5 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:

**Recommended Starting Point**:
```python
training_args = TrainingArguments(
    learning_rate=2e-4,              # Higher for LoRA than full fine-tune
    num_train_epochs=3,              # Start small, increase if needed
    per_device_train_batch_size=4,   # Limited by VRAM
    gradient_accumulation_steps=4,   # Effective batch = 16
    warmup_ratio=0.03,               # 3% warmup
    weight_decay=0.01,               # Light regularization
    lr_scheduler_type="cosine",      # Smooth decay
    ...
)
```

**Hyperparameter Decision Tree**:
```
Loss not decreasing? → Increase learning rate
Loss unstable? → Decrease learning rate, increase warmup
Overfitting? → More weight decay, fewer epochs
OOM? → Reduce batch size, increase grad accumulation
```

**Maps to Evals**: Foundation for training success

---

#### Lesson 3: Unsloth Setup

**Learning Objective**: Set up Unsloth training environment on Colab T4

**Stage**: Layer 2 (Hands-on)

**Duration**: 50 min

**New Concepts** (count: 3):
1. Unsloth installation
2. FastLanguageModel loading
3. LoRA configuration with PEFT

**Cognitive Load Validation**: 3 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:
```python
# Colab installation
!pip install -q unsloth
!pip install -q --no-deps xformers trl peft accelerate bitsandbytes

from unsloth import FastLanguageModel
import torch

# Load model with 4-bit quantization
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-3-8b-bnb-4bit",
    max_seq_length=2048,
    dtype=None,  # Auto-detect
    load_in_4bit=True,
)

# Configure LoRA
model = FastLanguageModel.get_peft_model(
    model,
    r=16,                    # Rank
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                   "gate_proj", "up_proj", "down_proj"],
    lora_alpha=32,           # Alpha = 2*r
    lora_dropout=0.05,
    bias="none",
    use_gradient_checkpointing=True,
)
```

**Maps to Evals**: FR-005 (4-bit), FR-004 (Colab)

---

#### Lesson 4: Training Loop

**Learning Objective**: Run supervised fine-tuning with Unsloth SFTTrainer

**Stage**: Layer 2 (Hands-on)

**Duration**: 60 min

**New Concepts** (count: 4):
1. Dataset formatting for SFT
2. SFTTrainer configuration
3. Training execution
4. Basic monitoring

**Cognitive Load Validation**: 4 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:
```python
from trl import SFTTrainer
from transformers import TrainingArguments
from datasets import load_dataset

# Load Task API dataset
dataset = load_dataset("json", data_files="task_api_train.jsonl", split="train")

# Format function for chat template
def format_example(example):
    conversation = example["conversations"]
    text = tokenizer.apply_chat_template(
        [{"role": c["from"].replace("gpt", "assistant"),
          "content": c["value"]} for c in conversation],
        tokenize=False,
        add_generation_prompt=False
    )
    return {"text": text}

dataset = dataset.map(format_example)

# Training
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=2048,
    args=TrainingArguments(
        output_dir="./task_api_lora",
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        learning_rate=2e-4,
        num_train_epochs=3,
        warmup_ratio=0.03,
        logging_steps=10,
        save_steps=100,
        fp16=True,
    ),
)

trainer.train()
```

**Maps to Evals**: SC-001 (complete fine-tuning), SC-002 (<5% OOM)

---

#### Lesson 5: Monitoring and Debugging

**Learning Objective**: Monitor training progress and debug common issues

**Stage**: Layer 2 (Hands-on)

**Duration**: 50 min

**New Concepts** (count: 4):
1. Loss curve interpretation
2. Validation loss tracking
3. Overfitting detection
4. Common failure modes

**Cognitive Load Validation**: 4 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:

**Loss Curve Patterns**:
```
Healthy Training:
  Loss starts high, decreases smoothly, plateaus
  Train/val loss track together

Underfitting:
  Loss decreases slowly, never reaches target
  → Increase epochs, learning rate, model capacity

Overfitting:
  Train loss decreases, val loss increases
  → Early stopping, more data, regularization

Unstable:
  Loss oscillates wildly
  → Decrease learning rate, increase warmup
```

**Debugging Checklist**:
```python
# Add validation monitoring
from transformers import TrainerCallback

class ValidationCallback(TrainerCallback):
    def on_evaluate(self, args, state, control, metrics, **kwargs):
        print(f"Step {state.global_step}: Val Loss = {metrics.get('eval_loss', 'N/A')}")

trainer = SFTTrainer(
    ...,
    eval_dataset=val_dataset,
    callbacks=[ValidationCallback()],
)
```

**Maps to Evals**: Training success

---

#### Lesson 6: Hyperparameter Tuning

**Learning Objective**: Tune hyperparameters for optimal model performance

**Stage**: Layer 2 (AI Collaboration)

**Duration**: 55 min

**New Concepts** (count: 3):
1. Systematic tuning approach
2. Learning rate finder
3. Ablation studies

**Cognitive Load Validation**: 3 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Three Roles Demonstration**:

**AI as Teacher**: Suggests tuning strategy
- "Start with learning rate — most impactful"
- "Use powers of 10: 1e-5, 1e-4, 1e-3"
- "Tune rank after learning rate is stable"

**AI as Student**: Student provides results
- "1e-4 gives stable training"
- "r=16 seems sufficient for our task"

**AI as Co-Worker**: Converge on optimal config
```python
# Optimal config for Task API
optimal_config = {
    "learning_rate": 2e-4,
    "r": 16,
    "lora_alpha": 32,
    "num_epochs": 3,
    "batch_size": 4,
    "grad_accum": 4,
}
```

**Maps to Evals**: SC-003 (20%+ improvement)

---

#### Lesson 7: Checkpointing and Resume

**Learning Objective**: Save checkpoints and resume interrupted training

**Stage**: Layer 2 (Hands-on)

**Duration**: 40 min

**New Concepts** (count: 3):
1. Checkpoint saving
2. Resume from checkpoint
3. Best model selection

**Cognitive Load Validation**: 3 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:
```python
# Save checkpoints during training
training_args = TrainingArguments(
    save_steps=100,
    save_total_limit=3,  # Keep only 3 best
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    ...
)

# Resume from checkpoint (if Colab disconnects)
trainer.train(resume_from_checkpoint="./task_api_lora/checkpoint-200")

# Save final adapter
model.save_pretrained("./task_api_lora_final")
tokenizer.save_pretrained("./task_api_lora_final")
```

**Maps to Evals**: Edge case handling (training interruption)

---

#### Lesson 8: Model Export

**Learning Objective**: Export trained model for deployment

**Stage**: Layer 2 (Hands-on)

**Duration**: 45 min

**New Concepts** (count: 3):
1. Adapter-only saving
2. Merged model saving
3. GGUF export for Ollama

**Cognitive Load Validation**: 3 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:
```python
# Option 1: Save adapter only (small, requires base model)
model.save_pretrained("./task_api_adapter")

# Option 2: Merge and save (larger, self-contained)
merged_model = model.merge_and_unload()
merged_model.save_pretrained("./task_api_merged")

# Option 3: Export to GGUF for Ollama
model.save_pretrained_gguf(
    "task_api_gguf",
    tokenizer,
    quantization_method="q4_k_m",  # 4-bit quantization
)
```

**Export Comparison**:
| Method | Size | Use Case |
|--------|------|----------|
| Adapter only | ~30MB | When base model available |
| Merged | ~16GB (FP16) | Cloud deployment |
| GGUF q4 | ~4GB | Local Ollama |

**Maps to Evals**: SC-004 (local deployment)

---

#### Lesson 9: Capstone — Task API Assistant

**Learning Objective**: Fine-tune complete Task API assistant as Digital FTE

**Stage**: Layer 4 (Spec-Driven Capstone)

**Duration**: 90 min

**New Concepts** (count: 0 — synthesis):
All concepts composed into production model

**Spec-First Structure**:
```markdown
# Spec: Task API Assistant v1.0

## Intent
Fine-tune Llama-3-8B to serve as Task API assistant that:
- Understands task management domain
- Follows consistent response format
- Handles create/update/complete/delete operations

## Constraints
- Train on Colab T4 (15GB VRAM)
- Use 4-bit quantization
- Training time < 2 hours
- Export to GGUF for Ollama

## Success Criteria
- Val loss < 1.0
- Task creation accuracy > 90% on test set
- Response latency < 500ms on Ollama
```

**Capstone Workflow**:
1. Load Task API dataset from Chapter 63
2. Configure Unsloth with optimal hyperparameters
3. Train with validation monitoring
4. Export to GGUF
5. Test on Ollama
6. Document as Digital FTE

**Maps to Evals**: SC-001, SC-003, SC-007, User Story 1

---

## Chapter 65: Identity & Persona Tuning

### Chapter Analysis

**Chapter Type**: Technical — Focused application of SFT for persona/style tuning. Builds on Chapter 64 patterns.

**Proficiency Tier**: B2 (Upper-Intermediate)

**Concept Density**: 7 core concepts

1. Style vs knowledge tuning
2. Persona dataset design
3. Consistency in voice
4. Tone and formality control
5. Domain vocabulary injection
6. Multi-persona models
7. Persona evaluation metrics

**Justified Lesson Count**: 7 lessons
- Layer 1 (Conceptual): 2 lessons (style vs knowledge, persona design)
- Layer 2 (Hands-on): 4 lessons (dataset creation, training, evaluation, multi-persona)
- Layer 3 (Capstone): 1 lesson (Digital FTE: TaskMaster Persona)

---

### Lesson Sequence

#### Lesson 1: Style vs Knowledge Tuning

**Learning Objective**: Distinguish between style tuning (how to say) and knowledge tuning (what to know)

**Stage**: Layer 1 (Conceptual Foundation)

**Duration**: 40 min

**New Concepts** (count: 3):
1. Style tuning (tone, formality, personality)
2. Knowledge tuning (domain facts, procedures)
3. When each applies

**Cognitive Load Validation**: 3 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:

**Style vs Knowledge**:
| Aspect | Style Tuning | Knowledge Tuning |
|--------|-------------|------------------|
| Focus | How to communicate | What to communicate |
| Examples | Formal/casual, friendly/professional | Domain terminology, procedures |
| Data needed | 100-500 examples | 500-5000 examples |
| Use case | Brand voice, persona | Expert assistant |

**Task API Example**:
- Style: "TaskMaster" persona — encouraging, productivity-focused
- Knowledge: Task management best practices, priority frameworks

**Maps to Evals**: Foundation for persona design

---

#### Lesson 2: Persona Dataset Design

**Learning Objective**: Design training dataset that captures consistent persona traits

**Stage**: Layer 1 (Conceptual Foundation)

**Duration**: 45 min

**New Concepts** (count: 4):
1. Persona specification
2. Trait consistency
3. Response patterns
4. Edge case handling for persona

**Cognitive Load Validation**: 4 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:

**Persona Specification Template**:
```markdown
# Persona: TaskMaster

## Core Traits
- Encouraging: "Great job on completing that task!"
- Productivity-focused: Always suggests efficiency improvements
- Professional but friendly: Business casual tone
- Action-oriented: Focuses on next steps

## Vocabulary
- "Let's get this done"
- "You're making great progress"
- "Here's a tip to work smarter"

## Response Patterns
- Start with acknowledgment
- Provide action confirmation
- End with encouragement or next suggestion

## What NOT to do
- Never be condescending
- Never use excessive emojis
- Never give generic responses
```

**Maps to Evals**: Foundation for persona training

---

#### Lesson 3: Creating Persona Datasets

**Learning Objective**: Generate training data that captures persona consistently

**Stage**: Layer 2 (Hands-on)

**Duration**: 55 min

**New Concepts** (count: 2):
1. Persona-guided generation
2. Consistency validation

**Cognitive Load Validation**: 2 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:
```python
def generate_persona_example(client, persona_spec: str, scenario: str) -> dict:
    """Generate training example following persona spec."""

    prompt = f"""Generate a conversation following this persona:

{persona_spec}

Scenario: {scenario}

Generate a natural conversation where the user interacts with this persona.
The persona's responses must strictly follow the trait guidelines.
Format as JSON with 'conversations' array.
"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"}
    )

    return json.loads(response.choices[0].message.content)

# Validate persona consistency
def validate_persona_traits(example: dict, traits: list[str]) -> float:
    """Score how well example matches persona traits."""
    # Use LLM-as-judge for trait scoring
    ...
```

**Maps to Evals**: Data quality for persona

---

#### Lesson 4: Persona Fine-Tuning

**Learning Objective**: Fine-tune model for consistent persona expression

**Stage**: Layer 2 (Hands-on)

**Duration**: 50 min

**New Concepts** (count: 2):
1. System prompt injection during training
2. Persona-specific hyperparameters

**Cognitive Load Validation**: 2 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:
```python
# Include persona in system message during training
def format_persona_example(example, persona_system_prompt):
    messages = [{"role": "system", "content": persona_system_prompt}]
    for turn in example["conversations"]:
        role = "assistant" if turn["from"] == "gpt" else "user"
        messages.append({"role": role, "content": turn["value"]})

    return tokenizer.apply_chat_template(messages, tokenize=False)

# Persona training typically needs fewer epochs
# (we're adjusting style, not teaching new knowledge)
training_args = TrainingArguments(
    num_train_epochs=2,  # Less than knowledge tuning
    learning_rate=1e-4,  # Lower for subtle adjustments
    ...
)
```

**Maps to Evals**: SC-003 (persona quality)

---

#### Lesson 5: Persona Evaluation

**Learning Objective**: Evaluate persona consistency and quality

**Stage**: Layer 2 (Hands-on)

**Duration**: 45 min

**New Concepts** (count: 3):
1. Trait adherence scoring
2. A/B comparison
3. Human evaluation protocol

**Cognitive Load Validation**: 3 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:
```python
def evaluate_persona(model, tokenizer, test_prompts: list[str], persona_traits: list[str]) -> dict:
    """Evaluate model's persona consistency."""
    results = []

    for prompt in test_prompts:
        response = generate_response(model, tokenizer, prompt)

        # Score each trait
        trait_scores = {}
        for trait in persona_traits:
            score = llm_judge_trait(response, trait)
            trait_scores[trait] = score

        results.append({
            "prompt": prompt,
            "response": response,
            "trait_scores": trait_scores,
            "avg_score": sum(trait_scores.values()) / len(trait_scores)
        })

    return {
        "examples": results,
        "overall_score": sum(r["avg_score"] for r in results) / len(results),
        "trait_breakdown": aggregate_traits(results)
    }
```

**Maps to Evals**: Quality validation

---

#### Lesson 6: Multi-Persona Models

**Learning Objective**: Train model that can switch between multiple personas

**Stage**: Layer 2 (Hands-on)

**Duration**: 50 min

**New Concepts** (count: 2):
1. Persona switching via system prompt
2. Mixed-persona training

**Cognitive Load Validation**: 2 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:
```python
# Train with multiple personas
personas = {
    "taskmaster": "You are TaskMaster, an encouraging productivity coach...",
    "analyst": "You are TaskAnalyst, a data-driven task reviewer...",
    "planner": "You are TaskPlanner, a strategic planning assistant..."
}

# Mix personas in training data
training_examples = []
for persona_name, persona_prompt in personas.items():
    examples = load_persona_data(persona_name)
    for ex in examples:
        formatted = format_persona_example(ex, persona_prompt)
        training_examples.append(formatted)

# At inference, switch via system prompt
def chat_with_persona(prompt: str, persona: str):
    system = personas[persona]
    return generate(model, tokenizer, system, prompt)
```

**Maps to Evals**: Advanced persona capabilities

---

#### Lesson 7: Capstone — TaskMaster Persona Bot

**Learning Objective**: Create deployable TaskMaster persona as Digital FTE

**Stage**: Layer 4 (Spec-Driven Capstone)

**Duration**: 75 min

**New Concepts** (count: 0 — synthesis):
All persona concepts composed into production bot

**Spec-First Structure**:
```markdown
# Spec: TaskMaster Persona Bot

## Intent
Fine-tune Task API assistant with TaskMaster persona that:
- Maintains encouraging, productivity-focused voice
- Uses consistent vocabulary and response patterns
- Handles all task operations with persona consistency

## Success Criteria
- Persona trait adherence > 90% on test set
- Response quality comparable to GPT-4 with persona prompt
- Deploys to Ollama for local use
```

**Digital FTE Outcome**: Deployable persona bot that could be licensed to productivity apps

**Maps to Evals**: User Story 1, SC-007

---

## Chapter 66: Agentic & Function-Calling Fine-Tuning

### Chapter Analysis

**Chapter Type**: Technical — Advanced SFT for structured outputs and tool-calling. Connects back to Part 6 agent frameworks.

**Proficiency Tier**: B2-C1 (Upper-Intermediate to Advanced)

**Concept Density**: 9 core concepts

1. Structured output formats (JSON, function schemas)
2. Tool-calling dataset design
3. Function schema encoding
4. Multi-turn tool conversations
5. Tool result handling
6. Parallel tool calls
7. Error handling in tool calls
8. Agentic evaluation metrics
9. Integration with OpenAI Agents SDK

**Justified Lesson Count**: 8 lessons
- Layer 1 (Conceptual): 2 lessons (structured outputs, tool-calling patterns)
- Layer 2 (Hands-on): 5 lessons (dataset, training, evaluation, integration, testing)
- Layer 3 (Capstone): 1 lesson (Digital FTE: Task Agent Backend)

---

### Lesson Sequence

#### Lesson 1: Structured Output Fundamentals

**Learning Objective**: Explain why structured outputs matter for agentic applications

**Stage**: Layer 1 (Conceptual Foundation)

**Duration**: 40 min

**New Concepts** (count: 3):
1. Why JSON output for agents
2. Schema adherence
3. Parsing reliability

**Cognitive Load Validation**: 3 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:

**Why Structure Matters**:
```python
# Unstructured (unreliable for agents):
"I'll create a task called 'Review budget' for you."

# Structured (agent can parse):
{
    "action": "create_task",
    "parameters": {
        "title": "Review budget",
        "due_date": "2024-01-15",
        "priority": "high"
    }
}
```

**Agent Integration Requirement**:
- OpenAI Agents SDK expects tool calls in specific format
- Custom model must match expected schema
- Parsing failures = agent workflow breaks

**Maps to Evals**: SC-005 (95%+ JSON accuracy)

---

#### Lesson 2: Tool-Calling Patterns

**Learning Objective**: Design training data for tool-calling capabilities

**Stage**: Layer 1 (Conceptual Foundation)

**Duration**: 45 min

**New Concepts** (count: 4):
1. Tool definition format
2. Tool call format
3. Tool result handling
4. Multi-turn tool conversations

**Cognitive Load Validation**: 4 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:

**Tool Definition**:
```json
{
  "name": "create_task",
  "description": "Create a new task with specified details",
  "parameters": {
    "type": "object",
    "properties": {
      "title": {"type": "string"},
      "due_date": {"type": "string", "format": "date"},
      "priority": {"type": "string", "enum": ["low", "medium", "high"]}
    },
    "required": ["title"]
  }
}
```

**Tool Call in Conversation**:
```json
{
  "conversations": [
    {"from": "human", "value": "Create a task for reviewing the budget by Friday"},
    {"from": "gpt", "value": null, "tool_calls": [
      {"name": "create_task", "arguments": {"title": "Review budget", "due_date": "Friday", "priority": "high"}}
    ]},
    {"from": "tool", "name": "create_task", "value": {"task_id": 123, "status": "created"}},
    {"from": "gpt", "value": "I've created task #123 'Review budget' due Friday with high priority."}
  ]
}
```

**Maps to Evals**: Foundation for tool-calling training

---

#### Lesson 3: Tool-Calling Dataset Creation

**Learning Objective**: Create training dataset with tool-calling examples

**Stage**: Layer 2 (Hands-on)

**Duration**: 60 min

**New Concepts** (count: 2):
1. Synthetic tool-call generation
2. Tool response simulation

**Cognitive Load Validation**: 2 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:
```python
# Task API tools
TASK_TOOLS = [
    {
        "name": "create_task",
        "description": "Create a new task",
        "parameters": {...}
    },
    {
        "name": "update_task",
        "description": "Update an existing task",
        "parameters": {...}
    },
    {
        "name": "complete_task",
        "description": "Mark a task as completed",
        "parameters": {...}
    },
    {
        "name": "list_tasks",
        "description": "List all tasks with optional filters",
        "parameters": {...}
    }
]

def generate_tool_example(client, tools: list[dict], scenario: str) -> dict:
    """Generate training example with tool calls."""

    prompt = f"""Generate a conversation where a user interacts with a task assistant.

Available tools:
{json.dumps(tools, indent=2)}

Scenario: {scenario}

The assistant should:
1. Understand user intent
2. Call appropriate tool with correct arguments
3. Handle tool response
4. Confirm action to user

Format as JSON with 'conversations' array including tool_calls.
"""
    # Generate and validate
    ...
```

**Maps to Evals**: SC-005 (tool accuracy)

---

#### Lesson 4: Agentic Fine-Tuning

**Learning Objective**: Fine-tune model for reliable tool-calling

**Stage**: Layer 2 (Hands-on)

**Duration**: 55 min

**New Concepts** (count: 3):
1. Tool-call token format
2. Special token handling
3. Structured output training

**Cognitive Load Validation**: 3 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:
```python
# Format examples with tool-call tokens
def format_tool_example(example, tools):
    messages = [
        {"role": "system", "content": f"You are a task assistant. Available tools:\n{json.dumps(tools)}"}
    ]

    for turn in example["conversations"]:
        if turn["from"] == "human":
            messages.append({"role": "user", "content": turn["value"]})
        elif turn["from"] == "gpt" and "tool_calls" in turn:
            # Format tool call
            messages.append({
                "role": "assistant",
                "content": None,
                "tool_calls": turn["tool_calls"]
            })
        elif turn["from"] == "tool":
            messages.append({
                "role": "tool",
                "name": turn["name"],
                "content": json.dumps(turn["value"])
            })
        else:
            messages.append({"role": "assistant", "content": turn["value"]})

    return tokenizer.apply_chat_template(messages, tokenize=False)
```

**Maps to Evals**: SC-005 (95%+ accuracy)

---

#### Lesson 5: Agentic Evaluation

**Learning Objective**: Evaluate tool-calling accuracy and reliability

**Stage**: Layer 2 (Hands-on)

**Duration**: 50 min

**New Concepts** (count: 3):
1. Tool call accuracy metrics
2. Argument validation
3. End-to-end workflow testing

**Cognitive Load Validation**: 3 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:
```python
def evaluate_tool_calling(model, tokenizer, test_cases: list[dict]) -> dict:
    """Evaluate tool-calling accuracy."""
    results = {
        "total": len(test_cases),
        "correct_tool": 0,
        "correct_args": 0,
        "valid_json": 0,
        "end_to_end": 0
    }

    for case in test_cases:
        response = generate_response(model, tokenizer, case["prompt"])

        try:
            parsed = json.loads(response)
            results["valid_json"] += 1

            if parsed.get("name") == case["expected_tool"]:
                results["correct_tool"] += 1

                if validate_args(parsed.get("arguments"), case["expected_args"]):
                    results["correct_args"] += 1
                    results["end_to_end"] += 1
        except json.JSONDecodeError:
            pass

    return {
        "json_accuracy": results["valid_json"] / results["total"],
        "tool_accuracy": results["correct_tool"] / results["total"],
        "arg_accuracy": results["correct_args"] / results["total"],
        "end_to_end": results["end_to_end"] / results["total"]
    }
```

**Maps to Evals**: SC-005 validation

---

#### Lesson 6: OpenAI Agents SDK Integration

**Learning Objective**: Integrate custom model as backend for OpenAI Agents SDK

**Stage**: Layer 2 (AI Collaboration)

**Duration**: 55 min

**New Concepts** (count: 2):
1. Custom model client
2. Tool response handling

**Cognitive Load Validation**: 2 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Three Roles Demonstration**:

**AI as Teacher**: Explains SDK integration pattern
```python
# Custom model acts as drop-in replacement
from openai import OpenAI

# Point to local model
client = OpenAI(
    base_url="http://localhost:11434/v1",  # Ollama
    api_key="ollama"
)
```

**AI as Student**: Student specifies constraints
- "Model must match OpenAI tool-call format"
- "Response must include function_call field"

**AI as Co-Worker**: Build integration together

**Content Elements**:
```python
from agents import Agent, function_tool

# Define tools using OpenAI SDK
@function_tool
def create_task(title: str, due_date: str | None = None, priority: str = "medium"):
    """Create a new task."""
    # Implementation...
    return {"task_id": 123, "status": "created"}

# Agent with custom model backend
agent = Agent(
    name="TaskAgent",
    model="llama-3-8b-task-api",  # Our fine-tuned model
    tools=[create_task, update_task, complete_task],
)

# Run agent
result = Runner.run_sync(agent, "Create a task for reviewing the budget")
```

**Maps to Evals**: User Story 2 (replace expensive API)

---

#### Lesson 7: Error Handling and Robustness

**Learning Objective**: Handle tool-calling errors gracefully

**Stage**: Layer 2 (Hands-on)

**Duration**: 45 min

**New Concepts** (count: 3):
1. Invalid argument handling
2. Tool execution failures
3. Retry strategies

**Cognitive Load Validation**: 3 concepts <= 10 (B2 limit) → WITHIN LIMIT

**Content Elements**:
```python
# Training data includes error cases
error_examples = [
    {
        "conversations": [
            {"from": "human", "value": "Create a task with no title"},
            {"from": "gpt", "value": "I need a title to create a task. What would you like to call it?"}
        ]
    },
    {
        "conversations": [
            {"from": "human", "value": "Complete task 99999"},
            {"from": "gpt", "value": null, "tool_calls": [{"name": "complete_task", "arguments": {"task_id": 99999}}]},
            {"from": "tool", "name": "complete_task", "value": {"error": "Task not found"}},
            {"from": "gpt", "value": "I couldn't find task #99999. Would you like me to list your current tasks?"}
        ]
    }
]
```

**Maps to Evals**: Edge case handling

---

#### Lesson 8: Capstone — Task Agent Backend

**Learning Objective**: Deploy fine-tuned model as production agent backend

**Stage**: Layer 4 (Spec-Driven Capstone)

**Duration**: 90 min

**New Concepts** (count: 0 — synthesis):
All agentic concepts composed into production system

**Spec-First Structure**:
```markdown
# Spec: Task Agent Backend v1.0

## Intent
Fine-tune Llama-3-8B as drop-in replacement for GPT-4 in Task API agent workflows:
- Handle all Task API tools (create, update, complete, list)
- Maintain 95%+ JSON accuracy
- Support multi-turn tool conversations
- Deploy via Ollama for local inference

## Success Criteria
- Tool-calling accuracy > 95%
- Valid JSON output > 99%
- Latency < 500ms on consumer hardware
- Seamless integration with OpenAI Agents SDK
```

**Digital FTE Outcome**: Production agent backend that:
- Replaces $10K/mo API costs with $300/mo local inference
- Maintains quality parity with GPT-4 for Task API domain
- Can be licensed as "Task Management AI Backend"

**Maps to Evals**: User Story 2, User Story 3, SC-005, SC-007

---

## Summary: Chapters 61-66

### Lesson Counts

| Chapter | Title | Lessons | Layer Progression |
|---------|-------|---------|-------------------|
| 61 | Introduction to LLMOps | 7 | L1→L2→L3 |
| 62 | LLM Architecture & Compute Reality | 8 | L1→L2→L3 |
| 63 | Data Engineering for Training | 8 | L00→L1→L2→L3 |
| 64 | Supervised Fine-Tuning (SFT) | 9 | L00→L1→L2→L4 |
| 65 | Identity & Persona Tuning | 7 | L1→L2→L4 |
| 66 | Agentic & Function-Calling | 8 | L1→L2→L4 |

**Total**: 47 lessons across 6 chapters

### Skills Produced

| Chapter | Skill Created |
|---------|--------------|
| 61 | `llmops-decision-framework` |
| 62 | `llmops-compute-planner` |
| 63 | `llmops-data-engineer` |
| 64 | `llmops-fine-tuner` |

### Running Example Progression

```
Ch 61-62: Understand when/how to fine-tune for Task API
    ↓
Ch 63: Create 500-row Task API training dataset
    ↓
Ch 64: Fine-tune base Task Assistant
    ↓
Ch 65: Add TaskMaster persona
    ↓
Ch 66: Add tool-calling for agent integration
    ↓
Result: Task Agent Backend (Digital FTE)
```

### Validation Checklist

**Chapter-Level Validation**:
- [x] All chapters follow 4-layer progression
- [x] Concept density within B2 limits (≤10 per lesson)
- [x] Running example (Task API) provides continuity
- [x] Skills produced in L00/L3 lessons
- [x] Digital FTE outcomes in capstones

**Technical Validation**:
- [x] All training compatible with T4 GPU (15GB VRAM)
- [x] 4-bit quantization used throughout
- [x] Colab Free Tier constraints respected
- [x] Cost targets achievable (<$1 per student)

**Constitutional Alignment**:
- [x] Spec-first in Layer 4 capstones
- [x] Three Roles demonstrated in Layer 2 lessons
- [x] No spec-first before Layer 4
- [x] Digital FTE production as outcome

---

**Plan Version**: 1.0.0
**Status**: Ready for Implementation (Chapters 61-66)
**Next Phase**: Plan chapters 67-72 (Evaluation, Deployment, Agent Integration, Capstone)
